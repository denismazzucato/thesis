\chapter{Quantitative Fairness for Neural Networks}
\labch{quantitative-fairness}

In this chapter, we introduce the context of neural network formal analysis, we define formally the concept of feed-forward deep neural networks for classification purposes.
We introduce two quantitative impact notions: the \changesname{} impact notion, which targets the repetitions of changes in the outcome, and the \qlibraname{} impact notion, which measures the amount of bias of a neural network. We then present the abstract implementation of \changesname{} and \qlibraname{}, respectively called $\abstractchangesname$ and $\abstractqlibraname$, and we show how to validate the $\defbound$-bounded impact property for both notions. Finally, we present the parallel analysis for efficient neural network verification of the $\abstractqlibraname$ impact notion.
This chapter is based on the work presented at the 28th Static Analysis Symposium (SAS 2021)~\cite{Mazzucato2021}.

\emph{Dans ce chapitre, nous introduisons le contexte de l'analyse formelle des réseaux neuronaux, et nous définissons formellement le concept de réseaux neuronaux profonds à propagation avant pour des objectifs de classification.
Nous introduisons deux notions d'impact quantitatif : la notion d'impact \changesname{}, qui cible les répétitions de changements dans le résultat, et la notion d'impact \qlibraname{}, qui mesure le degré de biais d'un réseau neuronal. Nous présentons ensuite l'implémentation abstraite de \changesname{} et \qlibraname{}, respectivement appelées $\abstractchangesname$ et $\abstractqlibraname$, et nous montrons comment valider la propriété d'impact bornée par $\defbound$ pour les deux notions. Enfin, nous présentons l'analyse parallèle pour la vérification efficace des réseaux neuronaux selon la notion d'impact $\abstractqlibraname$.
Ce chapitre est basé sur les travaux présentés lors du 28e Symposium sur l'Analyse Statique (SAS 2021)~\sidecite{Mazzucato2021}.}

% \input{src/chapters/extensional/sas21/overview}
% \input{src/chapters/extensional/sas21/changes}
% \input{src/chapters/extensional/sas21/qlibra}
% \input{src/chapters/extensional/sas21/evaluation-changes}
% \input{src/chapters/extensional/sas21/evaluation-qlibra}

\section{Neural Networks}
\labsec{neural-networks}

This section introduces feed forward neural networks formally and the transition system generated by executing the model. We then present the forward and backward reachability state semantics targeting neural networks.

\subsection{Feed-Forward Deep Neural Networks}

A \emph{feed forward deep neural network} is a directed acyclic graph where each node represents a neuron, and each edge represents a connection between neurons.
The nodes are arranged in layers, where the first layer is the input layer ($\inputlayer$), the last layer is the output layer ($\outputlayer$), and the layers in between are called hidden layers ($\hiddenlayers$).
Each layer $\hiddenlayer{i}$ is composed of a set of $\cardinalitynospaces{\hiddenlayer{i}}$ nodes, and is connected to the previous layer $\hiddenlayer{i-1}$ through a weight $\cardinalitynospaces{\hiddenlayer{i}}\times\cardinalitynospaces{\hiddenlayer{i-1}}$-matrix $\weightmatrix{i}$ and a bias $\cardinalitynospaces{\hiddenlayer{i}}$-vector $\biasvector{i}$.
The set of all nodes is denoted by $\networknodes$ and the set of nodes in layer $\hiddenlayer{i}$ is denoted by $\networknodesinlayer{i}$. The $j$-th node in layer $\hiddenlayer{i}$ is denoted by $\node$.
In this thesis we focus on feed forward neural networks used for classification tasks, where each node in the output layer represents a class, and the output of the network is the class yielding the highest value. In total, the network has $\cardinalitynospaces{\outputlayer}$ number of target classes.

An input value is a $\cardinalitynospaces{\inputlayer}$-dimensional vector $\networknodesinlayer{0}$; for simplicity, we assume that the input values are normalized to the interval $[0,1]$.
The value of each hidden and output node is computed by applying an activation function to the weighted sum of the values of the nodes in the previous layer and the bias. The activation function is usually a non-linear function, we consider the \relu{} activation function in this work, which is defined as $\relu(x) = \max(0,x)$. Thus, the value of node $\node$ in layer $\hiddenlayer{i}$ is computed as:
\begin{align*}
  \node = \relu\left(
    \sum_{k}^{\cardinalitynospaces{\hiddenlayer{i-1}}}\weight_{j, k}^{i} \node[i-1][k] + \bias_{j}^{i}
  \right)
\end{align*}
where $\weight_{j, k}^{i}$ is the weight connecting node $\node[i-1][k]$ in layer $\hiddenlayer{i-1}$ to node $\node$ in layer $\hiddenlayer{i}$, and $\bias_{j}^{i}$ is the bias of node $\node$ in layer $\hiddenlayer{i}$. Weights and biases are learned during the training phase of the network. In the following, we consider already trained networks.


\subsection{Classification Task}

In this thesis, we focus on feed forward neural networks used for classification tasks.
That is, given an input vector, the network classifies it into one of the target classes, each represented by a node in the output layer.
The way the network classifies an input vector is by computing the value of the nodes in the output layer, and then selecting the class associated with the node with the highest value, \ie{} $\argmax_{j} \node[\lastlayerindex][j]$.

The classification task is formalized by the means of the output observer $\outputobs$ (\refdef*{output-observer}) which maps the output values of the network to the target classes.
We define the output observer to return $1$ if the index of the node with the highest value is equal to the target class, and $0$ otherwise.
In such a way, when the output state of a trace from the network computation is passed to the output observer, the only non-zero value is the one corresponding to the target class. Formally, the output observer is defined as:
\begin{align*}
  \outputobs(\defstate) \DefeQ \lambda \defvariable.\spacer
  \begin{cases}
    1 & \text{if } \argmax_{j} \defstate(j) = \defvariable \\
    0 & \text{otherwise}
  \end{cases}
\end{align*}
The characterization above shows that two different states of the network can be distinguished by the output observer if and only if they are associated with different target classes, yielding in fact different outcomes.


\begin{example}
  Consider a simple neural network with two input and two output variables.
  The weights and biases connecting the input with the output layer are irrelevant for this example.
  Therefore, the network states are defined as $\defstate = \setdef{\langle x_{0, 0}, x_{0, 1}, x_{1, 0}, x_{1, 1} \rangle}{x_{0, 0}, x_{0, 1}, x_{1, 0}, x_{1, 1} \in \R}$. For simplicity, we assume all the variables to be real\sidenote{often neural networks normalize inputs in the interval [0, 1].}.
  Given, for instance, the network state $\langle 0, 1, 0, 1 \rangle$, the output observer $\outputobs$ returns the state $\outputobs(\defstate) = \langle 0, 0, 0, 1 \rangle$ to indicate that the target class is the second one, \cf{} the one relating $x_{1, 1}$.
  Given instead the network state $\langle 0.5, 1, 0.7, 0.2 \rangle$, the output observer returns the state $\outputobs(\defstate) = \langle 0, 0, 1, 0\rangle$ to indicate that the target class is the first one (the first two variables are the input variables), and so on.
\end{example}

Note that, the output observer always maps to zero the values of variables that do not belong to the output layer.

\section{Quantitative Impact Notions}
\labsec{quantitative-impact-notions}

This section elaborates on quantitative notions that can be used to measure the influence of input variables on the output of a neural network.
Namely, we introduce the \changesname{} and \qlibraname{} impact notions, which are specifically designed to handle the irregular input spaces produced by neural networks.

\subsection{The \changesname{} Impact Notion}[\changesname]
\labsec{changes-impact-notion}


This section introduces the first of the two impact notions of this chapter, called the \changesname{} impact notion.
This impact notion is designed to overcome the limitations of the previously defined impact notions when applied to neural networks.
Indeed, in the presence of neural networks, the input space is generally irregular, and all the possible input variations lead to every possible outcome.
Therefore, the impact notions defined in the previous chapter would not be useful as they measure the impact of input variables based on the number (\eg{} \outcomesname{}) or the magnitude (\eg{} \rangename{}) of the output values; even \qusedname{} would be meaningless in this setting as all the classification outcomes of a network are often reachable by any variations of input variables.
Therefore, these metrics would not provide any meaningful information about how the input variables influence the network outcome.

\begin{example}
  \denis{In this example I show a neural network input-output relations where outcomes, range and qused are not suitable. SUggesting that a way could be to exploit the repetitions or size of input space characterizations.}
\end{example}


The \changesname{} impact notion is designed to count how many times the network outcome changes by modification in the value of the input variables $\definputvariables$.
Notably, we recall similarities with the previously defined $\outcomesname$ impact notion: in this case, we consider changes in the outcome \emph{with repetitions}, that is, if two different variations in $\definputvariable$ result in the same change in outcome, it counts as double change.
The higher the number of changes in the outcome, the greater the influence on the program outcome.
Therefore, this notion demonstrates its effectiveness when the same outcomes are reachable by multiple variations.


\begin{example}
  \denis{Recall previous example showing that changes is instead suitable.}
\end{example}


Notably, we recall similarities with the previously defined $\outcomesname$ impact notion: in this case, we consider changes in the outcome \emph{with repetitions}, that is, if two different variations in the variables $\definputvariables$ result in the same change in outcome, it counts as double change.
The higher the number of changes in the outcome, the greater the influence on the program outcome.
Therefore, this definition demonstrates its effectiveness when the same outcomes are reachable by multiple variations.
Intuitively, such situation often arises in the presence of neural networks, where generally all the possible input variations lead to every possible outcome.
Thus, counting the repetitions is a potential solution to define a meaningful impact definition for neural networks.

Specifically, the \changesname{} impact definition considers only variations in the value of input configurations that do not belong to the same \emph{continuous region}, containing no gaps or interruptions.
To this end, we first define the function $\segments$, which takes as input a set of traces $\defsetoftraces$ and an output value $\defoutput$.
This function partitions the set of traces $\defsetoftraces$ into continuous subsets with respect to the input variables $\definputvariables$.
Each subset $\defsetoftraces'$ satisfies three conditions: (1) all the traces in $\defsetoftraces'$ share the same output value $\defoutput$, (2) for any two traces in $\defsetoftraces'$, there is no other trace
in $\defsetoftraces$ with an input value between the two traces leading to a different output value, and (3) the subset is maximal, that is, there is no other trace in $\defsetoftraces$ that can be added to $\defsetoftraces'$ without violating the first two conditions.
The function $\segments$ is defined as follows:


\begin{definition}[Segments of Continuous Regions]\labdef{segments}
  Let $\definputvariables\in\inputvariables$ be the set of input variables of interest.
  Given a set of traces $\defsetoftraces\in\tracetype$ and an output value $\defoutput\in\stateandbottom$, the function $\segments\in\pair\tracetype\stateandbottom\to\setof\tracetype$ is defined as:
  \begin{align*}
    \segments(\defsetoftraces, \defoutput) &\DefeQ
      \setdef{
        \defsetoftraces' \in \phi(\defsetoftraces)}{
          \foralldef{\defsetoftraces''\supset\defsetoftraces'}{
            \defsetoftraces''\notin\phi(\defsetoftraces)
          }
        }\\
    \text{where } \phi(\defsetoftraces) &\DefeQ
      \setdef*{
        \defsetoftraces'\subseteq\defsetoftraces
      }{
        \forall \deftrace\in\defsetoftraces'.\spacer \outputobs(\retrieveoutput{\deftrace}) = \defoutput \LanD \\
        \forall \deftrace, \deftrace'\in\defsetoftraces'.\spacer \exists \deftrace''\in\defsetoftraces.\spacer \\
          \quad\retrieveinput{\deftrace}(\definputvariables) \le \retrieveinput{\deftrace''}(\definputvariables) \le \retrieveinput{\deftrace'}(\definputvariables) \implies \deftrace'' \in \defsetoftraces'
      }
  \end{align*}
\end{definition}

Note that, the auxiliary function $\phi$ partitions the set of traces $\defsetoftraces$ into subsets that satisfy the first two conditions: all the traces in the subset share the same output value $\defoutput$, and for any two traces in the subset (\cf{} $\outputobs(\retrieveoutput{\deftrace}) = \defoutput$), there is no other trace in $\defsetoftraces$ with an input value between the two traces leading to a different output value (\cf{} $\retrieveinput{\deftrace}(\definputvariables) \le \retrieveinput{\deftrace''}(\definputvariables) \le \retrieveinput{\deftrace'}(\definputvariables) \implies \deftrace'' \in \defsetoftraces'$).
The function $\segments$ then returns the maximal subsets that satisfy the first two conditions.
To better illustrate how the function $\segments$ works, consider the following example.

\begin{example}
  \denis{Example of the segments function.}
\end{example}

The function \changesname{} is then defined as the maximum, for each input configuration, of the number of continuous regions in which the output value changes.
In words, a set of input variables $\definputvariables$ has a high impact on the network outcome if the output value changes in many continuous regions when the input variables are modified. Formally, the \changesname{} impact notion is defined as follows:


\begin{definition}[\changesname]\labdef{changes}
  Given a set of input variables of interest $\definputvariables\in\setof\inputvariables$, and an output observer $\outputobs$,
  the quantity $\changes\in\tracetype\to\Nplus$ is defined as:
  \begin{align*}
    \changes(\defsetoftraces) &\DefeQ
      \sup_{\definput\in\reducedstate}
        \sup_{\defoutput\in\stateandbottom}
          \cardinality{Q_{\definputvariables, \definput, \defoutput}} \\
    \text{where } Q_{\definputvariables, \definput, \defoutput} &\DefeQ
      \bigsetjoin_{\defoutput' \in \stateandbottom\setminus\{\defoutput\}}
        \segments(
          \setdef{\deftrace\in\defsetoftraces}{\retrieveinput\deftrace \stateeq{\inputvariableswithoutw} \definput}, \defoutput'
        )
  \end{align*}
\end{definition}
The auxiliary set $Q_{\definputvariables, \definput, \defoutput}$ contains the set of continuous regions leading to an output value different from $\defoutput$.
\marginnote{\denis{Fix here, the set of traces should not overlap, non-determinism => ok}}

\begin{example}
  \denis{Example of the changes function on the previous network space.}
\end{example}


\begin{lemma}[\changesname{} is Monotonic]
  \lablemma{changes-monotonic}
For all set of traces $\defsetoftraces, \defsetoftraces'\in\tracetype$, it holds that:
  \begin{align*}
    \defsetoftraces \subseteq \defsetoftraces' \ImplieS \changes(\defsetoftraces) \le \changes(\defsetoftraces')
  \end{align*}
\end{lemma}
\begin{proof}
  The proof is based on the observation that more traces in $\defsetoftraces'$ can only increase the number of continuous regions, never a lower amount.
  Hence, the number of continuous regions leading to a different output (\cf{} $Q_{\definputvariables, \definput, \defoutput}$) can only increase with more traces.
  We conclude that $\changes(\defsetoftraces) \le \changes(\defsetoftraces')$.
\end{proof}

We show the validation of the $\defbound$-bounded impact property when instantiated with the $\changesname$ impact notion,
we call such property $\boundedchanges$.

\siderefbox{def}{output-abstraction-semantics}
\begin{lemma}[$\boundedchanges$ Validation]\lablemma{changes-validation}
  \begin{align*}
    \collectingsemantics \subseteq \BOUNDEDCHANGES \IfF \outputsemantics \subseteq \outputabstraction(\dependencyabstraction(\BOUNDEDCHANGES))
  \end{align*}
\end{lemma}
\begin{proof}
  The $\changesname$ impact notion does not consider the intermediate states, in fact, it only employs the first state in the definition of $Q_{\definputvariables, \definput, \defoutput}$ and the last one in the definition of $\segments$.
  Thus, the abstraction to dependencies does not affect the validation of the property.
  Furthermore, even handling the output abstraction at the semantic level, by abstracting output states to abstract output states, does not affect the validation of the property as the $\changesname$ impact notion already abstracts the output values before comparing to the given output $\defoutput$, \cf{} $\outputobs(\retrieveoutput{\deftrace}) = \defoutput$ in \nrefdef{segments}.
\end{proof}

\reflemma{changes-monotonic} and \reflemma{changes-validation} are of significant importance as they show that the $\changesname$ impact notion can be used to certify that a program has impact of \emph{at most} $\defbound$, meaning that the program output changes at most $\defbound$ times when the input variables vary.


\subsection{The \qlibraname{} Impact Notion}[\qlibraname]
\labsec{qlibra-impact-notion}

The second notion introduced in this section is the \qlibraname{} impact notion.
Notably, this notion is designed to quantify the amount of bias of a neural network.
The concept of bias, or better the fairness property of a network, expresses that the classification determined by a model does not depend on the ``sensitive'' input variables.
In our context, the sensitive input variables are the represented by the set of input variables $\definputvariables$ and quantifying the amount of bias is equivalent to measuring the influence of the input variables $\definputvariables$ on the network outcome.
To this end, we define the \qlibraname{} impact notion as the volume of input space able to change the model classification by perturbation of the input variables $\definputvariables$.
The higher the volume, the higher the amount of space that is prone to bias, and thus the higher the bias of the network.

In practice, we begin by gathering the set of input configurations that yield the same output abstraction.
For each output $\defoutput$, we collect the set of input configurations for which there exists a trace leading to $\defoutput$, denoted as $\setdef{\retrieveinput{\deftrace}(\definputvariables)}{\deftrace\in\defsetoftraces\land\outputobs(\retrieveoutput{\deftrace})=\defoutput}$, where $\defsetoftraces$ is the set of traces.
Specifically, we focus on the set of input configurations obtained by removing the input variables $\definputvariables$, denoted as $\setdef{\retrieveinput{\deftrace}(\inputvariableswithoutw)}{\deftrace\in\defsetoftraces\land\outputobs(\retrieveoutput{\deftrace})=\defoutput}$.
Finally, we combine all the points that lead to at least two different output values.
This is achieved by joining together all the points that intersect sets originated by different output values.
We determine the volume of this set of points by applying the standard volume operation inherent to metric spaces.
Formally, the \qlibraname{} impact notion is defined as follows:

\begin{definition}[\qlibraname]\labdef{qlibra}
  Given a set of input variables of interest $\definputvariables\in\setof\inputvariables$, and an output descriptor $\outputobs$,
  the quantity $\qlibra\in\tracetype\to[0, 1]$ is defined as:
  \begin{align*}
    \qlibra(\defsetoftraces) &\DefeQ
        \volume(\bigsetjoin \setdef{
          \defsetoftraces' \setmeet \defsetoftraces''
        }{
          \defsetoftraces'\in Q \land \defsetoftraces''\in Q \setminus \{\defsetoftraces'\}
        }) \\
    \text{where } Q &\DefeQ
      \setdef{
        \setdef{
          \retrieveinput{\deftrace}(\inputvariableswithoutw)
        }{
          \deftrace \in \defsetoftraces \land \outputobs(\retrieveoutput{\deftrace}) = \defoutput
        }
      }{
        \defoutput\in\stateandbottom
      }
  \end{align*}
\end{definition}

\begin{example}
  \denis{Example of the qlibra function on the previous network space.}
\end{example}

\begin{lemma}[\qlibraname{} is Monotonic]
  \lablemma{qlibra-monotonic}
For all set of traces $\defsetoftraces, \defsetoftraces'\in\tracetype$, it holds that:
  \begin{align*}
    \defsetoftraces \subseteq \defsetoftraces' \ImplieS \qlibra(\defsetoftraces) \le \qlibra(\defsetoftraces')
  \end{align*}
\end{lemma}
\begin{proof}
  The proof is based on the observation that more traces in $\defsetoftraces'$ can only increase the volume of the set of points, never decrease it.
  Hence, the volume of the set of points leading to different output values can only increase with more traces.
  We conclude that $\qlibra(\defsetoftraces) \le \qlibra(\defsetoftraces')$.
\end{proof}

We show the validation of the $\defbound$-bounded impact property when instantiated with the $\qlibraname$ impact notion,
we call such property $\boundedqlibra$.

\siderefbox{def}{output-abstraction-semantics}
\begin{lemma}[$\boundedqlibra$ Validation]\lablemma{qlibra-validation}
  \begin{align*}
    \collectingsemantics \subseteq \BOUNDEDQLIBRA \IfF \outputsemantics \subseteq \outputabstraction(\dependencyabstraction(\BOUNDEDQLIBRA))
  \end{align*}
\end{lemma}
\begin{proof}
  The $\qlibraname$ impact notion does not consider the intermediate states, in fact, it only employs the first state in the definition of $Q$.
  Thus, the abstraction to dependencies does not affect the validation of the property.
  Furthermore, even handling the output abstraction at the semantic level, by abstracting output states to abstract output states, does not affect the validation of the property as the $\qlibraname$ impact notion already abstracts the output values before comparing to the given output $\defoutput$, \cf{} $\outputobs(\retrieveoutput{\deftrace}) = \defoutput$ in \nrefdef{qlibra}.
\end{proof}

\reflemma{qlibra-monotonic} and \reflemma{qlibra-validation} are of significant importance as they show that the $\qlibraname$ impact notion can be used to certify that a program has impact of \emph{at most} $\defbound$, meaning that the program output changes at most $\defbound$ times when the input variables vary.

\section{Quantitative Analysis of Neural Networks}
\labsec{quantitative-analysis-neural-networks}

This section presents the abstract implementation of the \changesname{} and \qlibraname{} impact notions, respectively called $\abstractchangesname$ and $\abstractqlibraname$.
We show how to validate the $\defbound$-bounded impact property for both notions.



\subsection{Abstract Implementation \texorpdfstring{$\abstractchanges$}{Abstract Changes}}[Abstract \texorpdfstring{$\abstractchanges$}{Changes}]

In this section, we introduce $\abstractchanges$ as a sound implementation $\changes$.
First, we describe the implementation, then we present the validation of the $\defbound$-bounded impact property for the $\changesname$ impact notion, \cf{} $\boundedchanges$.

The underlying idea is to gather together abstract elements that represent continuous regions together.
To this end, our abstract domain employs a notion of disjunctive sets, which allows us to represent sets of distinct continuous regions.
Specifically, we leverage the \textit{disjunctive polyhedra abstract domain} $\disjunctivepolyabstractdomain$, defined as
$
  \langle \disjunctivepolyabstractdomaintype, \disjunctivepolyabstractdomainsubseteq \rangle
$, where $\polyabstractdomain$ represents the \textit{convex polyhedra abstract domain}~\cite{Cousot1978} and $\disjunctivepolyabstractdomaintype$ is the set of all finite subset of $\polyabstractdomain$.
The domain $\polyabstractdomain$ is also equipped with the function $\abstractdomainproject$ computing the Fourier-Motzkin elimination algorithm~\cite{Dantzig1973}.
Specifically, $\abstractdomainproject$ takes as input a variable $\definputvariables$ and a polyhedron in $d$-dimensions, returning a polyhedron in $(d-1)$-dimensions, removing the variable $\definputvariables$.
\denis{fix the fact that we do not allow any generic (convex) abstrac domain but we require the polyhedra domain.}


The function $\abstractchanges$ takes as input the variable of interest $\definputvariables$, $\numberofbuckets$ output buckets $\buckets\in\vectorbuckets$, and $\numberofbuckets$ disjunctions of polyhedra $\prefrombucket\in\disjunctivepolyabstractdomaintype^\numberofbuckets$ obtained from the backward pre-analysis.
We can access each polyhedra and disjunctions of polyhedra via indexing as in a matrix-based structure, that is, $\prefrombucket=\{\{P_{1,1}\vee \dots\vee P_{1,p}\},\dots,\{P_{n,1}\vee \dots\vee P_{n,q}\}\}$ where $p = \cardinalitynospaces{\prefrombucket_1}$ and $q = \cardinalitynospaces{\prefrombucket_n}$. For instance, $\prefrombucket[j]$ refers to the disjunction of polyhedra $\{P_{j,1}\vee \dots\vee P_{j,k}\vee \dots\}$, for $j\le n$, and $\prefrombucket[{j, k}]$ refers to the polyhedra $P_{j,k}\in\polyabstractdomain$.

The function iterates over each disjunction of polyhedra $\prefrombucket[j]\in\prefrombucket$, \ref{loopa}.
In turns, it iterates again over each polyhedron $\prefrombucket[{j, k}]\in\prefrombucket[j]$, \ref{loopb}.
Then, it projects away the input variable $\definputvariables$ from each polyhedron $\prefrombucket[{j, k}]$ (\ref{project}).
The projected polyhedra represent regions where $\definputvariables$ ranges on all possible values, considering all potential variations of this variable.
The function $\intersectallfunction$ performs
a breadth-first search traversal and gathers the set of connected components, denoted as $\higher{M}$ at \ref{connectedcomponentsindices-call}.
In our context, a connected component represents the set of simultaneously intersecting polyhedra (after the elimination of the variable $\definputvariables$).
The underlying idea is that each connected component corresponds to the set of continuous regions reachable through variations of $\definputvariables$.
Note that, each connected component is implemented as a \textit{multiset} \denis{no explaination of why using a multiset}, \ie, a set that allows multiple instances for each of its elements.
This allows us to later exclude regions leading to the same bucket.
%
Finally, $\abstractchanges$ determines the maximum count of changes across all connected components $\higher{M}$ and buckets $\buckets$ (\ref{count-changes}).
It counts the number of indices $k$ in each connected component $M\in\higher{M}$ where $k$ is not equal to $j$, thus excluding the polyhedra leading to the same output bucket $j$.

\begin{definition}[$\abstractchanges$]\labdef{abstractchanges}
  We define $\abstractchanges\in\pair\vectorbuckets\vectorbuckets\to\valuesposplus$ as:
  \begin{align*}
    &\abstractchanges(\presfrombuckets, \buckets) \DefeQ \\
      &\quad \max
      \setdef{
        \max_{j \le \numberofbuckets} \cardinalitynospaces{\setdef{l \in J}{l \neq j}}
      % \\ &\qquad\quad
      }{
        J \in \intersectallfunction((\abstractdomainproject(\prefrombucket))_{j\le\numberofbuckets})
      }
  \end{align*}
\end{definition}

\denis{Here the soundness argument.}


\section{Abstract Implementation \texorpdfstring{$\abstractqlibra$}{Abstract QLibra}}[Abstract \texorpdfstring{$\abstractqlibra$}{QLibra}]
\labsec{abstract-qlibra}


We introduce $\abstractqlibra$ as a sound implementation of $\qlibra$.
We assume $\abstractdomain$ is equipped with an additional abstract operator $\abstractdomainvolume\in\abstractdomain\to[0, 1]$, which returns the volume of the given abstract element normalized in the interval $[0, 1]$.
The abstract implementation $\abstractqlibra$ is defined as the volume of the intersecting abstract regions leading to different output buckets.
Specifically, the volume is computed by first projecting away the input variables $\definputvariables$ from all the given abstract values resulting from the backward analysis, \cf{} $\presfrombuckets$.
Then, it collects together all the possible pairwise intersections among the projected abstract values to find the portion of input space leading to different output buckets only via variations of the input variables $\definputvariables$.
Assuming that all the input variables are bounded in $[0, 1]$, which is a common practice in neural networks, the volume of the abstract element is normalized in the interval $[0, 1]$.

\begin{example}
  In the context of the interval domain, where each input variable is related to a possibly unbounded lower and upper bound, $\abstractdomainvolume(\langle \defvariable \mapsto [2, 4]\rangle) = 2$.
  On the other hand, whenever the input abstract element is unbounded, the size is $+\infty$, \eg{} $\abstractdomainvolume(\langle \defvariable \mapsto [0, +\infty]\rangle) = +\infty$.
  The function $\abstractdomainvolume$ expects only a single variable to be constrained in the abstract domain, or in other words, only one variable is allowed to be not $\top$.
\end{example}

The abstract range $\abstractqlibra$ first projects away the input variables $\definputvariables$ from all the given abstract values.
Then, it collects all the possible intersections among the projected abstract values.
These intersections represent concrete input configurations where variations on the values of $\definputvariables$ \emph{may} lead to changes of program outcome, from a bucket to another.
All the possible combination of intersections are joined together to find the maximum range of the extreme values of the buckets.
Formally, the abstract implementation $\abstractqlibra$ is defined as follows:

\begin{definition}[$\abstractqlibra$]\labdef{abstractqlibra}
  We define $\abstractqlibra\in\pair\vectorbuckets\vectorbuckets\to[0, 1]$ as:
  \begin{align*}
    &\abstractqlibra(\presfrombuckets, \buckets) \DefeQ \\
      &\quad \abstractdomainvolume\left(
        \bigjoin_{
          J \in \intersectallfunction((\abstractdomainproject(\prefrombucket))_{j\le\numberofbuckets})
        }
          \setdef{
            \prefrombucket[j] \abstractdomainmeet \prefrombucket[k]
          }{
            j, k \in J \land j \neq k
          }
      \right)
  \end{align*}
\end{definition}

\begin{example}
  \denis{Example of the abstractqlibra function on the previous network space.}
\end{example}

To prove that $\abstractqlibra$ is a sound implementation of $\qlibra$, we require the following soundness condition on the abstract operator $\abstractdomainvolume$ to ensure that the abstract volume is always greater than the concrete one.

\begin{definition}[Soundness of \texorpdfstring{$\abstractdomainvolume$}{Volume}]\labdef{soundness-volume}
  Given an abstract value $\defstate^\natural\in\abstractdomain$ and the set of input variables of interest $\definputvariables\in\inputvariables$, it holds that:
  \[\abstractdomainvolume(\defstate^\natural) \GE \cardinality{\setdef{\retrieveinput{\defstate}(\definputvariables)}{\defstate\in\abstractdomainconcretization(\defstate^\natural)}}\]
\end{definition}

The next result shows that the abstract impact $\abstractqlibra$ is a sound over-approximation of the concrete impact $\qlibra$, \cf{} \refdef{qlibra}.

\begin{lemma}[$\abstractqlibra$ is a Sound Implementation of $\qlibra$]\lablemma{abstractqlibra-is-sound}
  Let $\definputvariables\in\variables$ be the input variable of interest, $\abstractdomain$ the abstract domain, $\backwardsemanticsnoparam$ the family of semantics, and $\buckets\in\vectorbuckets$ the starting output buckets.
  Whenever the following conditions hold:
  \begin{enumerate}[label=(\roman*)]
    \item \label{rts1} $\buckets$ covers the subset of potential outcomes, \cf{} \refdef*{covering}[*-2],
    \item \label{rts2} $\buckets$ is compatible with $\outputobs$, \cf{} \refdef*{compatibility}[*4],
    \item \label{rts3} $\abstractdomainproject$ is sound, \cf{} \refdef*{soundness-project}[*8], and
    \item \label{rts4} $\abstractdomainvolume$ is sound, \cf{} \refdef{soundness-volume}[*12];
  \end{enumerate}
  then, $\abstractqlibra$ is a sound implementation of $\qlibra$.
\end{lemma}
\begin{proof}
  \denis{todo}
\end{proof}


\begin{example}
  The quantities computed by the abstract implementation $\abstractqlibra$ in \refexample{abstract-qlibra} are sound over-approximations of the concrete implementation $\qlibra$:
  \begin{align*}
    & \qlibraname_{\{\texttt{angle}\}}(\dependencysemanticsnoparam\semanticsof{\landingprogram}) = 3 \\
    &\qquad\GE \abstractqlibraname_{\{\texttt{angle}\}}(\presfrombuckets, \buckets) = 1
  \end{align*}
  \begin{align*}
    & \qlibraname_{\{\texttt{speed}\}}(\dependencysemanticsnoparam\semanticsof{\landingprogram}) = 2 \\
    &\qquad\GE \abstractqlibraname_{\{\texttt{speed}\}}(\presfrombuckets, \buckets) = 1
  \end{align*}
  as expected by \refthm*{soundness}, where $\landingprogram$ is the program of the landing alarm system, \cf{} \refprog{landing-alarm-system}.
\end{example}



\section{Parallel Analysis for Efficient Validation of the \texorpdfstring{$\boundedqlibra$ Property}{k-Bounded Impact Property applied to qlibra}}

Although sound, the abstract implementation $\abstractqlibra$ does not work well in presence of imprecision from the abstract domain employed during the backward analysis.
In fact, the abstract implementation $\abstractqlibra$ applied directly to the backward analysis represents the so called \emph{na\"ive casual-fairness analysis}~\sidecite{Urban2020}.
Such analysis suffers from the choice of existing abstract domains, which are rather fast but too imprecise to handle non-linear constraints, such as those arising from the activation functions in neural networks.
Indeed, even using the polyhedra domain for the backward analysis, handling the \relu{} activation function would over-approximate what effectively is a conditional branch, leading to a loss of precision that is reflected for each node of the neural network.
On the other hand, one could use a disjunctive completion of the polyhedra domain~\sidecite{Cousot1979}, which would retain a separate polyhedron each condition.
However, this analysis would be extremely slow.

To overcome this limitation, we first reason at a concrete-semantics level, introducing two additional semantics, the \emph{partitioning} and \emph{parallel semantics}.
Intuitively, we show how partitioning the input space into \emph{fair} partitions still allows for property validation.
Then, we show how to abstract the partitioning semantics into the parallel semantics, which is a sound over-approximation of the partitioning semantics.
Finally, we show how to validate the $\defbound$-bounded impact property for the parallel semantics.
In the abstract, such validation can now compute different fair partitions in parallel, the total volume is the sum of the volumes of the individual partitions.







\subsection{Partitioning Semantics}


\begin{definition}[Adjoints for the Partitioning Semantics]
  \labdef{adjoints-outcome-semantics}
  \begin{align*}
    \outcomeabstraction \IN& \inputoutputtype \to \outcometype \\
    \outcomeabstraction(\defsetofsetofdependencies) \DefeQ& \setdef{
      \setdef{\inputoutputtuple{\defseq} \in\defsetofdependencies}{\retrieveoutput{\defseq} = \defstate}
    }{
      \defsetofdependencies\in\defsetofsetofdependencies \land
      \defstate \in \stateandbottom
    }\\
    \outcomeconcretization \IN& \outcometype \to \inputoutputtype \\
    \outcomeconcretization(\defsetofsetofdependencies) \DefeQ& \setdef{
      \dots
    }{
      \dots
    }
  \end{align*}
\end{definition}

\begin{theorem}\labthm{inputoutput-outcome-galois-connection}
  The input-output $\inputoutputsemanticsnoparam$ and outcome $\outcomesemanticsnoparam$ semantics form a \emph{Galois Connection}:
\begin{align*}
  \galoisbetweensemantics{inputoutput}{outcome}
\end{align*}
\end{theorem}

\begin{definition}[Outcome Semantics]\labdef{outcome-semantics}
  \begin{align*}
    \outcomesemanticsnoparam\DefeQ& \outcomeabstraction(\inputoutputsemanticsnoparam)
  \end{align*}
\end{definition}

\begin{theorem}\labthm{outcome-validation}
  \begin{align*}
    \inputoutputsemantics \subseteq \unused \IfF \outcomesemantics \subseteq \outcomeabstraction(\unused)
  \end{align*}
\end{theorem}


\subsection{Parallel Semantics}


\begin{definition}[Adjoints for the Parallel Semantics]
  \labdef{adjoints-parallel-semantics}
  \begin{align*}
    \parallelabstraction \IN& \outcometype \to \paralleltype \\
    \parallelabstraction(\defsetofsetofdependencies) \DefeQ& \setdef{
      \dots
    }{
      \dots
    }\\
    \parallelconcretization \IN& \paralleltype \to \outcometype \\
    \parallelconcretization(\defsetofsetofdependencies) \DefeQ& \setdef{
      \dots
    }{
      \dots
    }
  \end{align*}
\end{definition}

\begin{theorem}\labthm{outcome-parallel-galois-connection}
  The input-output $\outcomesemanticsnoparam$ and parallel $\parallelsemanticsnoparam$ semantics form a \emph{Galois Connection}:
\begin{align*}
  \galoisbetweensemantics{outcome}{parallel}
\end{align*}
\end{theorem}

\begin{definition}[Parallel Semantics]\labdef{parallel-semantics}
  \begin{align*}
    \parallelsemanticsnoparam\DefeQ& \parallelabstraction(\outcomesemanticsnoparam)
  \end{align*}
\end{definition}

\begin{theorem}\labthm{parallel-validation}
  \begin{align*}
    \outcomesemantics \subseteq \unused \IfF \parallelsemantics \subseteq \parallelabstraction(\unused)
  \end{align*}
\end{theorem}

\subsection{Abstract Domains for Neural Network Analysis}

\paragraph{Boxes}

\paragraph{Symbolic Constant Propagation}

\paragraph{DeepPoly}

\paragraph{Neurify}

\paragraph{Convex Polyhedra}

\subsection{Reduced Product}
