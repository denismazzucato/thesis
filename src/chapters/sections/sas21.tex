\setchapterpreamble[u]{\margintoc}


\chapter{Quantitative Verification for Neural Networks}
\labch{quantitative-fairness}

In this chapter, we introduce feed-forward deep neural networks classifiers.
We define two new quantitative impact quantifiers: the \changesname{} impact quantifier, which targets the repetitions of changes in the outcome, and the \qlibraname{} impact quantifier, which measures the amount of unused input space. We then present the abstract implementation of the two impact quantifiers, respectively called $\abstractchangesname$ and $\abstractqlibraname$, and we show how to validate the $\defbound$-bounded impact property for both quantifiers. Finally, we present the parallel implementation $\parallelqlibraname$ for efficient neural network verification of the $\qlibraname$ impact quantifier.
This chapter is based on the work presented at the 28th Static Analysis Symposium (SAS) 2021~\cite{Mazzucato2021}.
The next chapter will present the experimental evaluation for the quantitative verification of neural networks.


\frenchdiv

\emph{Dans ce chapitre, nous introduisons les classificateurs de réseaux neuronaux profonds feed-forward. Nous définissons deux nouvelles notions d'impact quantitatif : la notion d'impact \changesname{}, qui cible les répétitions de changements dans le résultat, et la notion d'impact \qlibraname{}, qui mesure la quantité d'espace d'entrée inutilisé. Nous présentons ensuite l'implémentation abstraite des deux notions d'impact, respectivement appelées $\abstractchangesname$ et $\abstractqlibraname$, et nous montrons comment valider la propriété d'impact bornée par $\defbound$ pour les deux notions. Enfin, nous présentons l'implémentation parallèle $\parallelqlibraname$ pour une vérification efficace des réseaux neuronaux de la notion d'impact \qlibraname{}. Ce chapitre est basé sur le travail présenté au 28e Symposium d'Analyse Statique (SAS) 2021~\sidecite{Mazzucato2021}. Le prochain chapitre présentera l'évaluation expérimentale pour la vérification quantitative des réseaux neuronaux.}

% \input{src/chapters/extensional/sas21/overview}
% \input{src/chapters/extensional/sas21/changes}
% \input{src/chapters/extensional/sas21/qlibra}
% \input{src/chapters/extensional/sas21/evaluation-changes}
% \input{src/chapters/extensional/sas21/evaluation-qlibra}

\section{Neural Networks}
\labsec{neural-networks}

This section introduces the computational model of a feed-forward deep neural network and how it can be used for classification purposes.
\subsection{Feed-Forward Deep Neural Networks}

A \emph{feed-forward deep neural network} is a directed acyclic graph where each node represents a neuron, and each edge represents a connection between neurons. The nodes are organized into layers: the first layer is the input layer ($\inputlayer$), the last layer is the output layer ($\outputlayer$), and the layers in between are called hidden layers ($\hiddenlayers$). Each layer $\hiddenlayer{i}$, for $1 \le i \le \lastlayerindex$, consists of $\cardinalitynospaces{\hiddenlayer{i}}$ nodes and is connected to the previous layer $\hiddenlayer{i-1}$ through a weight matrix $\weightmatrix{i}$ of size $\cardinalitynospaces{\hiddenlayer{i}}\times\cardinalitynospaces{\hiddenlayer{i-1}}$ and a bias vector $\biasvector{i}$ of size $\cardinalitynospaces{\hiddenlayer{i}}$.

The entire set of nodes in the network is denoted by $\networknodes$, and the set of nodes in layer $\hiddenlayer{i}$ is denoted by $\networknodesinlayer{i}$. The $j$-th node in layer $\hiddenlayer{i}$ is denoted by $\node$. In this thesis, we focus on feed-forward neural networks used for classification tasks, where each node in the output layer represents a class, and the output of the network is the class corresponding to the node in the output layer with the highest value. The network has $\cardinalitynospaces{\outputlayer}$ target classes in total.

An input value is a $\cardinalitynospaces{\inputlayer}$-dimensional vector $\networknodesinlayer{0}$. For simplicity, in the rest of the chapter, we assume that the input values are normalized to the interval $[0,1]$. The value of each node in hidden layers is computed by applying an activation function to the weighted sum of the values of the nodes in the previous layer and the bias. The activation function is usually a non-linear function; in this work, we consider the \relu{} activation function, defined as $\relu(x) = \max(0, x)$. Thus, the value of node $\node$ in layer $\hiddenlayer{i}$ is computed as:
\begin{align*}
\node = \relu\left(
\sum_{k=1}^{\cardinalitynospaces{\hiddenlayer{i-1}}} \weight_{j,k}^{i} \node[i-1][k] + \bias_{j}^{i}
\right)
\end{align*}
where $\weight_{j,k}^{i}$ is the weight connecting node $\node[i-1][k]$ in layer $\hiddenlayer{i-1}$ to node $\node$ in layer $\hiddenlayer{i}$, and $\bias_{j}^{i}$ is the bias of node $\node$ in layer $\hiddenlayer{i}$. Weights and biases are learned during the training phase of the network. In the following, we consider networks that have been previously trained.

For the rest of this chapter, the letter $\defmodel$ denotes a neural network model. The trace semantics of a neural network model is denoted by $\tracesemanticsnoparam\semanticsof{\defmodel}\in\tracetype$.
Note that, for the models we consider in this thesis, we have that the output of the network is deterministic for each input configuration, and that all the traces terminate after a specific number of steps, which is exactly the number of nodes in the network.

\subsection{Classification Task}

In this thesis, we focus on feed-forward neural networks used for classification tasks. Given an input vector, the network classifies it into one of the target classes, each represented by a node in the output layer. The network classifies an input vector by computing the values of the nodes in the output layer and then selecting the class associated with the node having the highest value, \ie, $\argmax_{j} \node[\lastlayerindex][j]$.

Building on the quantitative framework of \nrefch{quantitative-input-data-usage}, the classification task is formalized using the output observer $\outputobs$ (\refdef*{output-observer}), which maps the output values of the network to the target classes. The output observer returns $1$ if the given node (\ie, a variable of the neural network) corresponds to the target class (\ie, nodes in the output layer) with the highest value, and $0$ otherwise. Thus, when the output state of a trace from the network computation is observed by $\outputobs$, the only non-zero value corresponds to the target class. Formally, the output observer is defined as:
\marginnote{
  $\networknodesinlayer\outputlayer$ denotes the set of nodes in the output layer of a neural network.
}
\begin{align*}
\outputobs(\defstate) \DefeQ \lambda \texttt{y}.\spacer
\begin{cases}
1 & \text{if } \texttt{y} \spacearound{=} \argmax_{\node \in\networknodesinlayer\outputlayer} \defstate(\node) \\
0 & \text{otherwise}
\end{cases}
\end{align*}
where $\texttt{y}$ is a node (not necessarily in the output layer) of the network.
This characterization shows that two different states of the network can be distinguished by the output observer if and only if they are associated with different target classes, thus yielding different outcomes.

\begin{example}
Consider a simple neural network with two input and two output variables. The weights and biases connecting the input to the output layer are irrelevant for this example. Therefore, the network states are defined as $\state = \setdef{\langle x_{0, 0}, x_{0, 1}, x_{1, 0}, x_{1, 1} \rangle}{x_{0, 0}, x_{0, 1}, x_{1, 0}, x_{1, 1} \in [0,1]}$.
Given, for instance, the neural network state $\defstate=\langle 0, 1, 0, 1 \rangle$, the output observer $\outputobs$ returns the state $\outputobs(\defstate) = \langle 0, 0, 0, 1 \rangle$ to indicate that the target class is the second one, corresponding to $x_{1, 1}$. Given the network state $\langle 0.5, 1, 0.7, 0.2 \rangle$, the output observer returns the state $\outputobs(\defstate) = \langle 0, 0, 1, 0 \rangle$ to indicate that the target class is the first one (the first two variables are the input variables), and so on.
\end{example}

Note that the output observer always maps the values of neural network nodes that do not belong to the output layer to zero.

\subsection{Neural Network Abstract Analysis}
\labsec{neural-network-abstract-analysis}

Abstract interpretation of a neural network model involves propagating an abstract element through each neuron of the network, over-approximating the possible states the model may reach. The analysis can be performed in two directions: forward, from the input layer to the output layer, or backward, from the output layer to the input layer.
The networks considered in this thesis are acyclic, meaning that the abstract iterator always reaches the fixpoint after a finite number of iterations, without the need to traverse the same node multiple times.
The forward analysis starts with an abstract element representing the input values and returns an abstract element representing an over-approximation of the reachable target classes. Conversely, the backward analysis begins with an abstract element representing the target classes and returns an abstract element representing an over-approximation of the input values that may lead to one of the given target classes.

When a neural network processes an input vector, it computes the value of each hidden node via an affine transformation followed by an activation function, in our case, the \relu{} function. The \relu{} function sets negative values to zero and leaves positive values unchanged. A node is said to be \emph{active} if its value is positive and \emph{inactive} otherwise. This flag is called the node's \emph{activation status}. An \emph{activation pattern} fixes the activation status of all nodes in the network, inducing a \emph{path} in the neural network from
regions in the input space where the network behaves similarly.


An \emph{abstract activation pattern} is a partial activation pattern that assigns a fixed activation status only to a subset of \relu{} nodes, thus representing a set of possible activation patterns. \relu{} nodes with an unknown activation status are those whose corresponding flag does not have a fixed value. (Abstract) Activation patterns are valuable for the static analysis of networks as the knowledge of a node status can prune the search space, reducing computational cost. Specifically, the \relu{} function is simpler to handle when the activation status is known: it behaves as the identity function for active nodes and as a constant zero for inactive nodes. Abstract activation patterns can be discovered from a forward analysis of the network: if the over-approximation of input values to a node is greater than zero, the node is considered active; if less than zero, inactive; otherwise, unknown.


\begin{marginalgorithm}
  \caption{Forward analysis of neural networks.}
  \labalg{forward-neural-networks}
\begin{lstlisting}[
  language=algorithm,
  style=algorithm,
  escapechar=\%,
  ]
Forward%${}_{\defmodel,\abstractdomain}$%(%$\defabstractvalue$%):
  %$\defabstractactivationpattern$% = %$\emptyset$%
  for %$i$% = 1 up to %$\lastlayerindex$% do
    for %$j$% = 1 up to %$\cardinalitynospaces{\hiddenlayer{i}}$% do
      %$\defabstractvalue$% = %$\abstractdomainforwardaffine\semanticsof{\node}\defabstractvalue$%
      %$\defabstractvalue$%, %$\defabstractactivationpattern$% = %$\abstractdomainforwardrelu\semanticsof{\node}\defabstractvalue$%
  return %$\defabstractvalue$%, %$\defabstractactivationpattern$%
\end{lstlisting}
\end{marginalgorithm}

\marginnote{
  In the algorithm above (\cf{} $\abstractdomainforward_{\defmodel, \abstractdomain}$), $\lastlayerindex$ is the number of layers of the neural network $\defmodel$, $\cardinalitynospaces{\hiddenlayer{i}}$ is the number of nodes in layer $i$, and $\node$ is the $j$-th node in layer $i$.
  Even though not explicitly written, the handler for affine and activation functions (respectively, $\abstractdomainforwardaffine$ and $\abstractdomainforwardrelu$) employs the abstract domain $\abstractdomain$.
}


Given an abstract domain $\abstractdomain$ with sound forward and backward operators for affine and \relu{} operations, the forward analysis can be defined as a network computation using the abstract sound counterparts of the concrete operators. \refalg{forward-neural-networks} illustrates a forward analysis of neural network models, parameterized by the abstract domain $\abstractdomain$ and an abstract element $\defabstractvalue$ representing the input values. The abstract operator $\abstractdomainforwardaffine$ computes the result $\defabstractvalue$ of the analysis of the
affine transformation performed on a given node, while $\abstractdomainforwardrelu$ additionally handles the \relu{} activation function and determines the node's activation status $\defabstractactivationpattern$.

\begin{marginalgorithm}
  \caption{Backward analysis of neural networks.}
  \labalg{backward-neural-networks}
\begin{lstlisting}[
  language=algorithm,
  style=algorithm,
  escapechar=\%,
  ]
Backward%${}_{\defmodel,\abstractdomain, \defabstractactivationpattern}$%(%$\defabstractvalue$%):
  for %$i$% = %$\lastlayerindex$% down to 1 do
    for %$j$% = %$\cardinalitynospaces{\hiddenlayer{i}}$% down to 1 do
      %$\defabstractvalue$% = %$\abstractdomainbackwardrelu\semanticsof{\node}\defabstractvalue$%
      %$\defabstractvalue$% = %$\abstractdomainbackwardaffine\semanticsof{\node}\defabstractvalue$%
  return %$\defabstractvalue$%
\end{lstlisting}
\end{marginalgorithm}
\marginnote{
  Similarly to the notes of the forward analysis, in the algorithm above (\cf{} $\abstractdomainbackward_{\defmodel, \abstractdomain}$), $\lastlayerindex$ is the number of layers of the neural network $\defmodel$, $\cardinalitynospaces{\hiddenlayer{i}}$ is the number of nodes in layer $i$, and $\node$ is the $j$-th node in layer $i$.
  Even though not explicitly written, the backward handler for affine and activation functions (respectively, $\abstractdomainbackwardaffine$ and $\abstractdomainbackwardrelu$) employs the abstract domain $\abstractdomain$.
}

Conversely, a backward analysis of neural networks replaces concrete operators with their abstract sound \emph{backward} counterparts. This analysis is also parameterized by the abstract domain $\abstractdomain$ and an abstract element $\defabstractvalue$ representing the target classes.
\refalg{backward-neural-networks} shows the backward analysis of neural network models, where the abstract operator $\abstractdomainbackwardaffine$ computes the result $\defabstractvalue$ of the analysis of the
backward affine transformation performed on a given node, and $\abstractdomainbackwardrelu$ manages the \relu{} activation function given the node's activation status $\defabstractactivationpattern$.




Both analyses are sound, meaning they over-approximate the possible states the network may reach.
Before formally showing the soundness of the forward and backward analyses, we introduce the forward concretization $\forwardconcretization$.
Similar to the backward concretization $\backwardconcretization$, \cf{} \refdef*{backward-concretization}, it is defined as:
\[
  \forwardconcretization(\forwardsemanticsnoparam)\defabstractvalue \DefeQ
  \setof{\setdef{
    \inputoutputtuple{\defstate}
  }{
    \retrieveoutput{\defstate}\in\abstractdomainconcretization(\forwardsemanticsnoparam(\defabstractvalue))\land\retrieveinput{\defstate}\in\abstractdomainconcretization(\defabstractvalue)
    }
  }
  \]

\begin{lemma}[Soundness of Forward Neural Network Analysis]\lablemma{soundness-forward-network}
  Given a neural network model $\defmodel$ and an abstract domain $\abstractdomain$ with sound forward operators for affine and \relu{} operations, the forward analysis of the network is sound whenever it holds that:
  \[
    \reduceinit[\outputsemanticsnoparam\semanticsof{\defmodel}]{\abstractdomainconcretization(\defabstractvalue)} \SubseteQ \forwardconcretization(\abstractdomainforward_{\defmodel, \abstractdomain})\defabstractvalue
  \]
  \end{lemma}
\begin{proof}
  Trivially follows from the definition of the forward concretization and the soundness of the network operators.
\end{proof}
\marginnote{
  The symbol $\reduceinit[\outputsemanticsnoparam\semanticsof{\defmodel}]{\abstractdomainconcretization(\defabstractvalue)}$ denotes the reduction of the semantics $\outputsemanticsnoparam\semanticsof{\defmodel}$ to the input states $\abstractdomainconcretization(\defabstractvalue)$, \cf{} \refdef{input-reduction}.
  }
\siderefbox{def}{input-reduction}

The next result instead shows the soundness of the backward analysis of neural networks.

\begin{lemma}[Soundness of Backward Neural Network Analysis]\lablemma{soundness-backward-network}
  Given a neural network model $\defmodel$ and an abstract domain $\abstractdomain$ with sound backward operators for affine and \relu{} operations, the backward analysis of the network is sound whenever it holds that:
  \[
    \reduce[\outputsemanticsnoparam\semanticsof{\defmodel}]{\abstractdomainconcretization(\defabstractvalue)} \SubseteQ \backwardconcretization(\abstractdomainbackward_{\defmodel, \abstractdomain})\defabstractvalue
  \]
  \end{lemma}
\marginnote{
  The symbol $\reduce[\outputsemanticsnoparam\semanticsof{\defmodel}]{\abstractdomainconcretization(\defabstractvalue)}$ denotes the reduction of the semantics $\outputsemanticsnoparam\semanticsof{\defmodel}$ to the output states $\abstractdomainconcretization(\defabstractvalue)$, \cf{} \refdef{output-reduction}.
  }
\siderefbox{def}{output-reduction}
\begin{proof}
  Trivially follows from the definition of the backward concretization and the soundness of the network operators.
\end{proof}


\subsection{Abstract Domains for Neural Network Analysis}


Different abstract domains can be used for the forward analysis of neural networks.
The choice of the abstract domain depends on the trade-off between precision and scalability.
Here, we present four abstract domains: \boxes{} \cite{Cousot1976,Hickey2001}, \symbolic{} \cite{Wang2018b,Min_e2006b}, \deeppoly{} \cite{Singh2019}, and \neurify{} \cite{Wang2018a}. Additionally, we propose a generic reduced product domain construction, called \reducedproduct, to combine any of these domains together.


\paragraph{Boxes}


The \boxes{} domain simply uses interval arithmetic \sidecite{Cousot1976, Hickey2001} to compute concrete lower and upper bound estimations $l$ and $u$ for the value of each neuron \texttt{x} in the neural network.


\paragraph{Symbolic Constant Propagation}

\begin{marginfigure}
  \centering
  \begin{tikzpicture}[scale=0.9]
    % x and y axes
    \draw[->, thick] (0,0) -- (5,0) node[above]{$x$};
    \draw[->, thick] (3,-0.5) -- (3,2) node[right]{\footnotesize $\relu(x)$};

    % lower and upper bound label on x axis
    \draw (0.5,0) -- (0.5,-0.2) node[below]{$l$};
    \draw (4.5,0) -- (4.5,-0.2) node[below]{$u$};

    % upper bound on y axis
    \draw (3,1.5) -- (3.2,1.5) node[above right]{$u$};

    % box around the function, fill with opaque \green and the contour in \green
    \fill[color=seabornBlue, opacity=0.5] (0.5,0) -- (4.5,0) -- (4.5,1.5) -- (0.5,1.5) -- cycle;
    \draw[color=seabornBlue, ultra thick] (0.5,0) -- (4.5,0) -- (4.5,1.5) -- (0.5,1.5) -- cycle;

    % label above the rectangle on the left: $\relu(x) \le u$
    \node[above] at (1,1.5) {\footnotesize $\relu(x) \le u$};

    % label above the rectangle below: $0 \ge \relu(x)$
    \node[below] at (2,0) {\footnotesize $0 \le \relu(x)$};
  \end{tikzpicture}
  \caption{Na\"ive convex approximation of a \relu{} activation function.}
  \labfig{naive}
\end{marginfigure}

The \symbolic{} domain \sidecite{Wang2018b} combines \boxes{} with symbolic constant propagation \sidecite{Min_e2006b}: in addition to being bounded by concrete lower and upper bounds, the value of each neuron \texttt{x} is represented symbolically as a linear combination of the input neurons and the value of the non-fixed \relu{} nodes in previous layers. Specifically, given \texttt{x} bounded by $l < 0$ and $u > 0$, $\relu(x)$ is represented by a fresh symbolic variable bounded by $0$ and $u$ (\cf{} \reffig{naive}). By retaining variable dependencies, symbolic representations yield a tighter over-approximation of the value of each neuron in the network.

\begin{marginfigure}
  \centering
  \begin{tikzpicture}[scale=0.9]
    % x and y axes
    \draw[->, thick] (0,0) -- (5,0) node[above]{$x$};
    \draw[->, thick] (3,-0.5) -- (3,2) node[right]{\footnotesize $\relu(x)$};

    % lower and upper bound label on x axis
    \draw (0.5,0) -- (0.5,-0.2) node[below]{$l$};
    \draw (4.5,0) -- (4.5,-0.2) node[below]{$u$};

    % upper bound on y axis
    \draw (3,1.5) -- (2.8,1.5) node[above left]{$u$};


    % upper and lower lines
    \draw[dashed] (3,1.5) -- (4.5,1.5);

    % box around the function, fill with opaque \green and the contour in \green
    \fill[color=seabornBlue, opacity=0.5] (0.5,0) -- (4.5,0) -- (4.5,1.5) -- cycle;
    \draw[color=seabornBlue, ultra thick] (0.5,0) -- (4.5,0) -- (4.5,1.5) -- cycle;

    % sloped label with an angle of 30 degrees above the rectangle: $\relu(x) \le u$
    \node[rotate=21] at (1.6,0.8) {\footnotesize $\relu(x) \le \frac{u(x-l)}{u-l}$};
    \node[below] at (2,0) {\footnotesize $0 \le \relu(x)$};
  \end{tikzpicture}
  \begin{tikzpicture}[scale=0.9]
    % x and y axes
    \draw[->, thick] (0,0) -- (5,0) node[above]{$x$};
    \draw[->, thick] (3,-3) -- (3,2) node[right]{\footnotesize $\relu(x)$};

    % lower and upper bound label  lefton x axi
    \draw (0.5,0) -- (0.5,-0.2) node[below left]{$l$};
    \draw (4.5,0) -- (4.5,-0.2) node[below]{$u$};

    % upper bound on y axis
    \draw (3,-2.5) -- (3.2,-2.5) node[right]{$l$};
    \draw (3,1.5) -- (2.8,1.5) node[above left]{$u$};

    % upper and lower lines
    \draw[dashed] (4.5,0) -- (4.5,1.5);
    \draw[dashed] (3,1.5) -- (4.5,1.5);
    \draw[dashed] (0.5,-2.5) -- (3,-2.5);

    % box around the function, fill with opaque \green and the contour in \green
    \fill[color=seabornBlue, opacity=0.5] (0.5,0) -- (0.5,-2.5) -- (4.5,1.5) -- cycle;
    \draw[color=seabornBlue, ultra thick] (0.5,0) -- (0.5,-2.5) -- (4.5,1.5) -- cycle;

    % label above the rectangle on the left: $\relu(x) \le u$
    \node[rotate=21] at (1.6,0.8) {\footnotesize $\relu(x) \le \frac{u(x-l)}{u-l}$};


    % label above the rectangle below: $0 \ge \relu(x)$
    \node[rotate=45] at (1.8,-1.6) {\footnotesize $x \le \relu(x)$};
  \end{tikzpicture}
  \caption{\deeppoly's convex approximation of a \relu{} activation function.}
  \labfig{deeppoly}
\end{marginfigure}




\paragraph{DeepPoly}

The \deeppoly{} domain \sidecite{Singh2019} associates to each neuron $x$ of a neural network concrete lower and upper bounds $l$ and $u$ as well as symbolic bounds expressed as linear combinations of neurons in the preceding layer of the network.
%
The concrete bounds are computed by back-substitution of the symbolic bounds up to the input layer. Non-fixed \relu{} nodes are overapproximated by partially retaining dependencies with preceding neurons using the tighter convex approximation between those shown in \reffig{deeppoly} (\ie, above when $u \leq -l$, and below otherwise).

\begin{marginfigure}
  \centering
  \begin{tikzpicture}[scale=0.9]
    % x and y axes
    \draw[->, thick] (0,0) -- (5,0) node[above]{$x$};
    \draw[->, thick] (3,0) -- (3,2) node[right]{\footnotesize $\relu(x)$};

    % lower and upper bound label on x axis
    \draw (0.5,0) -- (0.5,-0.2) node[below left]{$l$};
    \draw (4.5,0) -- (4.5,-0.2) node[below]{$u$};

    % upper bound on y axis
    \draw (3,1.5) -- (2.8,1.5) node[above left]{$u$};

    % upper and lower lines
    \draw[dashed] (4.5,0) -- (4.5,1.5);
    \draw[dashed] (3,1.5) -- (4.5,1.5);

    % box around the function, fill with opaque \green and the contour in \green
    \fill[color=seabornBlue, opacity=0.5] (0.5,0) -- (0.5,-0.95) -- (4.5,0.55) -- (4.5,1.5) -- cycle;
    \draw[color=seabornBlue, ultra thick] (0.5,0) -- (0.5,-0.95) -- (4.5,0.55) -- (4.5,1.5) -- cycle;

    % label above the rectangle on the left: $\relu(x) \le u$
    \node[rotate=21] at (1.6,0.8) {\footnotesize $\relu(x) \le \frac{u(x-l)}{u-l}$};


    % label above the rectangle below: $0 \ge \relu(x)$
    \node[rotate=21] at (2.4,-0.7) {\footnotesize $\frac{ux}{x-l} \le \relu(x)$};
  \end{tikzpicture}
  \caption{\neurify's convex approximation of a \relu{} activation function.}
  \labfig{neurify}
\end{marginfigure}

\paragraph{Neurify}

The \neurify{} domain \sidecite{Wang2018a} similarly maintains symbolic lower and upper bounds $low$ and $up$ for each neuron $x$ of neural network. Unlike \deeppoly, concrete lower and upper bounds are computed for \emph{each} symbolic bound: $l_{low}$ and $u_{low}$ for the symbolic lower bound, and $l_{up}$ and $u_{up}$ for the symbolic upper bound.
%
The over-approximation of non-fixed \relu{} nodes is done \emph{independently} for each symbolic bound, \ie, for the $low$ bound if $l_{low} < 0 < u_{low}$, and for the $up$ bound if $l_{up} < 0 < u_{up}$.
%
\reffig{neurify} shows the approximation for $l = l_{low} = l_{up}$ and $u = u_{low} = u_{up}$. In general, the slope of the symbolic constraints will differ through successive approximation steps.

\paragraph{Reduced Product}

Finally, as part of the thesis contributions, the \emph{Product Builder} \sidecite{Mazzucato2021} provides a parametric interface for constructing the product of the above domains \sidecite{Cousot1979a}.
The reduction function consists in an exchange of concrete bounds between domains. In particular, this allows determining tighter lower and upper bound estimations for each neuron in the network and thus reducing the over-approximation error introduced by the \relu{} nodes.
New abstract domains only need to implement the interface to share bounds information to enable their combination with other domains by the Product Builder.



\section{Quantitative Impact Quantifiers}
\labsec{quantitative-impact-quantifiers}

This section elaborates on quantifiers that can be used to measure the influence of input variables on the output of a neural network.
Namely, we introduce the \changesname{} and \qlibraname{} impact quantifiers, which are specifically designed to handle neural network models.

\subsection{The \changesname{} Impact Quantifier}[\changesname]
\labsec{changes-impact-quantifier}


The \changesname{} impact quantifier is designed to overcome the limitations of the impact quantifiers defined in the previous chapter when applied in the context of neural networks.
Indeed, for neural networks, all possible input variations typically lead to all possible outcomes (\ie, classifications).
Therefore, the impact quantifiers defined in the previous chapter would not be useful as they measure the impact of input variables based on the number (\eg, \outcomesname{}) or the magnitude (\eg, \rangename{}) of the output values; even \qusedname{} would be meaningless in this setting as all the classification targets are often reachable by all the input values.
% Therefore, these metrics would not provide any meaningful information about how the input variables influence the network's outcome.

\begin{example}
  \begin{marginfigure}
    \centering
  \begin{tikzpicture}[scale=0.9]
    % Grid
    % \draw[help lines, color=gray!30, dashed] (-0.1,-0.1) grid (4.1,4.1);
    % x-axis ticks
    % \foreach \x in {-4,-3,-2,-1,0,1,2,3,4}
    %     \draw (\x+5,0.1) -- (\x+5,-0.1) node[below] {\x};
    % % y-axis ticks
    % \foreach \y in {1,2,3}
    %     \draw (0.1,\y) -- (-0.1,\y) node[left] {\y};
    % Polyhedra
    \fill[color=seabornYellow, opacity=0.5] (1,1.75) -- (2,2.75) -- (2.5,1.75) -- (3,2.75) -- (3,4) -- (2,4) -- cycle;
    % \draw[color=seabornYellow, ultra thick] (1,1.75) -- (2,2.75) -- (3,2.75) -- (3,4) -- (2,4) -- cycle;
    % Polyhedra
    \fill[color=seabornGreen, opacity=0.5] (0,2.25) -- (0.5,2.25) -- (1,1.75) -- (2,4) -- (0,4) -- cycle;
    \draw[color=seabornGreen, pattern=north west lines, pattern color=seabornGreen, ultra thick] (0,2.25) -- (0.5,2.25) -- (1,1.75) -- (2,4) -- (0,4) -- cycle;
    \fill[color=seabornGreen, opacity=0.5] (3,2.75) -- (4,1.5) -- (4,4) -- (3,4) -- cycle;
    \draw[color=seabornGreen, pattern=north west lines, pattern color=seabornGreen, ultra thick] (3,2.75) -- (4,1.5) -- (4,4) -- (3,4) -- cycle;
    % Polyhedra
    \fill[color=seabornRed, opacity=0.5] (0,0) -- (4,0) -- (4,1.5) -- (3,2.75) -- (2.5,1.75) -- (2,2.75) -- (1,1.75) -- (0.5,2.25) -- (0,2.25) -- cycle;
    \draw[color=seabornRed, pattern=dots, pattern color=seabornRed, ultra thick] (0,0) -- (4,0) -- (4,1.5) -- (3,2.75) -- (2.5,1.75) -- (2,2.75) -- (1,1.75) -- (0.5,2.25) -- (0,2.25) -- cycle;
    % Nodes
    % \fill[color=seabornRed] (0+1,0+1) circle[radius=2pt];
    % \node[above left] at (0+1,0+1) {$3$};
    % x-axis
    \draw[->,ultra thick] (0,0)--(4.3,0) node[below]{$x$};
    \draw[ultra thick] (0,4)--(4,4);
    % y-axis
    \draw[->,ultra thick] (0,0)--(0,4.3) node[left]{$y$};
    \draw[ultra thick] (4,0)--(4,4);

    \draw[dashed] (0,2.1) node[left] {$y_0$} -- (4,2.1);
    \draw[dashed] (1.75,0) node[below] {$x_0$} -- (1.75,4);
    % little dot in the intersection x_0 y_0
    \fill[color=black] (1.75,2.1) circle[radius=2pt];
  \end{tikzpicture}
    \caption{Input space with two input variables ($x$ and $y$) and three possible outcomes (stripped green, plain yellow, dotted red).
    }
    \labfig{irregular}
  \end{marginfigure}
  Consider a simple neural network with two input variables $x$ and $y$, and three possible outcomes, denoted by the colors green, yellow, and red.
  A potential input space of such a model $\defmodel$ is represented in \reffig{irregular}, where each outcome is reachable by the portion of the input space that is colored with the corresponding color.
  In this case, the \outcomesname{} impact quantifier would not be useful as all the outcomes are reachable by perturbations of any input value. Consider for example the point $(x_0, y_0)$ in the input space of outcome red, by applying a perturbation of one axis, the outcome can change to green or yellow, for both axes. Thus, the impact of both $x$ and $y$ is maximal, \cf{} $\outcomesname_x(\semanticsof\defmodel) = \outcomesname_y(\semanticsof\defmodel) = 3$.
  A similar reasoning applies to the \rangename{} or \qusedname{} impact quantifiers, hence neither of them provides any insight.
\end{example}

On the other hand, we notice that one way to discriminate which input variable is more impactful is to consider the number of times the classification changes when the input variables are modified.

\begin{example}
  Consider the input point $(x_0, y_0)$ in the input space of the neural network in \reffig{irregular}.
  By applying a perturbation of the $x$-axis, the outcome can change to green or yellow multiple times, a higher number of changes in the outcome is observed compared to the $y$-axis.
\end{example}


The \changesname{} impact quantifier is designed to count how many times the network outcome changes by modification in the value of the input variables $\definputvariables$.
In contrast with the previously defined $\outcomesname$ impact quantifier, in this case, we consider changes in the outcome \emph{with repetitions}: if two different variations in $\definputvariables$ result in the same change in outcome, it counts as double change.
The higher the number of changes in the outcome, the greater the influence on the program outcome.
Therefore, this quantifier demonstrates its effectiveness when the same outcomes are reachable by multiple variations.
Such situation often arises in the context of neural networks, where generally all the possible input variations lead to every possible outcome.
The design of this quantifier builds on this observation.
Thus, counting the repetitions is a potential solution to define a meaningful impact definition for neural networks.

\begin{marginfigure}
  \centering
  \begin{tikzpicture}[scale=0.9]
    % Grid
    % \draw[help lines, color=gray!30, dashed] (-0.1,-0.1) grid (4.1,4.1);
    % x-axis ticks
    % \foreach \x in {-4,-3,-2,-1,0,1,2,3,4}
    %     \draw (\x+5,0.1) -- (\x+5,-0.1) node[below] {\x};
    % % y-axis ticks
    % \foreach \y in {1,2,3}
    %     \draw (0.1,\y) -- (-0.1,\y) node[left] {\y};
    % Polyhedra
    \fill[color=seabornYellow, opacity=0.5] (1,1.75) -- (2,2.75) -- (2.5,1.75) -- (3,2.75) -- (3,4) -- (2,4) -- cycle;
    % \draw[color=seabornYellow, ultra thick] (1,1.75) -- (2,2.75) -- (3,2.75) -- (3,4) -- (2,4) -- cycle;
    % Polyhedra
    \fill[color=seabornGreen, opacity=0.5] (0,2.25) -- (0.5,2.25) -- (1,1.75) -- (2,4) -- (0,4) -- cycle;
    \draw[color=seabornGreen, pattern=north west lines, pattern color=seabornGreen, ultra thick] (0,2.25) -- (0.5,2.25) -- (1,1.75) -- (2,4) -- (0,4) -- cycle;
    \fill[color=seabornGreen, opacity=0.5] (3,2.75) -- (4,1.5) -- (4,4) -- (3,4) -- cycle;
    \draw[color=seabornGreen, pattern=north west lines, pattern color=seabornGreen, ultra thick] (3,2.75) -- (4,1.5) -- (4,4) -- (3,4) -- cycle;
    % Polyhedra
    \fill[color=seabornRed, opacity=0.5] (0,0) -- (4,0) -- (4,1.5) -- (3,2.75) -- (2.5,1.75) -- (2,2.75) -- (1,1.75) -- (0.5,2.25) -- (0,2.25) -- cycle;
    \draw[color=seabornRed, pattern=dots, pattern color=seabornRed, ultra thick] (0,0) -- (4,0) -- (4,1.5) -- (3,2.75) -- (2.5,1.75) -- (2,2.75) -- (1,1.75) -- (0.5,2.25) -- (0,2.25) -- cycle;
    % Nodes
    % \fill[color=seabornRed] (0+1,0+1) circle[radius=2pt];
    % \node[above left] at (0+1,0+1) {$3$};
    % x-axis
    \draw[->,ultra thick] (0,0)--(4.3,0) node[below]{$x$};
    \draw[ultra thick] (0,4)--(4,4);
    % y-axis
    \draw[->,ultra thick] (0,0)--(0,4.3) node[left]{$y$};
    \draw[ultra thick] (4,0)--(4,4);

    % rectangle with label A on a corner
    \draw[ultra thick] (0.5,0.2) -- (2,0.2) -- (2,1.5) -- (0.5,1.5) -- cycle;
    \node[above right] at (0.5,0.2) {$A$};


    % rectangle with label B on a corner
    \draw[ultra thick] (2,3.5) -- (3.7,3.5) -- (3.7,2.5) -- (2,2.5) -- cycle;
    \node[below left] at (3.7,3.5) {$B$};

  \end{tikzpicture}
    \caption{Example of stable ($A$) and unstable ($B$) regions for the variable $x$.
    }
    \labfig{stable}
  \end{marginfigure}

In practice, the \changesname{} impact definition considers variations in the value of input configurations that do not belong to the same \emph{stable region}.
In words, a stable region is a subset of the input space where the network output remains stable for all possible perturbations of the input variables.
\reffig{stable} shows an example of a stable region (\cf{} region $A$) for the variable $x$, where the network output remains the same (\cf{} dotted red) for all possible variations of $x$.
Instead, in the same figure, the region $B$ is unstable for the variable $x$, where the network output changes for different variations of $x$ (for example, both the outcomes of plain yellow and stripped green belong to this region).
We first define the function $\segments$, which takes as input a set of traces $\defsetoftraces$ and an output value $\defoutput$.
This function partitions the set of traces $\defsetoftraces$ into stable subsets with respect to the input variables $\definputvariables$.
Each subset $\defsetoftraces'$ satisfies three conditions: (1) all the traces in $\defsetoftraces'$ share the same output value $\defoutput$, (2) for any two traces in $\defsetoftraces'$, there is no other trace
in $\defsetoftraces$ with an input value between the two traces leading to a different output value, and (3) the subset is maximal, that is, there is no other trace in $\defsetoftraces$ that can be added to $\defsetoftraces'$ without violating the first two conditions.
The function $\segments$ is defined as follows:


\begin{definition}[Segments of Continuous Regions]\labdef{segments}
  Let $\definputvariables\in\setof\inputvariables$ be the set of input variables of interest.
  Given a set of traces $\defsetoftraces\in\tracetype$ and an output value $\defoutput\in\stateandbottom$, the function $\segments\in\pair\tracetype\stateandbottom\to\setof\tracetype$ is defined as:
  \begin{align*}
    \segments(\defsetoftraces, \defoutput) &\DefeQ
      \setdef{
        \defsetoftraces' \in \phi(\defsetoftraces)}{
          \foralldef{\defsetoftraces''\supset\defsetoftraces'}{
            \defsetoftraces''\notin\phi(\defsetoftraces)
          }
        }\\
    \text{where } \phi(\defsetoftraces) &\DefeQ
      \setdef*{
        \defsetoftraces'\subseteq\defsetoftraces
      }{
        \forall \deftrace\in\defsetoftraces'.\spacer \outputobs(\retrieveoutput{\deftrace}) = \defoutput \LanD \\
        \forall \deftrace, \deftrace'\in\defsetoftraces'.\spacer \exists \deftrace''\in\defsetoftraces.\spacer \\
          \quad\retrieveinput{\deftrace}(\definputvariables) \le \retrieveinput{\deftrace''}(\definputvariables) \le \retrieveinput{\deftrace'}(\definputvariables) \implies \deftrace'' \in \defsetoftraces'
      }
  \end{align*}
\end{definition}

Note that, the auxiliary function $\phi$ partitions the set of traces $\defsetoftraces$ into subsets that satisfy the first two conditions: (1) all the traces in the subset share the same output value $\defoutput$, and (2) for any two traces in the subset (\cf{} $\outputobs(\retrieveoutput{\deftrace}) = \defoutput$), there is no other trace in $\defsetoftraces$ with an input value between the two traces leading to a different output value (\cf{} $\retrieveinput{\deftrace}(\definputvariables) \le \retrieveinput{\deftrace''}(\definputvariables) \le \retrieveinput{\deftrace'}(\definputvariables) \implies \deftrace'' \in \defsetoftraces'$).
The function $\segments$ then returns the maximal subsets (3) that satisfy the first two conditions.
To better illustrate how the function $\segments$ works, consider the following example.

\begin{marginfigure}
  \centering
  \begin{tikzpicture}[scale=0.9]
    % Grid
    % \draw[help lines, color=gray!30, dashed] (-0.1,-0.1) grid (4.1,4.1);
    % x-axis ticks
    % \foreach \x in {-4,-3,-2,-1,0,1,2,3,4}
    %     \draw (\x+5,0.1) -- (\x+5,-0.1) node[below] {\x};
    % % y-axis ticks
    % \foreach \y in {1,2,3}
    %     \draw (0.1,\y) -- (-0.1,\y) node[left] {\y};
    % Polyhedra
    \fill[color=seabornYellow, opacity=0.5] (1,1.75) -- (2,2.75) -- (2.5,1.75) -- (3,2.75) -- (3,4) -- (2,4) -- cycle;
    % \draw[color=seabornYellow, ultra thick] (1,1.75) -- (2,2.75) -- (3,2.75) -- (3,4) -- (2,4) -- cycle;
    % Polyhedra
    \fill[color=seabornGreen, opacity=0.5] (0,2.25) -- (0.5,2.25) -- (1,1.75) -- (2,4) -- (0,4) -- cycle;
    \draw[color=seabornGreen, pattern=north west lines, pattern color=seabornGreen, ultra thick] (0,2.25) -- (0.5,2.25) -- (1,1.75) -- (2,4) -- (0,4) -- cycle;
    \fill[color=seabornGreen, opacity=0.5] (3,2.75) -- (4,1.5) -- (4,4) -- (3,4) -- cycle;
    \draw[color=seabornGreen, pattern=north west lines, pattern color=seabornGreen, ultra thick] (3,2.75) -- (4,1.5) -- (4,4) -- (3,4) -- cycle;
    % Polyhedra
    \fill[color=seabornRed, opacity=0.5] (0,0) -- (4,0) -- (4,1.5) -- (3,2.75) -- (2.5,1.75) -- (2,2.75) -- (1,1.75) -- (0.5,2.25) -- (0,2.25) -- cycle;
    \draw[color=seabornRed, pattern=dots, pattern color=seabornRed, ultra thick] (0,0) -- (4,0) -- (4,1.5) -- (3,2.75) -- (2.5,1.75) -- (2,2.75) -- (1,1.75) -- (0.5,2.25) -- (0,2.25) -- cycle;
    % Nodes
    % \fill[color=seabornRed] (0+1,0+1) circle[radius=2pt];
    % \node[above left] at (0+1,0+1) {$3$};
    % x-axis
    \draw[->,ultra thick] (0,0)--(4.3,0) node[below]{$x$};
    \draw[ultra thick] (0,4)--(4,4);
    % y-axis
    \draw[->,ultra thick] (0,0)--(0,4.3) node[left]{$y$};
    \draw[ultra thick] (4,0)--(4,4);


    \draw[color=white] (0,-1) node[left, color=black] {$\green{\defsetoftraces'}\left\{\right.$} -- (4,-1) node[right, color=black] {$\left.\right\}$};
    \draw[color=white] (0,-1.5) node[left, color=black] {$\yellow{\defsetoftraces''}\left\{\right.$} -- (4,-1.5) node[right, color=black] {$\left.\right\}$};
    \draw[color=white] (0,-2) node[left, color=black] {$\red{\defsetoftraces'''}\left\{\right.$} -- (4,-2) node[right, color=black] {$\left.\right\}$};

    % horizontal line at y_0
    \draw[very thick] (0,2.1) node[left] {$y_0$} -- (4,2.1);

    \draw[dashed] (0, -2) -- (0, 4);
    \draw[dashed] (0.65,-2) -- (0.65,4);
    \draw[dashed] (1.15,-1.5) -- (1.15,4);
    \draw[dashed] (1.35,-2) -- (1.35,4);
    \draw[dashed] (2.3,-2) -- (2.3,4);
    \draw[dashed] (2.68,-2) -- (2.68,4);
    \draw[dashed] (3.5,-2) -- (3.5,4);
    \draw[dashed] (4,-1) -- (4,4);

    \draw[very thick] (0.65,2) -- (0.65,2.2);
    \draw[very thick] (1.15,2) -- (1.15,2.2);
    \draw[very thick] (1.35,2) -- (1.35,2.2);
    \draw[very thick] (2.3,2) -- (2.3,2.2);
    \draw[very thick] (2.68,2) -- (2.68,2.2);
    \draw[very thick] (3.5,2) -- (3.5,2.2);

    \draw[very thick, color=seabornRed] (0,-0.5) node[left, color=black] {$y_0$} -- (0.65,-0.5);
    \draw[very thick, color=seabornGreen] (0.65,-0.5) -- (1.15,-0.5);
    \draw[very thick, color=seabornYellow] (1.15,-0.5) -- (1.35,-0.5);
    \draw[very thick, color=seabornRed] (1.35,-0.5) -- (2.3,-0.5);
    \draw[very thick, color=seabornYellow] (2.3,-0.5) -- (2.68,-0.5);
    \draw[very thick, color=seabornRed] (2.68,-0.5) -- (3.5,-0.5);
    \draw[very thick, color=seabornGreen] (3.5,-0.5) -- (4,-0.5);

    \draw[very thick] (0,-0.6) -- (0,-0.4);
    \draw[very thick] (0.65,-0.6) -- (0.65,-0.4);
    \draw[very thick] (1.15,-0.6) -- (1.15,-0.4);
    \draw[very thick] (1.35,-0.6) -- (1.35,-0.4);
    \draw[very thick] (2.3,-0.6) -- (2.3,-0.4);
    \draw[very thick] (2.68,-0.6) -- (2.68,-0.4);
    \draw[very thick] (3.5,-0.6) -- (3.5,-0.4);
    \draw[very thick] (4,-0.6) -- (4,-0.4);

    \draw[very thick, color=seabornGreen] (0.65,-1) -- (1.15,-1);
    \draw[very thick, color=seabornGreen] (3.5,-1) -- (4,-1);

    \draw[very thick] (0.65,-1.1) -- (0.65,-0.9);
    \draw[very thick] (1.15,-1.1) -- (1.15,-0.9);
    \draw[very thick] (3.5,-1.1) -- (3.5,-0.9);
    \draw[very thick] (4,-1.1) -- (4,-0.9);

    \draw[very thick, color=seabornYellow] (1.15,-1.5) -- (1.35,-1.5);
    \draw[very thick, color=seabornYellow] (2.3,-1.5) -- (2.68,-1.5);

    \draw[very thick] (1.15,-1.6) -- (1.15,-1.4);
    \draw[very thick] (1.35,-1.6) -- (1.35,-1.4);
    \draw[very thick] (2.3,-1.6) -- (2.3,-1.4);
    \draw[very thick] (2.68,-1.6) -- (2.68,-1.4);

    \draw[very thick, color=seabornRed] (0,-2) -- (0.65,-2);
    \draw[very thick, color=seabornRed] (1.35,-2) -- (2.3,-2);
    \draw[very thick, color=seabornRed] (2.68,-2) -- (3.5,-2);

    \draw[very thick] (0,-2.1) -- (0,-1.9);
    \draw[very thick] (0.65,-2.1) -- (0.65,-1.9);
    \draw[very thick] (1.35,-2.1) -- (1.35,-1.9);
    \draw[very thick] (2.3,-2.1) -- (2.3,-1.9);
    \draw[very thick] (2.68,-2.1) -- (2.68,-1.9);
    \draw[very thick] (3.5,-2.1) -- (3.5,-1.9);


  \end{tikzpicture}
    \caption{Function \segments.}
    \labfig{segments}
  \end{marginfigure}

\begin{example}
  Let us consider the set of traces from the neural network in \reffig{irregular} with $y=y_0$, we call this set $\defsetoftraces$.
  The function $\segments$ partitions the set of traces $\defsetoftraces$ into three subsets, each containing the traces leading to the same output value.
  \reffig{segments} shows the three partitions graphically, respectively for the outcome green ($\green{\defsetoftraces'}$), yellow ($\yellow{\defsetoftraces''}$), and red ($\red{\defsetoftraces'''}$).
\end{example}

Formally, \changesname{} is defined as the maximum, for each input configuration, of the number of stable regions in which the output value changes.
In words, a set of input variables $\definputvariables$ has a high impact on the network outcome if the output value changes in many stable regions when the input variables are modified.


\begin{definition}[\changesname]\labdef{changes}
  Given a set of input variables of interest $\definputvariables\in\setof\inputvariables$, and an output observer $\outputobs$,
  the quantity $\changes\in\tracetype\to\Nplus$ is defined as:
  \begin{align*}
    \changes(\defsetoftraces) &\DefeQ
      \sup_{\definput\in\reducedstate}
        \sup_{\defoutput\in\stateandbottom}
          \cardinality{Q_{\definputvariables, \definput, \defoutput}} \\
    \text{where } Q_{\definputvariables, \definput, \defoutput} &\DefeQ
      \bigsetjoin_{\defoutput' \in \stateandbottom\setminus\{\defoutput\}}
        \segments(
          \setdef{\deftrace\in\defsetoftraces}{\retrieveinput\deftrace \stateeq{\inputvariableswithoutw} \definput}, \defoutput'
        )
  \end{align*}
\end{definition}
The auxiliary set $Q_{\definputvariables, \definput, \defoutput}$ contains the set of stable regions leading to an output value different from $\defoutput$.
Additionally, $\changes$ requires the set of traces $\defsetoftraces$ to be deterministic, that is, that there is only one output value for each input configuration. Such a requirement is necessary to ensure that segments are well-defined.
Otherwise, the \changesname{} impact quantifier would potentially be meaningless in such a setting.
Nevertheless, since we consider neural networks for classification tasks in this chapter, the output value is deterministic for each input configuration, and thus the set of traces $\defsetoftraces$ is deterministic.

\begin{marginfigure}
  \centering
  \begin{tikzpicture}[scale=0.9]

    \draw[very thick, color=seabornRed] (0,1) node[left, color=black] {$y_0$} -- (0.65,1);
    \draw[very thick, color=seabornGreen] (0.65,1) -- (1.15,1);
    \draw[very thick, color=seabornYellow] (1.15,1) -- (1.35,1);
    \draw[very thick, color=seabornRed] (1.35,1) -- (2.3,1);
    \draw[very thick, color=seabornYellow] (2.3,1) -- (2.68,1);
    \draw[very thick, color=seabornRed] (2.68,1) -- (3.5,1);
    \draw[very thick, color=seabornGreen] (3.5,1) -- (4,1);

    \draw[very thick] (0,0.9) -- (0,1.1);
    \draw[very thick] (0.65,0.9) -- (0.65,1.1);
    \draw[very thick] (1.15,0.9) -- (1.15,1.1);
    \draw[very thick] (1.35,0.9) -- (1.35,1.1);
    \draw[very thick] (2.3,0.9) -- (2.3,1.1);
    \draw[very thick] (2.68,0.9) -- (2.68,1.1);
    \draw[very thick] (3.5,0.9) -- (3.5,1.1);
    \draw[very thick] (4,0.9) -- (4,1.1);

    \draw[thick, ->] (0.9, 1.1) to[out=120, in=45] (0.32, 1.1);
    \draw[thick, ->] (0.9, 1.1) to[out=45, in=120] (1.28, 1.1);
    \draw[thick, ->] (0.9, 1.1) to[out=45, in=120] (1.9, 1.1);
    \draw[thick, ->] (0.9, 1.1) to[out=45, in=120] (2.5, 1.1);
    \draw[thick, ->] (0.9, 1.1) to[out=45, in=120] (3.1, 1.1);



    \draw[very thick, color=seabornRed] (0,0) node[left, color=black] {$y_0$} -- (0.65,0);
    \draw[very thick, color=seabornGreen] (0.65,0) -- (1.15,0);
    \draw[very thick, color=seabornYellow] (1.15,0) -- (1.35,0);
    \draw[very thick, color=seabornRed] (1.35,0) -- (2.3,0);
    \draw[very thick, color=seabornYellow] (2.3,0) -- (2.68,0);
    \draw[very thick, color=seabornRed] (2.68,0) -- (3.5,0);
    \draw[very thick, color=seabornGreen] (3.5,0) -- (4,0);

    \draw[very thick] (0,-0.1) -- (0,0.1);
    \draw[very thick] (0.65,-0.1) -- (0.65,0.1);
    \draw[very thick] (1.15,-0.1) -- (1.15,0.1);
    \draw[very thick] (1.35,-0.1) -- (1.35,0.1);
    \draw[very thick] (2.3,-0.1) -- (2.3,0.1);
    \draw[very thick] (2.68,-0.1) -- (2.68,0.1);
    \draw[very thick] (3.5,-0.1) -- (3.5,0.1);
    \draw[very thick] (4,-0.1) -- (4,0.1);

    \draw[thick, ->] (1.28, 0.1) to[out=120, in=45] (0.32, 0.1);
    \draw[thick, ->] (1.28, 0.1) to[out=120, in=45] (0.9, 0.1);
    \draw[thick, ->] (1.28, 0.1) to[out=45, in=120] (1.9, 0.1);
    \draw[thick, ->] (1.28, 0.1) to[out=45, in=120] (3, 0.1);
    \draw[thick, ->] (1.28, 0.1) to[out=45, in=120] (3.7, 0.1);


    \draw[very thick, color=seabornRed] (0,-1) node[left, color=black] {$y_0$} -- (0.65,-1);
    \draw[very thick, color=seabornGreen] (0.65,-1) -- (1.15,-1);
    \draw[very thick, color=seabornYellow] (1.15,-1) -- (1.35,-1);
    \draw[very thick, color=seabornRed] (1.35,-1) -- (2.3,-1);
    \draw[very thick, color=seabornYellow] (2.3,-1) -- (2.68,-1);
    \draw[very thick, color=seabornRed] (2.68,-1) -- (3.5,-1);
    \draw[very thick, color=seabornGreen] (3.5,-1) -- (4,-1);

    \draw[very thick] (0,-1.1) -- (0,-0.9);
    \draw[very thick] (0.65,-1.1) -- (0.65,-0.9);
    \draw[very thick] (1.15,-1.1) -- (1.15,-0.9);
    \draw[very thick] (1.35,-1.1) -- (1.35,-0.9);
    \draw[very thick] (2.3,-1.1) -- (2.3,-0.9);
    \draw[very thick] (2.68,-1.1) -- (2.68,-0.9);
    \draw[very thick] (3.5,-1.1) -- (3.5,-0.9);
    \draw[very thick] (4,-1.1) -- (4,-0.9);

    \draw[thick, ->] (0.32, -0.9) to[out=45, in=120] (0.9, -0.9);
    \draw[thick, ->] (0.32, -0.9) to[out=45, in=120] (1.28, -0.9);
    \draw[thick, ->] (0.32, -0.9) to[out=45, in=120] (2.5, -0.9);
    \draw[thick, ->] (0.32, -0.9) to[out=45, in=120] (3.7, -0.9);
  \end{tikzpicture}
    \caption{Function \changesname.}
    \labfig{changes}
  \end{marginfigure}

\begin{example}
  \reffig{changes} shows the \changesname{} impact quantifier applied to the input space of the neural network in \reffig{irregular}.
  The function $\changes$ returns the maximum number of stable regions in which the output value changes when the input variables are modified.
  As illustrated, for the variable $x$, this can only happen when $y=y_0$ as all the other values of $y$ lead to the same or fewer changes.
  Consider the outcomes green, yellow, and red (graphically represented by the three figures in \reffig{changes}, top to bottom), modifying the input variable $x$ may lead to 5, 5, and 4 different outcomes, respectively.
  Therefore, the \changesname{} impact quantifier for the input variable $x$ is 5.
\end{example}


\begin{lemma}[\changesname{} is Monotonic]
  \lablemma{changes-monotonic}
For all set of traces $\defsetoftraces, \defsetoftraces'\in\tracetype$, it holds that:
  \begin{align*}
    \defsetoftraces \subseteq \defsetoftraces' \ImplieS \changes(\defsetoftraces) \le \changes(\defsetoftraces')
  \end{align*}
\end{lemma}
\begin{proof}
  The proof is based on the observation that more traces in $\defsetoftraces'$ can only increase the number of stable regions.
  Hence, the number of stable regions leading to a different output (\cf{} $Q_{\definputvariables, \definput, \defoutput}$) can only increase with more traces.
  We conclude that $\changes(\defsetoftraces) \le \changes(\defsetoftraces')$.
\end{proof}

We show the validation of the $\defbound$-bounded impact property when instantiated with the $\changesname$ impact quantifier,
denoted $\boundedchanges$.

\siderefbox{def}{output-abstraction-semantics}
\begin{lemma}[$\boundedchanges$ Validation]\lablemma{changes-validation}
  \begin{align*}
    \collectingsemantics \subseteq \BOUNDEDCHANGES \IfF \outputsemantics \subseteq \outputabstraction(\dependencyabstraction(\BOUNDEDCHANGES))
  \end{align*}
\end{lemma}
\begin{proof}[Proof (Sketch)]
  The $\changesname$ impact quantifier does not consider the intermediate states, in fact, it only employs the first state in the definition of $Q_{\definputvariables, \definput, \defoutput}$ and the last one in the definition of $\segments$.
  Thus, the abstraction to dependencies does not affect the validation of the property.
  Furthermore, even handling the output abstraction at the semantic level, by abstracting output states to abstract output states, does not affect the validation of the property as the $\changesname$ impact quantifier already abstracts the output values before comparing to the given output $\defoutput$, \cf{} $\outputobs(\retrieveoutput{\deftrace}) = \defoutput$ in \nrefdef{segments}.
\end{proof}

% \reflemma{changes-monotonic} and \reflemma{changes-validation} show that the $\changesname$ impact quantifier can be used to certify that a program has impact of \emph{at most} $\defbound$, meaning that the program output changes at most $\defbound$ times when the input variables vary.
% In case of non-deterministic set of traces, the $\changesname$ impact quantifier would not be meaningful, as the number of stable regions leading to a different output value would be always infinite.


\subsection{The \qlibraname{} Impact Quantifier}[\qlibraname]
\labsec{qlibra-impact-quantifier}

The second quantifier introduced in this section is the \qlibraname{} impact quantifier, which stands for \underline{q}uantitative \underline{a}bstract \underline{u}nused.
Notably, this quantifier is designed to quantify the amount of neural network's input space that \emph{does not use} a given set of input features.
In the context of neural networks, the \qlibraname{} impact quantifier can be used to induce a quantification of the fair space of a model.
A network is fair whenever the classification determined by a model does not depend on the ``sensitive'' input variables.
In our setting, the sensitive input variables are represented by the set of input variables $\definputvariables$ and measuring the inability of the input variables $\definputvariables$ to affect the network outcome is equivalent to quantifying the amount of fair space.
To this end, we define the \qlibraname{} impact quantifier as the volume of input space that is not able to change the model classification by perturbation of the input variables $\definputvariables$.
The higher the volume the higher the fairness of the model, and thus the lower the amount of space that is prone to bias.

In practice, we collect the input space (without the input variables $\definputvariables$ to account for any possible permutations of their input values) where the variables $\definputvariables$ do not influence the network outcome.
To do so, we employ the $\unusediowrapper$ predicate, \cf{} \refdef*{abstract-unused}, to check whether a subset of the given set of traces $\defsetoftraces$ is not able to change the network outcome.
% Among all the possible subsets, we consider the maximal one, that is, the one that cannot be extended without changing the network outcome.
% To do so, we retrieve the input values of traces that do not change the network outcome when the input variables $\definputvariables$ are modified, similarly to the $\unusediowrapper$ predicate.
We determine the volume of this set of points by applying the standard volume operation inherent to metric spaces.
Formally, the \qlibraname{} impact quantifier is defined as follows:

\begin{definition}[\qlibraname]\labdef{qlibra}
  Given a set of input variables of interest $\definputvariables\in\setof\inputvariables$, and an output descriptor $\outputobs$,
  the quantity $\qlibra\in\tracetype\to[0, 1]$ is defined as:
  \marginnote{The input space of neural networks is assumed to be normalized in the box $[0, 1]$ for each input variable. As a consequence, the maximum volume of the input space is 1.}
  % \begin{align*}
  %   \qlibra(\defsetoftraces) &\DefeQ
  %       \volume(\bigsetjoin \setdef{
  %         \defsetoftraces' \setmeet \defsetoftraces''
  %       }{
  %         \defsetoftraces'\in Q \land \defsetoftraces''\in Q \setminus \{\defsetoftraces'\}
  %       }) \\
  %   \text{where } Q &\DefeQ
  %     \setdef{
  %       \setdef{
  %         \retrieveinput{\deftrace}(\inputvariableswithoutw)
  %       }{
  %         \deftrace \in \defsetoftraces \land \outputobs(\retrieveoutput{\deftrace}) = \defoutput
  %       }
  %     }{
  %       \defoutput\in\stateandbottom
  %     }
  % \end{align*}
  % \begin{align*}
  %   &\qlibra(\defsetoftraces) \DefeQ
  %       \volume(\setdef{\retrieveinput{\deftrace}(\inputvariableswithoutw)}{\deftrace\in\defsetoftraces'}) \\
  %   &\quad\text{where } \defsetoftraces' \subseteq \defsetoftraces \text{ such that } \forall \defsetoftraces'' \supseteq \defsetoftraces'.\spacer
  %     \unusediowrapper(\defsetoftraces') \land \neg\unusediowrapper(\defsetoftraces'')
  % \end{align*}
  % \begin{align*}
  %   &\qlibra(\defsetoftraces) \DefeQ \\
  %   &\quad \volume\left(\setdef*{\retrieveinput{\deftrace}(\inputvariableswithoutw)}{
  %     \deftrace\in\defsetoftraces \LanD \forall \defvalue\in\values.\spacer
  %     \retrieveinput\deftrace(\definputvariables) \neq \defvalue \implies \\
  %     \exists \deftrace'\in\defsetoftraces.\spacer
  %     \retrieveinput{\deftrace'}(\definputvariables) = \defvalue \LanD \retrieveinput\deftrace \stateeq{\inputvariableswithoutw} \retrieveinput{\deftrace'} \LanD \\
  %     \quad \outputobs(\retrieveoutput{\deftrace}) = \outputobs(\retrieveoutput{\deftrace'})
  %   }\right)
  % \end{align*}
  \begin{align*}
    &\qlibra(\defsetoftraces) \spacearound= \\
       &\quad \volume(\setdef{\retrieveinput{\deftrace}(\inputvariableswithoutw)}{
          \unusediowrapper(\setdef{\deftrace'\in\defsetoftraces}{\retrieveinput{\deftrace'} \stateeq{\inputvariableswithoutw} \retrieveinput{\deftrace}})
        })
  \end{align*}
\end{definition}

In other words, the \qlibraname{} quantifies the volume of the biggest set of traces $\defsetoftraces' \subseteq \defsetoftraces$ that does not contain bias, \ie, where the predicate $\unusediowrapper$ holds.

% Such a definition is important because it highlights the relation between the \qlibraname{} impact quantifier and the $\unusediowrapper$ predicate.
% In fact, the \qlibraname{} impact quantifier quantifies the bigger volume of input space where the $\unusediowrapper$ predicate holds.

\begin{marginfigure}
  \centering
  \begin{tikzpicture}[scale=0.8]
    % Grid
    % \draw[help lines, color=gray!30, dashed] (-0.1,-0.1) grid (4.1,4.1);
    % x-axis ticks
    % \foreach \x in {-4,-3,-2,-1,0,1,2,3,4}
    %     \draw (\x+5,0.1) -- (\x+5,-0.1) node[below] {\x};
    % % y-axis ticks
    % \foreach \y in {1,2,3}
    %     \draw (0.1,\y) -- (-0.1,\y) node[left] {\y};
    % Polyhedra
    \fill[color=seabornYellow, opacity=0.5] (1,1.75) -- (2,2.75) -- (2.5,1.75) -- (3,2.75) -- (3,4) -- (2,4) -- cycle;
    % \draw[color=seabornYellow, ultra thick] (1,1.75) -- (2,2.75) -- (3,2.75) -- (3,4) -- (2,4) -- cycle;
    % Polyhedra
    \fill[color=seabornGreen, opacity=0.5] (0,2.25) -- (0.5,2.25) -- (1,1.75) -- (2,4) -- (0,4) -- cycle;
    \draw[color=seabornGreen, pattern=north west lines, pattern color=seabornGreen, ultra thick] (0,2.25) -- (0.5,2.25) -- (1,1.75) -- (2,4) -- (0,4) -- cycle;
    \fill[color=seabornGreen, opacity=0.5] (3,2.75) -- (4,1.5) -- (4,4) -- (3,4) -- cycle;
    \draw[color=seabornGreen, pattern=north west lines, pattern color=seabornGreen, ultra thick] (3,2.75) -- (4,1.5) -- (4,4) -- (3,4) -- cycle;
    % Polyhedra
    \fill[color=seabornRed, opacity=0.5] (0,0) -- (4,0) -- (4,1.5) -- (3,2.75) -- (2.5,1.75) -- (2,2.75) -- (1,1.75) -- (0.5,2.25) -- (0,2.25) -- cycle;
    \draw[color=seabornRed, pattern=dots, pattern color=seabornRed, ultra thick] (0,0) -- (4,0) -- (4,1.5) -- (3,2.75) -- (2.5,1.75) -- (2,2.75) -- (1,1.75) -- (0.5,2.25) -- (0,2.25) -- cycle;
    % Nodes
    % \fill[color=seabornRed] (0+1,0+1) circle[radius=2pt];
    % \node[above left] at (0+1,0+1) {$3$};
    % x-axis
    \draw[->,ultra thick] (0,0)--(4.3,0) node[below]{$x$};
    \draw[ultra thick] (0,4)--(4,4);
    % y-axis
    \draw[->,ultra thick] (0,0)--(0,4.3);
    \draw[ultra thick] (4,0)--(4,4);


    \draw[->,ultra thick] (-0.5,0)--(-0.5,4.3) node[left]{$y$};
    \draw[very thick] (-0.4,0) -- (-0.6,0);
    \draw[very thick] (-0.4,1.5) -- (-0.6,1.5);
    % \draw[very thick] (-0.4, 4) -- (-0.6, 4);

    \draw[dashed] (-0.5,1.5) -- (4,1.5);
    % \draw[dashed] (-0.5,4) -- (0,4);
    \draw[dashed] (-0.5,0) -- (0,0);

    \fill[color=black,opacity=0.2] (0,1.5) -- (4,1.5) -- (4,4) -- (0,4) -- cycle;

    \draw[thick,decorate,decoration={brace,amplitude=3pt,raise=4pt}] (-0.5,0) -- (-0.5,1.5) node[midway, left, xshift=-5pt] {$\defbound_x$};
  \end{tikzpicture}
    \caption{Function \qlibraname.}
    \labfig{qlibra}
  \end{marginfigure}

\begin{example}
Consider the network of example presented in \reffig{irregular}.
The \qlibraname{} impact quantifier quantifies the volume of the input space where the input variable $x$ does not influence the network outcome.
In \reffig{qlibra}, it is clear that the input variable $x$ does not influence the network outcome when $y$ is below a certain threshold. This volume of fair input space is denoted by $\defbound_x$ in \reffig{qlibra}.
\end{example}

As noticed for the previous impact quantifier $\changesname$, neural network models produce deterministic set of traces.
Hence, we have that by adding more traces to a given (deterministic) set of traces, the volume of the fair space can only decrease as more traces are able to change the network outcome.

\begin{lemma}[\qlibraname{} is Anti-Monotonic]
  \lablemma{qlibra-monotonic}
For all deterministic set of traces $\defsetoftraces, \defsetoftraces'\in\tracetype$, it holds that:
  \begin{align*}
    \defsetoftraces \subseteq \defsetoftraces' \ImplieS \qlibra(\defsetoftraces) \ge \qlibra(\defsetoftraces')
  \end{align*}
\end{lemma}
\begin{proof}
  The proof is based on the observation that more traces in $\defsetoftraces'$ can only increase the volume of bias space as, in the worst case scenario, the added traces are the ones able to change the network outcome.
  In fact, \refprop*{ani-predicate-equivalence} proves that the $\unusediowrapper$ predicate is equivalent to the $\aniwrapper$ predicate whenever the given set of traces is deterministic, which is anti-monotonic in the amount of traces.
  Hence, the volume of the set of points leading to different output values can only increase with more traces, hence the volume of fair space decreases.
  We conclude that $\qlibra(\defsetoftraces) \ge \qlibra(\defsetoftraces')$.
\end{proof}

We show the validation of the $\defbound$-bounded impact property when instantiated with the $\qlibraname$ impact quantifier,
denoted $\boundedqlibra$.

\siderefbox{def}{output-abstraction-semantics}
\begin{lemma}[$\boundedqlibra$ Validation]\lablemma{qlibra-validation}
  \begin{align*}
    \collectingsemantics \subseteq \BOUNDEDQLIBRA \IfF \outputsemantics \subseteq \outputabstraction(\dependencyabstraction(\BOUNDEDQLIBRA))
  \end{align*}
\end{lemma}
\begin{proof}
  The $\qlibraname$ impact quantifier employs the $\unusediowrapper$ predicate to determine the volume of the input space that does not contain bias.
  Thus, this proof directly follows from \refthm*{output-validation}.
\end{proof}


% For clarity, we unfold the $\unusediowrapper$ predicate in the definition of the \qlibraname{} impact quantifier (assuming the set of traces $\defsetoftraces$ is deterministic).
% From \refprop{ani-predicate-equivalence}, we notice that the $\unusediowrapper$ predicate is equivalent to the $\aniwrapper$ predicate whenever the set of traces is deterministic, thus obtaining the following definition:

% \begin{align*}
%   &\qlibra(\defsetoftraces) \\
%     &\quad\spacearound=
%     \volume\left(\setdef*{\retrieveinput{\deftrace}(\inputvariableswithoutw)}{
%       \deftrace\in\defsetoftraces \LanD \forall \deftrace' \in \defsetoftraces.\spacer \\
%       \retrieveinput{\deftrace} \stateeq{\inputvariableswithoutw} \retrieveinput{\deftrace'} \implies \outputobs(\retrieveoutput{\deftrace}) = \outputobs(\retrieveoutput{\deftrace'})
%     }\right)
% \end{align*}

% Furthermore, we could also devise a notion that reverse the amount of bias, that is, the volume of the input space that contains bias.

% \begin{align*}
%   &\qlibra(\defsetoftraces) \\
%     &\quad\spacearound=
%     1 - \volume\left(
%       \setdef*{\retrieveinput{\deftrace}(\inputvariableswithoutw)}{
%         \deftrace\in\defsetoftraces \LanD \exists \deftrace' \in \defsetoftraces.\spacer \\
%         \retrieveinput{\deftrace} \stateeq{\inputvariableswithoutw} \retrieveinput{\deftrace'} \LanD \outputobs(\retrieveoutput{\deftrace}) \neq \outputobs(\retrieveoutput{\deftrace'})
%       }
%       \right)
% \end{align*}

% In the latter, we quantify the volume of the input space that contains bias, and then we subtract this volume from 1 to obtain the volume of the input space that does not contain bias. All these definitions are equivalent.



\section{Quantitative Analysis of Neural Networks}
\labsec{quantitative-analysis-neural-networks}


This section presents the abstract implementations of the \changesname{} and \qlibraname{} impact quantifiers, respectively called $\abstractchangesname$ and $\abstractqlibraname$.
We show how to validate the $\defbound$-bounded impact property for both quantifiers.


\subsection{Abstract Implementation \texorpdfstring{$\abstractchanges$}{Abstract Changes}}[Abstract \texorpdfstring{$\abstractchanges$}{Changes}]

In this section, we introduce $\abstractchanges$ as a sound implementation of $\changes$.
First, we describe the implementation, followed by the validation of the $\defbound$-bounded impact property $\boundedchanges$.

% As previously noted, the $\changesname$ impact quantifier suffers from non-deterministic set of traces, which would render it meaningless.
% The backward analysis, starting from the output buckets, computes an over-approximation of the set of traces leading to the same output bucket.
% In this case, whenever the over-approximation overlaps the result of the abstract backward analysis coming from different output buckets, in the concrete, this would be like having non-deterministic set of traces.
% To address this issue, we need an abstract domain that is exact for our property and underlying network model.

As previously noted, the $\changesname$ impact quantifier is ineffective when the input is a non-deterministic set of traces.
The backward analysis, starting from the output buckets, computes an over-approximation of the set of traces leading to the same output bucket.
If this over-approximation overlaps with the results of the abstract backward analysis from different output buckets, it concretizes to a non-deterministic set of traces.
To address this issue, we require the backward analysis to be exact for our property and underlying network model.

The core idea of the $\changesname$ impact quantifier is to group together abstract elements representing distinct stable regions. Convex abstract domains, such as polyhedra, octagons, or intervals (introduced in \refsec{numerical-abstract-domains}), are inadequate for representing multiple disjoint stable regions because they also include the points in between. Therefore, the abstract domain $\abstractdomain$ used in the backward analysis $\backwardsemanticsnoparam$ needs to employ disjunctive sets, which allow for the representation of distinct stable regions.
%
We leverage the \textit{disjunctive polyhedra abstract domain} $\disjunctivepolyabstractdomain$, defined as
$
  \langle \disjunctivepolyabstractdomaintype, \disjunctivepolyabstractdomainsubseteq \rangle
$, where $\polyabstractdomain$ represents the \textit{convex polyhedra abstract domain}~\sidecite{Cousot1978} and $\disjunctivepolyabstractdomaintype$ is the set of all finite subset of $\polyabstractdomain$.
From the polyhedra domain $\polyabstractdomain$, the function $\abstractdomainproject$ uses the Fourier-Motzkin elimination algorithm~\sidecite{Dantzig1973} to project away the input variables $\definputvariables$.
Specifically, $\abstractdomainproject$ takes as input a set of variables $\definputvariables$ and a polyhedron in $d$-dimensions where $d \ge \cardinalitynospaces{\definputvariables}$, returning a polyhedron in $(d-\cardinalitynospaces{\definputvariables})$-dimensions, removing the variables in $\definputvariables$.
This domain is exact for the property of interest when applied to feed-forward \relu-activated neural networks.

The function $\abstractchanges$ takes as input the variables of interest $\definputvariables$, $\numberofbuckets$ output buckets $\buckets\in\vectorbuckets$, and $\numberofbuckets$ disjunctions of polyhedra $\prefrombucket\in\disjunctivepolyabstractdomaintype^\numberofbuckets$ obtained from the backward analysis.
For clarity, we access each polyhedron and disjunctions of polyhedra via indexing as in a matrix-based structure, that is, $\prefrombucket=\{\{P_{1,1}\vee \dots\vee P_{1,p}\},\dots,\{P_{n,1}\vee \dots\vee P_{n,q}\}\}$ where $p = \cardinalitynospaces{\prefrombucket[1]}$ and $q = \cardinalitynospaces{\prefrombucket[n]}$. For instance, $\prefrombucket[j]$ refers to the disjunction of polyhedra $\{P_{j,1}\vee \dots\vee P_{j,k}\vee \dots\}$, for $j\le n$, and $\prefrombucket[{j, k}]$ refers to the polyhedron $P_{j,k}\in\polyabstractdomain$.
The projection of disjunctions of polyhedra applies, in turn, the polyhedron projection to each polyhedron in the disjunction, then collects the projected polyhedra in a disjunction of polyhedra, \ie, $\abstractdomainproject(\prefrombucket \in \disjunctivepolyabstractdomaintype) \defeq \bigvee_{k \le \cardinalitynospaces{\prefrombucket}} \abstractdomainproject(\prefrombucket[j, k])$.

Computationally speaking, the function $\abstractchanges$ projects away the input variables $\definputvariables$ from each polyhedron $\prefrombucket[{j, k}]$.
The projected polyhedra represent regions where $\definputvariables$ ranges on all possible values, considering all potential variations of this variable.
The function $\intersectallfunction$ gathers the set of indexes $J$, also called the connected components, where the projected polyhedra intersect.
The underlying idea is that each connected component corresponds to the set of stable regions reachable through variations of $\definputvariables$.
% Note that, each connected component is implemented as a \textit{multiset} \denis{no explaination of why using a multiset}, \ie, a set that allows multiple instances for each of its elements.
% This allows us to later exclude regions leading to the same bucket.
%
Finally, $\abstractchanges$ determines the maximum count of changes across all connected components $J$ and buckets $\buckets$.
It counts the number of indices $l$ (\cf{} $\cardinalitynospaces{\setdef{l \in J}{l \neq j}}$) in each connected component $J$ where $k$ is not equal to $j$, thus excluding the polyhedra leading to the same output bucket $j$.

\begin{definition}[$\abstractchanges$]\labdef{abstractchanges}
  We define $\abstractchanges\in\pair\vectorbuckets\vectorbuckets\to\valuesposplus$ as:
  \begin{align*}
    &\abstractchanges(\presfrombuckets, \buckets) \DefeQ \\
      &\quad \max
      \setdef{
        \max_{j \le \numberofbuckets} \cardinalitynospaces{\setdef{l \in J}{l \neq j}}
      % \\ &\qquad\quad
      }{
        J \in \intersectallfunction((\abstractdomainproject(\prefrombucket))_{j\le\numberofbuckets})
      }
  \end{align*}
\end{definition}

Before proceeding with the soundness proof of $\abstractchanges$, we recall the requirements on the concrete impact quantifier $\changes$, namely, the requirement on determinism.
Such a requirement is necessary to ensure a meaningful quantity of changes in the outcome, otherwise the number of changes would be infinite in the presence of stable regions with non-deterministic output values.
Equivalently, in the abstract, any over-approximation from the backward analysis that overlaps different preconditions $\prefrombucket$ would lead to an infinite number of changes.
To this end, we require the backward analysis to be exact: sound and \emph{complete}.
The soundness ensures that the backward analysis does not miss any possible behavior, while the completeness ensures that the backward analysis does not introduce any spurious behavior.
In addition to \refdef*{sound-over-approximation}, we require the following completeness condition on the backward analysis:

\begin{definition}[Complete Under-Approximation]\labdef{complete-under-approximation}
  For all programs $\defprogram$, and output bucket $\bucket\in\abstractdomain$, the family of semantics $\backwardsemanticsnoparam$ is a \textup{complete under-approximation} of the output-abstraction semantics $\outputsemantics$
  when it holds that:
  \[\reducedoutputsemantics \SupseteQ \backwardconcretization(\backwardsemantics)\bucket\]
\end{definition}

Whenever the backward semantics $\backwardsemantics$ is both sound and complete, it yields an exact semantics.

\begin{lemma}[Exact Backward Semantics]\lablemma{exact-backward-semantics}
  Whenever the backward semantics $\backwardsemantics$ is sound and complete, it holds that:
  \[\reducedoutputsemantics \spacearound= \backwardconcretization(\backwardsemantics)\bucket\]
\end{lemma}
\begin{proof}
  Trivially follows from the definitions of soundness (\refdef{sound-over-approximation}) and completeness (\refdef{complete-under-approximation}).
\end{proof}

Note that, whenever the given program is deterministic, the traces concretized from the backward analysis are deterministic as well.
With the exactness of the backward analysis, we show that $\abstractchanges$ is a sound implementation of $\changes$.
The next result shows that the abstract impact $\abstractchanges$ is a sound over-approximation of the concrete $\changes$ \wrt{} the $\le$ operator.


\begin{lemma}[$\abstractchanges$ is a Sound Implementation of $\changes$]\lablemma{abstractchanges-is-sound}
  Let $\definputvariables\in\setof\inputvariables$ be the set of input variables of interest, $\abstractdomain$ the abstract domain, $\backwardsemanticsnoparam$ the family of semantics, and $\buckets\in\vectorbuckets$ the starting output buckets.
  Whenever the following conditions hold:
  \begin{enumerate}[label=(\roman*)]
    \item \label{tio1} $\buckets$ covers the subset of potential outcomes, \cf{} \refdef*{covering}[*-2],
    \item \label{tio2} $\buckets$ is compatible with $\outputobs$, \cf{} \refdef*{compatibility}[*4],
    \item \label{tio3} $\backwardsemanticsnoparam$ is an exact backward semantics, \cf{} \reflemma{exact-backward-semantics}, and
    \item \label{tio4} $\abstractdomainproject$ is sound, \cf{} \refdef*{soundness-project}[*8];
  \end{enumerate}
  then, $\abstractchanges$ is a sound implementation of $\changes$ \wrt{} the $\ge$ operator:
  \begin{align*}
    \changes(\tracesemanticsnoparam) \LE \abstractchanges(\multiconcretization(\multisemanticsnoparam)\buckets, \buckets)
  \end{align*}
\end{lemma}
\begin{proof}[Proof (Sketch)]
  We need to prove that
  \begin{gather*}
    \changes(\tracesemanticsnoparam) = \sup_{\definput\in\reducedstate}
    \sup_{\defoutput\in\stateandbottom}
      \cardinality{Q_{\definputvariables, \definput, \defoutput}}\\
    \LE \\
    \abstractchanges(\multiconcretization(\multisemanticsnoparam)\buckets, \buckets) = \\
    \max
    \setdef{
      \max_{j \le \numberofbuckets} \cardinalitynospaces{\setdef{l \in J}{l \neq j}}
    }{
      J \in \intersectallfunction((\abstractdomainproject(\prefrombucket))_{j\le\numberofbuckets})
    }
  \end{gather*}
  where $Q_{\definputvariables, \definput, \defoutput} =
  \setjoin_{\defoutput' \in \stateandbottom\setminus\{\defoutput\}}
    \segments(
      \setdef{\deftrace\in\defsetoftraces}{\retrieveinput\deftrace \stateeq{\inputvariableswithoutw} \definput}, \defoutput'
    )$.
  Let $\overline{\definput}\in\reducedstate$ and $\overline{\defoutput}\in\stateandbottom$ be such that:
  \begin{gather*}
    \sup_{\definput\in\reducedstate}
    \sup_{\defoutput\in\stateandbottom}
      \cardinality{Q_{\definputvariables, \definput, \defoutput}}
    \spacearound{=} \cardinality{Q_{\definputvariables, \overline{\definput}, \overline{\defoutput}}}
  \end{gather*}
  Let $\overline{J} \in \intersectallfunction((\abstractdomainproject(\prefrombucket))_{j\le\numberofbuckets})$ be such that:
  \begin{gather*}
    \max
    \setdef{
      \max_{j \le \numberofbuckets} \cardinalitynospaces{\setdef{l \in J}{l \neq j}}
    }{
      J \in \intersectallfunction((\abstractdomainproject(\prefrombucket))_{j\le\numberofbuckets})
    }
    \spacearound{=} \\
    \max_{j \le \numberofbuckets} \cardinalitynospaces{\setdef{l \in \overline{J}}{l \neq j}}
  \end{gather*}
  thus, we need to show that $\cardinality{Q_{\definputvariables, \overline{\definput}, \overline{\defoutput}}} \le \max_{j \le \numberofbuckets} \cardinalitynospaces{\setdef{l \in \overline{J}}{l \neq j}}$.
  % By contradiction, we assume that $\cardinality{Q_{\definputvariables, \overline{\definput}, \overline{\defoutput}}} > \max_{j \le \numberofbuckets} \cardinalitynospaces{\setdef{l \in \overline{J}}{l \neq j}}$, meaning that the number of stable regions leading to different output buckets is greater than the number of intersecting buckets in the abstract.
  Provided that $\buckets$ is compatible with $\outputobs$ and covers the subset of potential outcomes, respectively \ref{tio1} and \ref{tio2}, an implication of the exactness assumption (\cf{} \ref{tio3}) of the backward analysis $\backwardsemanticsnoparam$ is any stable region intersecting after projecting away the variables $\definputvariables$ is represented in the abstract by two indices $j$ and $k$ in the connected components (from \ref{tio4}).
  Hence, under these assumptions, the number of stable regions leading to different output buckets has to be lower the number of intersecting buckets in the abstract, \ie $\cardinality{Q_{\definputvariables, \overline{\definput}, \overline{\defoutput}}} \le \max_{j \le \numberofbuckets} \cardinalitynospaces{\setdef{l \in \overline{J}}{l \neq j}}$, proving that $\abstractchanges$ is a sound implementation of $\changes$.
\end{proof}

Next, we show \nrefthm{soundness} instantiated with the abstract implementation $\abstractchanges$ and the comparison operator $\le$.

\begin{theorem}[Soundness of $\boundedchangesle$] \labthm{soundness-boundedchangesle}
  Let $\boundedchangesle$ be the property of interest we want to verify for the program $\defprogram$ and the input variable $\definputvariables\in\setof\inputvariables$.
  Whenever,
  \begin{enumerate}[label=(\roman*)]
    \item $\buckets$ covers the subset of potential outcomes, \cf{} \refdef*{covering}[*-2],
    \item $\buckets$ is compatible with $\outputobs$, \cf{} \refdef*{compatibility}[*4],
    \item $\backwardsemanticsnoparam$ is an exact backward semantics, \cf{} \reflemma{exact-backward-semantics}, and
    \item $\abstractdomainproject$ is sound, \cf{} \refdef*{soundness-project}[*8];
\end{enumerate}
  the following implication holds:
  \begin{align*}
    \abstractchanges(\multisemantics\buckets, \buckets) = \defbound' \land \defbound' \le \defbound \ImplieS \defprogram \satisfies \BOUNDEDCHANGESLE
  \end{align*}
\end{theorem}
\begin{proof}
  \reflemma{abstractchanges-is-sound} shows that $\abstractchanges$ is a sound implementation of $\changes$, the proof follows directly by application of the \refthm{soundness} instantiated with the abstract implementation $\abstractchanges$ and comparison operator $\le$.
\end{proof}

While this result shows that an exact backward analysis is required to ensure a sound implementation of $\changes$, it also presents scalability challenges as the backward analysis is computationally expensive and requires exploring all the possible paths of the network.
In the next section, we introduce the \qlibraname{} impact quantifier, which is more scalable and efficient thanks to the use or parallelization.

\subsection{Abstract Implementation \texorpdfstring{$\abstractqlibra$}{Abstract QLibra}}[Abstract \texorpdfstring{$\abstractqlibra$}{QLibra}]
\labsec{abstract-qlibra}


% We assume $\abstractdomain$ is equipped with an additional abstract operator $\abstractdomainvolume\in\abstractdomain\to[0, 1]$, which returns the volume of the given abstract element normalized in the interval $[0, 1]$.
% The abstract implementation $\abstractqlibra$ is defined as the volume of the intersecting abstract regions leading to different output buckets.
% Specifically, the volume is computed by first projecting away the input variables $\definputvariables$ from all the given abstract values resulting from the backward analysis, \cf{} $\presfrombuckets$.
% Then, it collects together all the possible pairwise intersections among the projected abstract values to find the portion of input space leading to different output buckets only via variations of the input variables $\definputvariables$.
% Assuming that all the input variables are bounded in $[0, 1]$, which is a common practice in neural networks, the volume of the abstract element is normalized in the interval $[0, 1]$.

% \begin{example}
%   In the context of the interval domain, where each input variable is related to a possibly unbounded lower and upper bound, $\abstractdomainvolume(\langle \defvariable \mapsto [2, 4]\rangle) = 2$.
%   On the other hand, whenever the input abstract element is unbounded, the size is $+\infty$, \eg, $\abstractdomainvolume(\langle \defvariable \mapsto [0, +\infty]\rangle) = +\infty$.
%   The function $\abstractdomainvolume$ expects only a single variable to be constrained in the abstract domain, or in other words, only one variable is allowed to be not $\top$.
% \end{example}

% The abstract range $\abstractqlibra$ first projects away the input variables $\definputvariables$ from all the given abstract values.
% Then, it collects all the possible intersections among the projected abstract values.
% These intersections represent concrete input configurations where variations on the values of $\definputvariables$ \emph{may} lead to changes of program outcome, from a bucket to another.
% All the possible combination of intersections are joined together to find the maximum range of the extreme values of the buckets.
% Formally, the abstract implementation $\abstractqlibra$ is defined as follows:

% \begin{definition}[$\abstractqlibra$]\labdef{abstractqlibra}
%   We define $\abstractqlibra\in\pair\vectorbuckets\vectorbuckets\to[0, 1]$ as:
%   \begin{align*}
%     &\abstractqlibra(\presfrombuckets, \buckets) \DefeQ \\
%       &\quad \abstractdomainvolume\left(
%         \bigjoin_{
%           J \in \intersectallfunction((\abstractdomainproject(\prefrombucket))_{j\le\numberofbuckets})
%         }
%           \setdef{
%             \prefrombucket[j] \abstractdomainmeet \prefrombucket[k]
%           }{
%             j, k \in J \land j \neq k
%           }
%       \right)
%   \end{align*}
% \end{definition}

% \begin{example}
%   \denis{Example of the abstractqlibra function on the previous network space.}
% \end{example}

% To prove that $\abstractqlibra$ is a sound implementation of $\qlibra$, we require the following soundness condition on the abstract operator $\abstractdomainvolume$ to ensure that the abstract volume is always greater than the concrete one.

% \begin{definition}[Soundness of \texorpdfstring{$\abstractdomainvolume$}{Volume}]\labdef{soundness-volume}
%   Given an abstract value $\defstate^\natural\in\abstractdomain$ and the set of input variables of interest $\definputvariables\in\setof\inputvariables$, it holds that:
%   \[\abstractdomainvolume(\defstate^\natural) \GE \cardinality{\setdef{\retrieveinput{\defstate}(\definputvariables)}{\defstate\in\abstractdomainconcretization(\defstate^\natural)}}\]
% \end{definition}

% The next result shows that the abstract impact $\abstractqlibra$ is a sound over-approximation of the concrete impact $\qlibra$, \cf{} \refdef{qlibra}.

% \begin{lemma}[$\abstractqlibra$ is a Sound Implementation of $\qlibra$]\lablemma{abstractqlibra-is-sound}
%   Let $\definputvariables\in\variables$ be the set of input variables of interest, $\abstractdomain$ the abstract domain, $\backwardsemanticsnoparam$ the family of semantics, and $\buckets\in\vectorbuckets$ the starting output buckets.
%   Whenever the following conditions hold:
%   \begin{enumerate}[label=(\roman*)]
%     \item \label{rts1} $\buckets$ covers the subset of potential outcomes, \cf{} \refdef*{covering}[*-2],
%     \item \label{rts2} $\buckets$ is compatible with $\outputobs$, \cf{} \refdef*{compatibility}[*4],
%     \item \label{rts3} $\abstractdomainproject$ is sound, \cf{} \refdef*{soundness-project}[*8], and
%     \item \label{rts4} $\abstractdomainvolume$ is sound, \cf{} \refdef{soundness-volume}[*12];
%   \end{enumerate}
%   then, $\abstractqlibra$ is a sound implementation of $\qlibra$.
% \end{lemma}
% \begin{proof}
%   \denis{todo}
% \end{proof}


% \begin{example}
%   The quantities computed by the abstract implementation $\abstractqlibra$ in \refexample{abstract-qlibra} are sound over-approximations of the concrete implementation $\qlibra$:
%   \begin{align*}
%     & \qlibraname_{\{\texttt{angle}\}}(\dependencysemanticsnoparam\semanticsof{\landingprogram}) = 3 \\
%     &\qquad\GE \abstractqlibraname_{\{\texttt{angle}\}}(\presfrombuckets, \buckets) = 1
%   \end{align*}
%   \begin{align*}
%     & \qlibraname_{\{\texttt{speed}\}}(\dependencysemanticsnoparam\semanticsof{\landingprogram}) = 2 \\
%     &\qquad\GE \abstractqlibraname_{\{\texttt{speed}\}}(\presfrombuckets, \buckets) = 1
%   \end{align*}
%   as expected by \refthm*{soundness}, where $\landingprogram$ is the program of the landing alarm system, \cf{} \refprog{landing-alarm-system}.
% \end{example}


In this section, we present $\abstractqlibra$ as a sound implementation of $\qlibra$, computing an over-approximation of the volume of input space that may contain bias. Conversely, $\abstractqlibra$ computes an under-approximation of the fair input space.
We first present the implementation of $\abstractqlibra$, which is too na\"ive to be practical, but it is still useful for building upon in the parallel implementation $\parallelqlibra$ later in this chapter.


% \begin{marginalgorithm}
%   \caption{Abstract Implementation $\abstractqlibra$}
%   \labalg{abstract-qlibra}
% \begin{lstlisting}[
%   language=algorithm,
%   style=algorithm,
%   escapechar=\%,
%   ]
% QAUnused%${}^\natural_{\defmodel, \abstractdomain_1, \abstractdomain_2}$%(%$\definputvariables, \analysisinputspace$%):
%   %$\dots$%
% \end{lstlisting}
% \end{marginalgorithm}

\begin{definition}[\texorpdfstring{$\abstractqlibra$}{Abstract QAUnused}]\labdef{abstract-qlibra}
  We define $\abstractqlibra\in\pair\vectorbuckets\vectorbuckets\to\valuesposplus$ as:
  \begin{align*}
    \abstractqlibra(\presfrombuckets, \buckets) \DefeQ& 1-
    \abstractdomainvolume\left(
    \abstractdomainbigjoin
    \setdef@{\abstractdomainproject(\prefrombucket) \abstractdomainmeet \abstractdomainproject(\prefrombucket[k])}{j, k \le \numberofbuckets \land j \neq k}
    \right)
  \end{align*}
\end{definition}

% Instead of introducing an abstract implementation as a definition, we describe $\abstractqlibra$ in the form of a pseudo-code: \refalg{abstract-qlibra}.
% The algorithm takes as input the neural network model $\defmodel$, a set of input variables of interest $\definputvariables$ (the sensitive features), the $\numberofbuckets$ output buckets $\buckets$, and an abstract domain $\abstractdomain$.
The output buckets $\buckets$ represent all the possible classification targets of the neural network.
In this way, both covering (\refdef{covering}) and compatibility (\refdef{compatibility}) conditions are satisfied as all the target classes are covered and for any two different target classes, there exist two different output buckets representing them.
The analysis proceeds backwards from each output bucket $\bucket$ via the backward semantics $\backwardsemanticsnoparam$ in order to determine an over-approximation of the initial states $\prefrombucket$.
$\abstractqlibra$ projects away the input variables $\definputvariables$ from the abstract states $\prefrombucket$ to account for any possible permutations of their input values.
Then, it computes the pairwise intersections among the projected abstract states to find the portion of input space leading to different output buckets only via variations of the input variables $\definputvariables$.
The volume of this input space is an over-approximation of the biased space, hence its complement is an under-approximation of the fair input space.
We assume the classical soundness condition on the abstract operator $\abstractdomainvolume$ to ensure that the abstract volume is always bigger than the concrete one.
The quantity measured by $\abstractqlibra$, \cf{} \refdef{abstract-qlibra}, is always lower than the concrete $\qlibra$, \cf{} \refdef*{qlibra}, as it measures the volume of the input space that contains bias.

\begin{lemma}[$\abstractqlibra$ is a Sound Implementation of $\qlibra$]\lablemma{abstractqlibra-is-sound}
  Let $\definputvariables\in\setof\inputvariables$ be the set of input variables of interest, $\abstractdomain$ the abstract domain, $\backwardsemanticsnoparam$ the family of semantics, and $\buckets\in\vectorbuckets$ the starting output buckets.
  Whenever the following conditions hold:
  \begin{enumerate}[label=(\roman*)]
    \item \label{nioa1} $\buckets$ covers the subset of potential outcomes, \cf{} \refdef*{covering}[*-2],
    \item \label{nioa2} $\buckets$ is compatible with $\outputobs$, \cf{} \refdef*{compatibility}[*4],
    \item \label{nioa3} $\backwardsemanticsnoparam$ is a sound backward semantics, \cf{} \refdef{sound-over-approximation},
    \item \label{nioa4} $\abstractdomainproject$ is sound, \cf{} \refdef*{soundness-project}[*8], and
    \item \label{nioa5} $\abstractdomainvolume$ is sound;
  \end{enumerate}
  then, $\abstractqlibra$ is a sound implementation of $\qlibra$ \wrt{} the $\ge$ operator:
  \begin{align*}
    \qlibra(\tracesemanticsnoparam) \GE \abstractqlibra(\multiconcretization(\multisemanticsnoparam)\buckets, \buckets)
  \end{align*}
\end{lemma}
\begin{proof}[Proof (Sketch)]
  The soundness argument to prove this lemma is based on the soundness of the operators involved in the computation of $\abstractqlibra$, namely \ref{nioa3}, \ref{nioa4} and \ref{nioa5}, the fact that the buckets $\buckets$ cover the subset of potential outcomes \ref{nioa1} and are compatible with the output observations \ref{nioa2}.
  The proof follows by showing that $\abstractqlibra$ is the complement of the volume of biased space, computed by first projecting away the input variables $\definputvariables$ from all the given abstract values resulting from the backward analysis, then collecting all the possible pairwise intersections among the projected abstract values to find the portion of input space leading to, at least, two different output buckets.
  By the fact that all the operators involved are sound over-approximations (\cf{} \ref{nioa4} and \ref{nioa5}) as well as the backward analysis (\cf{} \ref{nioa3}), the proof follows directly.
  As the volume computed in the abstract over-approximates the biased space, its complement under-approximates the fair input space.
\end{proof}

We show \nrefthm{soundness} instantiated with the abstract implementation $\abstractqlibra$ and the comparison operator $\ge$.

\begin{theorem}[Soundness of $\boundedqlibrage$] \labthm{soundness-boundedqlibrage}
  Let $\boundedqlibrage$ be the property of interest we want to verify for the program $\defprogram$ and the input variable $\definputvariables\in\setof\inputvariables$.
  Whenever,
  \begin{enumerate}[label=(\roman*)]
    \item $\backwardsemanticsnoparam$ is sound with respect to $\outputsemanticsnoparam$, \cf{} \refdef*{sound-over-approximation}[*-5], and
    \item $\buckets$ covers the subset of potential outcomes, \cf{} \refdef*{covering}[*-1.5],
\end{enumerate}
  the following implication holds:
  \begin{align*}
    \abstractqlibra(\multisemantics\buckets, \buckets) = \defbound' \land \defbound' \ge \defbound \ImplieS \defprogram \satisfies \BOUNDEDQLIBRAGE
  \end{align*}
\end{theorem}
\begin{proof}
  \reflemma{abstractqlibra-is-sound} shows that $\abstractqlibra$ is a sound implementation of $\qlibra$, the proof follows directly by application of the \refthm{soundness} instantiated with the abstract implementation $\abstractqlibra$ and comparison operator $\le$.
\end{proof}

Although sound, the abstract implementation $\abstractqlibra$ does not work well in presence of imprecision from the abstract domain employed during the backward analysis.
In fact, the abstract implementation $\abstractqlibra$ applied directly to the backward analysis yields the so called \emph{na\"ive casual-fairness analysis}~\sidecite{Urban2020}.
Such analysis suffers from the choice of existing abstract domains, which are rather fast but too imprecise to handle non-linear constraints, such as those arising from the activation functions in neural networks.
Indeed, even using the polyhedra domain for the backward analysis, handling the \relu{} activation function would over-approximate what effectively is a conditional branch, leading to a loss of precision that is reflected for each node of the neural network.
On the other hand, one could use a disjunctive completion of the polyhedra domain~\sidecite{Cousot1979a}, which would retain a separate polyhedron each condition.
However, this analysis would be extremely slow.

\section{Parallel Analysis for Efficient Validation of the \texorpdfstring{$\boundedqlibra$ Property}{k-Bounded Impact Property applied to qlibra}}[Parallel Analysis]

To overcome the limitation of $\abstractqlibra$ described above, we first reason at a concrete-semantics level, introducing an additional semantics: the \emph{parallel semantics}.
Intuitively, we show how partitioning the input space into \emph{fair} partitions still allows for property validation.
Thus, we could measure the amount of bias in the input space in parallel for each partition, and then aggregate the quantity.
Finally, we show how to validate the $\defbound$-bounded impact property for the parallel semantics.
This section is based on the work presented in \sidetextcite{Urban2020} and \sidetextcite{Mazzucato2021}.


\subsection{Parallel Semantics}\labsec{parallel-semantics}

We observe that the semantics of a program satisfying the $\boundedqlibra$ property induces a partition of the input space restricted to the input variables in $\inputvariableswithoutw$.
We call this input partition \emph{fair}.

\begin{definition}[Fair Input Partition]\labdef{fair-input-partition}
  An input partition $\definputpartitions$ of $\reducedstate$ is \emph{fair} if all value choices $\values$ for the variables of interest $\definputvariables$ are possible in all the partitions:
  \begin{math}
    \forall \definputpartition\in\definputpartitions,\defvalue\in\values.\spacer
    \exists \retrieveinput\defstate\in\definputpartition.\spacer
    \retrieveinput\defstate(\definputvariables) = \defvalue
  \end{math}
\end{definition}

\begin{marginfigure}
  \centering
\begin{tikzpicture}[scale=0.8]
  % Grid
  % \draw[help lines, color=gray!30, dashed] (-0.1,-0.1) grid (4.1,4.1);
  % x-axis ticks
  % \foreach \x in {-4,-3,-2,-1,0,1,2,3,4}
  %     \draw (\x+5,0.1) -- (\x+5,-0.1) node[below] {\x};
  % % y-axis ticks
  % \foreach \y in {1,2,3}
  %     \draw (0.1,\y) -- (-0.1,\y) node[left] {\y};
  % Polyhedra
  \fill[color=seabornYellow, opacity=0.5] (1,1.75) -- (2,2.75) -- (2.5,1.75) -- (3,2.75) -- (3,4) -- (2,4) -- cycle;
  % \draw[color=seabornYellow, ultra thick] (1,1.75) -- (2,2.75) -- (3,2.75) -- (3,4) -- (2,4) -- cycle;
  % Polyhedra
  \fill[color=seabornGreen, opacity=0.5] (0,2.25) -- (0.5,2.25) -- (1,1.75) -- (2,4) -- (0,4) -- cycle;
  \draw[color=seabornGreen, pattern=north west lines, pattern color=seabornGreen, ultra thick] (0,2.25) -- (0.5,2.25) -- (1,1.75) -- (2,4) -- (0,4) -- cycle;
  \fill[color=seabornGreen, opacity=0.5] (3,2.75) -- (4,1.5) -- (4,4) -- (3,4) -- cycle;
  \draw[color=seabornGreen, pattern=north west lines, pattern color=seabornGreen, ultra thick] (3,2.75) -- (4,1.5) -- (4,4) -- (3,4) -- cycle;
  % Polyhedra
  \fill[color=seabornRed, opacity=0.5] (0,0) -- (4,0) -- (4,1.5) -- (3,2.75) -- (2.5,1.75) -- (2,2.75) -- (1,1.75) -- (0.5,2.25) -- (0,2.25) -- cycle;
  \draw[color=seabornRed, pattern=dots, pattern color=seabornRed, ultra thick] (0,0) -- (4,0) -- (4,1.5) -- (3,2.75) -- (2.5,1.75) -- (2,2.75) -- (1,1.75) -- (0.5,2.25) -- (0,2.25) -- cycle;
  % Nodes
  % \fill[color=seabornRed] (0+1,0+1) circle[radius=2pt];
  % \node[above left] at (0+1,0+1) {$3$};
  % x-axis
  \draw[->,ultra thick] (0,0)--(4.3,0) node[below]{$x$};
  \draw[ultra thick] (0,4)--(4,4);
  % y-axis
  \draw[->,ultra thick] (0,0)--(0,4.3) node[left]{$y$};
  \draw[ultra thick] (4,0)--(4,4);

  \draw[dashed] (0,1.2) node[left] {$y_0$} -- (4,1.2);
  \draw[dashed] (1.75,0) node[below] {$x_0$} -- (1.75,4);
  % little dot in the intersection x_0 y_0
  \fill[color=black] (1.75,1.2) circle[radius=2pt];

  \draw[thick] (0, 2) -- (4, 2);
  \draw[dashed] (0, 2) -- (-0.5, 2);
  \draw[thick] (0, 3) -- (4, 3);
  \draw[dashed] (0, 3) -- (-0.5, 3);
  \draw[dashed] (0, 4) -- (-0.5, 4);
  \draw[dashed] (0, 0) -- (-0.5, 0);

  \draw[thick,decorate,decoration={brace,amplitude=6pt,raise=0pt}] (-0.5,0) -- (-0.5,1.9) node[midway, left, xshift=-5pt] {$P_1$};
  \draw[thick,decorate,decoration={brace,amplitude=6pt,raise=0pt}] (-0.5,2.1) -- (-0.5,2.9) node[midway, left, xshift=-5pt] {$P_2$};
  \draw[thick,decorate,decoration={brace,amplitude=6pt,raise=0pt}] (-0.5,3.1) -- (-0.5,4) node[midway, left, xshift=-5pt] {$P_3$};
\end{tikzpicture}
  \caption{The partitions $P_1$, $P_2$, and $P_3$ are fair partitions for the variable $y$.}
  \labfig{fair-partitions}
\end{marginfigure}

\begin{example}
  \reffig{fair-partitions} show a potential partition of the input space into $P_1$ and $P_2$ and $P_3$, fair for the input variables $\{y\}$.
  All possible permutations in the $y$-axis of any input value $(x_0, y_0)$ result in a point $(x_0, y')$ that still belongs to the same partition of $(x_0, y_0)$.
\end{example}

Given a fair input partition $\definputpartitions$ of $\reducedstate$, we can verify whether a program $\defprogram$ has an impact of $\defbound$ for each element $\definputpartition\in\definputpartitions$, \emph{independently}, and aggregate the results.

\siderefbox{def}{qlibra}
\begin{lemma}\lablemma{sum-qlibra}
  Given a fair input partition $\definputpartitions$, it holds that:
  \begin{align*}
    \defprogram \satisfies \BOUNDEDQLIBRA \IfF
    \sum_{\definputpartition\in\definputpartitions} \qlibra(
      \reduceinit[\tracesemantics]{\definputpartition}
    ) \comparison \defbound
  \end{align*}
\end{lemma}
\begin{proof}
  Note that, by \refdef{qlibra}, $\qlibra(\defsetoftraces)$ computes the volume of input space that does not use the input variables $\definputvariables$ \wrt{} to the set of traces $\defsetoftraces$. It does so by partitioning the set of given traces that are perturbations of the input variables $\definputvariables$, \cf{} the set $\defsetoftraces_{\retrieveinput{\deftrace}}$ on the side. By definition, these partitions are fair partitions by \refdef{fair-input-partition}.
  Furthermore, we could call these partitions as \emph{minimal} in the sense that they are the smallest fair partitions that can be obtained from the input space.
  As a consequence, the $\qlibra$ impact quantifier could be written as the sum of the impacts of the minimal fair partitions, \ie, $\qlibra(\tracesemantics) = \sum_{\deftrace\in\tracesemantics} \qlibra(\setdef{\deftrace'\in\tracesemantics}{\retrieveinput{\deftrace'} \stateeq{\inputvariableswithoutw} \retrieveinput{\deftrace}})$.
  Since any combination of minimal fair partitions is a fair partition, it holds that:
  \begin{align}\labeq{qlibra-sum-of}
    \qlibra(\tracesemantics) = \sum_{\definputpartition\in\definputpartitions} \qlibra(\reduceinit[\tracesemantics]{\definputpartition})
  \end{align} where $\reduceinit[\tracesemantics]{\definputpartition}$ is the set of traces restricted to the input partition $\definputpartition$.
  We conclude:
  \begin{align*}
    &\defprogram \satisfies \boundedqlibra
      &&\text{by \refdef{bounded}} \\
    &\iff \qlibra(\tracesemantics) \comparison \defbound
      &&\text{Let } \defbound' \comparison \defbound \text{ such that} \\
      & &&\quad \qlibra(\tracesemantics) = \defbound' \\
    &\iff \qlibra(\tracesemantics) = \defbound'
      &&\text{by \refeq{qlibra-sum-of}} \\
    &\iff \sum_{\definputpartition\in\definputpartitions} \qlibra(
      \defsetoftraces_{\definputpartition}
    ) = \defbound'
      &&\text{by }\defbound' \comparison \defbound \\
    &\iff \sum_{\definputpartition\in\definputpartitions} \qlibra(
      \defsetoftraces_{\definputpartition}
    ) \comparison \defbound
  \end{align*}
  In fact, we proved an even stronger result, the fact that given a fair input partition $\definputpartitions$, the volume of fair input space is exactly as the sum of the volumes of fair space in each fair partition.
\end{proof}

We exploit the above insight to further abstract the output-abstraction semantics $\outputabstraction$ to the \emph{parallel semantics} $\parallelsemanticsnoparam$.
Formally, the right adjoint\sidenote{The left adjoint is uniquely defined by the right one.} $\parallelabstraction$ for the parallel semantics is defined as:
%
\begin{definition}[Right Adjoint for the Parallel Semantics]\labdef{right-adjoint-for-the-parallel-abstraction-semantics}
\begin{align*}
  \parallelabstraction \IN& \outputtype \to \paralleltype \\
  \parallelabstraction(\defsetofsetofdependencies) \DefeQ& \setdef{
    \partition{\defsetofdependencies}{\definputpartition}
  }{
    \defsetofdependencies \in \defsetofsetofdependencies \LanD \definputpartition\in\definputpartitions
  }\\
  \parallelconcretization \IN& \paralleltype \to \outputtype \\
  \parallelconcretization(\defsetofsetofdependencies) \DefeQ&
  \setdef{
    \bigsetjoin_{\defsetofdependencies\in G} \defsetofdependencies
  }{
    G \in \prod_{\definputpartition\in\definputpartitions} \setjoin_{\defsetofdependencies\in\defsetofsetofdependencies} \partition{\defsetofdependencies}{\definputpartition}
  }
\end{align*}
where $\partition{\defsetofdependencies}{\definputpartition}$ is the set of dependencies restricted to the input partition $\definputpartition$, \ie, $\partition{\defsetofdependencies}{\definputpartition} \defeq \setdef{\defstate}{\inputoutputtuple\defstate\in\defsetofdependencies \land \retrieveinput{\defstate}\in\definputpartition}$.
\end{definition}

The parallel abstraction $\parallelabstraction$ is defined as the set of dependencies restricted to each fair input partition. Conversely, the parallel concretization $\parallelconcretization$ brings back all the original set of dependencies by joining all the dependencies in each fair input partition. Potentially, the concretization adds set of dependencies that are not in the original set of dependencies as spurious combinations of different fair input partitions.

% The order of the parallel semantics $\parallelsubseteq$ is the pointwise ordering between sets of pairs of states restricted to the same input partition of $\definputpartitions$. Formally,
% \begin{align*}
%   \defsetofsetofdependencies \parallelsubseteq \defsetofsetofdependencies' \IfF
%   \forall \defsetofdependencies\in\defsetofsetofdependencies, \defsetofdependencies'\in\defsetofsetofdependencies'.\spacer
%   \defsetofdependencies \neq \emptyset \land \defsetofdependencies' \neq \emptyset \ImplieS
%   \bigwedge_{\definputpartition\in\definputpartitions}\partition{\defsetofdependencies}{\definputpartition} \subseteq \partition{\defsetofdependencies'}{\definputpartition}
% \end{align*}

% Ensuring that all the non-empty set of dependencies in $\defsetofsetofdependencies$ are included in $\defsetofsetofdependencies'$ for each fair partition $\definputpartition$.
We have the following result:


\begin{theorem}\labthm{output-parallel-galois-connection}
  The two adjoints $\tuple{\parallelabstraction}{\parallelconcretization}$ form a \emph{Galois connection}:
\begin{align*}
  \galoisinjbetweensemantics{output}{parallel}
\end{align*}
\end{theorem}
\begin{proof}
  We need to show that $\parallelabstraction(\defsetofsetofdependencies)\subseteq \defsetofsetofdependencies' \iff \defsetofsetofdependencies\subseteq\parallelconcretization(\defsetofsetofdependencies')$.
  First, we show the direction $(\implies)$.
  Assuming $\parallelabstraction(\defsetofsetofdependencies)\subseteq \defsetofsetofdependencies'$, we have that $\parallelconcretization(\defsetofsetofdependencies')$ contains all the possible semantics made of different combinations of input fair partitions of $\defsetofsetofdependencies'$.
   Thus, $\parallelconcretization(\defsetofsetofdependencies')$ at least retrieves all the semantics in $\defsetofsetofdependencies$, \ie, $\defsetofsetofdependencies\subseteq\parallelconcretization(\defsetofsetofdependencies')$.
  To show $(\Leftarrow)$, we assume $\defsetofsetofdependencies\subseteq\parallelconcretization(\defsetofsetofdependencies')$.
  It is easy to note that $\parallelabstraction(\parallelconcretization(\defsetofsetofdependencies')) = \defsetofsetofdependencies'$ since the concretization joins together set of dependencies from different fair input partitions and the abstraction splits them subdivided into fair input partitions, obtaining the original set.
  Hence, by monotonicity of $\parallelabstraction$, we obtain $\parallelabstraction(\defsetofsetofdependencies)\subseteq \parallelabstraction(\parallelconcretization(\defsetofsetofdependencies')) = \defsetofsetofdependencies'$.
\end{proof}


We can now derive the parallel semantics as an abstraction of the output-abstraction semantics.
\begin{definition}[Parallel Abstraction Semantics]\labdef{parallel-abstraction-semantics}
  The \emph{parallel semantics} $\parallelsemanticsnoparam\in\paralleltype$ is defined as:
  \begin{align*}
    \parallelsemanticsnoparam\DefeQ\parallelabstraction(\outputsemanticsnoparam)
    % \spacearound{=}&\parallelabstraction(\{\spacearound{\setdef{\inputoutputtuple{\deftrace}}{\deftrace \in \tracesemanticsnoparam}}\}) \\
    \spacearound{=}
    \setdef{\spacearound{
      \setdef{
        \tuple{\retrieveinput{\deftrace}}{\outputobs(\retrieveoutput{\deftrace})}
      }{
        \deftrace \in \partition\tracesemanticsnoparam\definputpartition
      }
    }}{\definputpartition\in\definputpartitions}
  \end{align*}
  where $\partition\tracesemanticsnoparam\definputpartition \DefeQ \setdef{\defstate}{\inputoutputtuple\defstate\in\tracesemanticsnoparam \land \retrieveinput{\defstate}\in\definputpartition}$.
\end{definition}

It remains to show soundness and completeness for the parallel semantics when applied to the $\boundedqlibra$ property.
\begin{theorem}
  Given a fair input partition $\definputpartitions$ of $\reducedstate$, it holds that:
  \begin{align*}
    \defprogram \satisfies \boundedqlibra \IfF
    \parallelsemantics \parallelsubseteq \parallelabstraction(\outputabstraction(\dependencyabstraction(\boundedqlibra)))
  \end{align*}
\end{theorem}
\begin{proof}
Let $\defprogram \satisfies \boundedqlibra$.
From \reflemma*{qlibra-validation}, we have that $\outputsemantics \subseteq \outputabstraction(\dependencyabstraction(\boundedqlibra))$.
Thus, from the Galois connections in \refthm{output-parallel-galois-connection}, we have that $\parallelabstraction(\outputsemantics) \subseteq \parallelabstraction(\outputabstraction(\dependencyabstraction(\boundedqlibra)))$.
From \refdef{parallel-abstraction-semantics}, we can conclude that $\parallelsemantics \parallelsubseteq \parallelabstraction(\outputabstraction(\dependencyabstraction(\boundedqlibra)))$.
\end{proof}



\subsection{Parallel Implementation \texorpdfstring{$\parallelqlibra$}{QLibra}}[Parallel \texorpdfstring{$\parallelqlibra$}{QLibra}]
\labsec{parallel-implementation-qlibra}

In this section, we build upon the parallel semantics to design an abstract implementation $\abstractqlibra$, called $\parallelqlibra$, that computes an over-approximation of the volume of input space that may be fair.
This static analysis automatically find a fair partition of the input space, then computes the volume of the fair input space.
The abstract implementation $\parallelqlibra$ is defined as the sum of the volumes of the fair partitions.

{ % start of the scope for the newcommands
\newcommand\F{\textup{F}}
\newcommand\E{\textup{E}}
\newcommand\C{\textup{C}}
\renewcommand\B{\textup{B}}
\newcommand\I{\textup{I}}
\renewcommand\a{\defabstractvalue}
\newcommand\p{\defabstractactivationpattern}
\renewcommand\O{\textup{O}}
\renewcommand\o{\textup{o}}
\renewcommand\j{\textit{j}}



\refalg{parallel-qlibra} shows the implementation of $\parallelqlibra$, combining a forward with a backward analysis.
The forward analysis (\cf{} \refalg{forward-neural-networks}) uses an abstract domain $\abstractdomain$ and builds partition $\definputpartitions$ of the input space, while the backward analysis (\cf{} \refalg{backward-neural-networks}) employs the disjunction of polyhedra abstract domain $\disjunctivepolyabstractdomain$.
Then, we perform the quantification of the biased space.

\marginnote{
  The symbol $\reducedstate$ represents the set of input states, \ie, let $\inputvariables$ the set of input variables:
  \[\reducedstate \DefeQ \setdef{\defstate(\inputvariables)}{\defstate\in\state}\]
}
More specifically, the forward analysis bounds the number of paths that the backward analysis has to explore.
We represent each path by an abstract activation pattern.\sidenote{The activation pattern determines the activation status of every \relu{} operation in the network model. The abstract activation pattern is a partial activation pattern where some activation status may be unknown.}
The analysis receives a \emph{budget} parameter, called $\tune$ in \refline{parameters}, providing an upper bound $\upperbound$ on the number of tolerated \relu{} nodes with an unknown activation status for each element $\definputpartition\in\definputpartitions$, \ie, an upper bound on the number of paths that are to be explored by the backward analysis in each partition $\definputpartition$.
The forward analysis starts with the trivial partition of the whole input space, \ie, $\definputpartitions = \{\reducedstate\}$ in \refline{initial-partition}.
Then, it proceeds forward for each element $\definputpartition\in\definputpartitions$ by computing the abstract activation patterns that are compatible with the input partition $\definputpartition$ (\cf{} \refline{forward}), starting from the empty set of activation patterns. If $\definputpartition$ leads to a unique outcome (\cf{} \refline{unique}), then the partition is already fair without further analysis.
Therefore, we add the partition $\definputpartition$ to the set of \emph{completed} partitions $\completedpartitions$, (\cf{} \refline{completed}).
On the other hand, if abstract activation pattern $\defabstractactivationpattern$ fixes the activation status of enough \relu{} nodes, then we declare the partition $\definputpartition$ \emph{feasible} and ready for the backward analysis.
In this case, the pair of $\defabstractactivationpattern$ and $\definputpartition$ is added into a map $\feasiblepartitions$ from abstract activation patterns to input partitions (\cf{} \refline{feasible}).
Otherwise, the partition $\definputpartition$ needs to be further refined, with respect to the variables in $\inputvariableswithoutw$ (\cf{} \refline{partition}).
Partitioning may continue until the volume of the partition is below a certain threshold $\lowerbound$ from the budget configuration $\tune$.
Finally, whenever the partition is smaller than $\lowerbound$ (\cf{} \refline{lower}), we exclude it from the set of partitions to be analyzed until more resources become available (\cf{} \refline{excluded}).
Note that, the forward analysis does not need expressive and slow abstract domain since it does not need to precisely handle polyhedral constraints.

\begin{marginalgorithm}[*-13]
  \caption{Parallel Implementation $\parallelqlibra$}
  \labalg{parallel-qlibra}
\begin{lstlisting}[
  language=algorithm,
  style=algorithm,
  escapechar=\%,
  ]
QAUnused%${}^\parallel_{\defmodel, \abstractdomain}$%(%$\definputvariables$%, %$\tune$%):
  %$\upperbound$%, %$\lowerbound$%, %$\maxupperbound$%, %$\minlowerbound$%, %$\heuristic$% = %$\tune$%%\labline{parameters}%
  %\F%, %\E%, %\C% = %$\emptyset$%, %$\emptyset$%, %$\emptyset$%%\labline{A}%
  %$\definputpartitions$% = %$\{\reducedstate\}$%%\labline{initial-partition}%
  do
    while %$\definputpartitions \neq \emptyset$% do
      %\I% = %$\definputpartitions$%.pop()
      %$\a$%, %$\p$% = Forward%${}_{\defmodel, \abstractdomain}$%(%\I%)%\labline{forward}%
      if %$\a$% is unique then%\labline{unique}%
        %\C% = %$\C \cup \{\tuple{\p }{ \I}\}$%%\labline{completed}%
      else if %$\cardinalitynospaces\defmodel - \cardinalitynospaces\p \le \upperbound$% then
        %\F% = %$\F \cup \{\I\}$%%\labline{feasible}%%\labline{B}%
      else if %$\cardinalitynospaces\I \le \lowerbound$% then%\labline{lower}%
        %\E% = %$\E \cup \{\I\}$%%\labline{excluded}%
      else
        %$\definputpartitions$% = %$\definputpartitions$% %$\cup$% Partition%${}_{\inputvariableswithoutw}$%(%\I%)%\labline{partition}%
    %$\definputpartitions$% = %\E%%\labline{previously-excluded}%
    %$\upperbound$%, %$\lowerbound$% = %$\heuristic$%(%$\upperbound$%, %$\lowerbound$%)%\labline{tuning}%
  while %$\upperbound \neq \maxupperbound$% and %$\lowerbound \neq \minlowerbound$%
  %\E% = %$\definputpartitions$%
  %\B% = %$\emptyset$%
  for %$\tuple{\p}{ \definputpartitions}$% in %\F% do%\labline{independent}%
    %\O% = %$\emptyset$%
    for %\j% = 0 up to %$\cardinalitynospaces{\hiddenlayer{\lastlayerindex}}$% do
      %$\a$% = Backward%${}_{\defmodel, \disjunctivepolyabstractdomain, \p}$%(%$\node[\hiddenlayer{\lastlayerindex}]$%)%\labline{backward}%
      %\O% = %$\O \cup \{\tuple{\node[\hiddenlayer{\lastlayerindex}] }{ \a}\}$%
    for %\I% in %$\definputpartitions$% do
      %\O'% = %$\emptyset$%
      for %$\tuple{\o }{ \a}$% in %\O% do
        %\O'% = %$\O' \cup \{\tuple{\o }{ \abstractdomainprojectothers(\a)}\}$%
      %\B% = %$\B \cup \textup{Check}(\O')$%
  return %$1-\sum_{\a\in\B} \abstractdomainvolume(\a)$%%\labline{sum}%
\end{lstlisting}
\end{marginalgorithm}

The budget configuration of the pre-analysis (\ie, choices of an abstract domain, lower bound $\lowerbound$, and upper bound $\upperbound$) allows trading-off between precision and scalability of the approach. Ultimately however, the optimal configuration largely depends on the analyzed neural network. For this reason, a \emph{configuration auto-tuning mechanism} dynamically updates the lower bound and upper bound configuration according to a chosen \emph{search heuristic} $\heuristic$ (\cf{} \refline{tuning}). By default, whenever an input partition exceeds the current configuration (\cf{} \refline{previously-excluded}), the pre-analysis alternates between increasing the upper bound by one, up to a maximum upper bound $\maxupperbound$, and halving the lower bound, down to a minimum lower bound $\minlowerbound$. Other bound update patterns are configurable (e.g., by updating both bounds at the same time, or performing multiple increments to the upper bound before halving the lower bound, etc.). To guarantee the termination of the whole analysis, the search heuristic should return $(\maxupperbound, \minlowerbound)$ after a finite number of iterations.


Then, the analysis proceeds backwards, independently for each abstract
activation pattern $\defabstractactivationpattern$ and input partition
$\definputpartition$ in $\feasiblepartitions$ (\cf{} \refline{independent}). By
exploiting the knowledge of the activation pattern, the backward analysis does
not need to explore all the possible paths in the network model, but only those
with an unknown activation status. Specifically, the backward analysis only
needs to split the abstract invariant only when a node has an unknown activation
status, as otherwise it is handled as either the identity of the constant
function. For each input partition, the analysis computes the abstract domain
representing the biased input space (\cf{} \refline{sum}). Specifically, the
function \textup{Check}, \cf{} \refalg{check}, performs the pair-wise
intersection among the abstract values coming from different output buckets
(\ie, different network classifications) and returns the set of abstract values
that intersect, representing the biased input space.

\begin{marginalgorithm}
  \caption{Bias check.}
  \labalg{check}
\begin{lstlisting}[
  language=algorithm,
  style=algorithm,
  escapechar=\%,
  ]
Check(%\O%):
  %\B% = %$\emptyset$%
  for %$\o_1$%, %$\a_1$% in %\O% do
    for %$\o_2 \neq \o_1$%, %$\a_2$% in %\O% do
      if %$\a_1 \abstractdomainmeet \a_2 \neq \abstractdomainbottom$% then
        %\B% = %$\B \cup \{\a_1 \abstractdomainmeet \a_2\}$%
  return %\B%
\end{lstlisting}
\end{marginalgorithm}

Finally, the abstract implementation $\parallelqlibra$ computes the sum of the volumes of the biased portion of each partition together (\cf{} \refline{sum}).
The complement of the biased input space is an under-approximation of the fair input space.

The next result shows that the abstract implementation $\parallelqlibra$ is a sound under-approximation of the concrete implementation $\qlibra$.
Therefore, it can be used to validate the $\boundedqlibrage$ property with the $\ge$ operator.


\begin{theorem}
  Let $\boundedqlibrage$ be the property of interest we want to verify for the model $\defmodel$ and the input variable $\definputvariables\in\setof\inputvariables$.
  Given an abstract domain $\abstractdomain$, and the budget $\tune = \langle \upperbound, \lowerbound, \maxupperbound, \minlowerbound\rangle$, the following implication holds:
  \begin{align*}
    \parallelqlibra_{\defmodel, \abstractdomain}(\definputvariables, \tune) = \defbound' \land \defbound' \ge \defbound \ImplieS \defmodel \satisfies \BOUNDEDQLIBRAGE
  \end{align*}
\end{theorem}
\begin{proof}
  $\parallelqlibra_{\defmodel, \abstractdomain}(\definputvariables, \tune)$ in \refalg{parallel-qlibra} first computes the abstract activation patterns that cover a fraction \C{} of the input space in which the analysis is feasible (\cf{} from \refline{A} to \refline{B}).
  Then, it computes an over-approximation of the regions of \C{} that yield each target class $\node[\outputlayer]$ (\cf{} \refline{backward}).
  Thus, it actually computes an over-approximation of the parallel semantics $\parallelsemantics$.
  In \refline{sum}, the complement of the sum of the volumes, one for each fair partition (\cf{} \refdef{fair-input-partition}), is always greater than the volume of the fair input space. We conclude $\defmodel \satisfies \BOUNDEDQLIBRAGE$ by \reflemma*{sum-qlibra}.
\end{proof}

Notably, the parallel implementation $\parallelqlibra$ yields the so called \emph{perfectly parallel fairness analysis}~\sidecite{Urban2020}.

} % end of the scope of the newcommand

\section{Related Work}
\labsec{sas21-related-work}

In this section, we discuss related work on the verification of neural networks and the quantification of input usage in machine learning models.

\paragraph{Verification of Neural Networks.}

The interest in the verification of neural networks has significantly increased over the last decade. For an introduction to the topic, refer to Albarghouthi's tutorial \sidecite{Albarghouthi2021}.
The most common approach in this domain is to verify the robustness of neural networks against minimal input perturbations \sidecite{Goodfellow2016}.
This research area is highly diverse and includes a variety of techniques, including SMT-based methods \sidecite{Katz2017,Katz2019,Pulina2011a,Pulina2011b,Pulina2012}, mixed-integer programming \sidecite{Botoeva2020,Lomuscio2017,Tjeng2019}, branch-and-bound algorithms \sidecite{Bunel2018,Palma2021,Ehlers2017,Wang2018b}, over-approximation of non-linear activation functions \sidecite{Ehlers2017,Singh2019,Weng2018,Zhang2018b}, and symbolic propagation \sidecite{Botoeva2020,Henriksen2020,Wang2018b,Wang2021b}.
For a comprehensive survey of the field, see \sidetextcite{K_onig2024} and \sidetextcite{Huang2020}.
Notably, no single algorithm consistently outperforms others across all verification problem instances.

The literature also extensively addresses fairness and bias in machine learning models.
For example, statistical approaches include \sidetextcite{Galhotra2017}, which proposes an efficient method for fairness testing, \sidetextcite{Udeshi2018}, which generates discriminatory inputs for machine learning models, and \sidetextcite{Tram_er2017}, which introduces the unwarranted-associations framework.
On the other hand, \sidetextcite{Bastani2019} provides probabilistic guarantees on the fairness of machine learning models, while \sidetextcite{Urban2020} uses abstract interpretation to verify fairness properties in neural networks, offering formal guarantees.
Additionally, \sidetextcite{Albarghouthi2017b} encodes the fairness problem as a probabilistic program property, verified using an SMT solver.
Furthermore, \sidetextcite{Albarghouthi2019} proposes a technique to repair biases in neural networks.

Recent formal developments~\sidecite{Ranzato2020,Ranzato2021,Pal2022} are expanding this research to other machine learning models, such as decision trees and support vector machines, by developing formal methods to analyze input variable usage. Quantitative analyses have the potential to enhance these techniques by leveraging a broader spectrum of possible results.


\paragraph{Quantitative Verification of Neural Networks.}

There is also a growing interest in the quantification of properties in the context of verification of neural networks, with most of the literature providing statistical guarantees.
While our thesis does not address probabilistic properties, it offers formal guarantees regarding the quantification of impact properties. For example, \sidetextcite{Tran2023} quantifies the probability of safety violations by propagating Gaussian-distributed inputs through neural networks. Similarly, Baluta et al. \sidecite{Baluta2019,Baluta2021} estimate the portion of the input space that satisfies properties such as robustness, fairness, or security, through sampling. These estimates come with statistical guarantees of being within a specified confidence interval. The framework presented by \sidetextcite{Zhang2021,Zhang2023} quantifies adversarial robustness in binarized neural networks by model counting adversarial examples. Another statistical approach, \sidecite{Webb2019}, uses Monte Carlo methods to estimate the likelihood of property violations, thereby providing robustness metrics. These works can be viewed as quantifications of the \outcomesname{} quantifier for fairness checks in a weaker, statistical sense.


\section{Summary}

In this chapter, we presented the \changesname{} and \qlibraname{} impact quantifiers, and their abstract implementations, $\abstractchangesname$ and $\abstractqlibraname$, respectively.
We validated the $\defbound$-bounded impact property for both quantifiers.
To address the scalability issues of the backward analysis, we introduced the parallel semantics and the abstract implementation $\parallelqlibraname$.
The next chapter will present the experimental results of the \changesname{} and \qlibraname{} impact quantifiers applied to neural networks.
Afterwards, the thesis will focus on intensional properties, specifically how to quantify the influence of input variables on the number of iterations of a program.


\frenchdiv

\emph{Dans ce chapitre, nous avons présenté les notions d'impact \changesname{} et \qlibraname{}, ainsi que leurs implémentations abstraites, $\abstractchangesname$ et $\abstractqlibraname$, respectivement. Nous avons validé la propriété d'impact bornée par $\defbound$ pour les deux notions. Pour aborder les problèmes de scalabilité de l'analyse rétrospective, nous avons introduit les sémantiques parallèles et l'implémentation abstraite $\parallelqlibraname$. Le prochain chapitre présentera les résultats expérimentaux des notions d'impact \changesname{} et \qlibraname{} appliquées aux réseaux neuronaux. Par la suite, la thèse se concentrera sur les propriétés intentionnelles, en particulier sur la manière de quantifier l'influence des variables d'entrée sur le nombre d'itérations d'un programme.}
