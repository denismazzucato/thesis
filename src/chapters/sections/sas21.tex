\chapter{Quantitative Verification for Neural Networks}
\labch{quantitative-fairness}

In this chapter, we formally define the concept of feed-forward deep neural networks for classification purposes.
We introduce two quantitative impact notions: the \changesname{} impact notion, which targets the repetitions of changes in the outcome, and the \qlibraname{} impact notion, which measures the amount of bias of a neural network. We then present the abstract implementation of the two impact notions, respectively called $\abstractchangesname$ and $\abstractqlibraname$, and we show how to validate the $\defbound$-bounded impact property for both notions. Finally, we present the parallel implementation $\parallelqlibraname$ for efficient neural network verification of the $\qlibraname$ impact notion.
This chapter is based on the work presented at the 28th Static Analysis Symposium (SAS) 2021~\cite{Mazzucato2021}.

\emph{Dans ce chapitre, nous définissons formellement le concept de réseaux neuronaux profonds à propagation avant à des fins de classification.
Nous introduisons deux notions d'impact quantitatif : la notion d'impact \changesname{}, qui cible les répétitions de changements dans le résultat, et la notion d'impact \qlibraname{}, qui mesure le degré de biais d'un réseau neuronal. Nous présentons ensuite la mise en œuvre abstraite des deux notions d'impact, respectivement appelées $\abstractchangesname$ et $\abstractqlibraname$, et nous montrons comment valider la propriété d'impact bornée par $\defbound$ pour les deux notions. Enfin, nous présentons la mise en œuvre parallèle $\parallelqlibraname$ pour une vérification efficace des réseaux neuronaux selon la notion d'impact $\qlibraname$.
Ce chapitre est basé sur les travaux présentés au 28ème Symposium d'Analyse Statique (SAS) 2021~\sidecite{Mazzucato2021}.}

% \input{src/chapters/extensional/sas21/overview}
% \input{src/chapters/extensional/sas21/changes}
% \input{src/chapters/extensional/sas21/qlibra}
% \input{src/chapters/extensional/sas21/evaluation-changes}
% \input{src/chapters/extensional/sas21/evaluation-qlibra}

\section{Neural Networks}
\labsec{neural-networks}

This section introduces the computational model of a feed-forward deep neural network and how it can be used for classification purposes.
\subsection{Feed-Forward Deep Neural Networks}

A \emph{feed-forward deep neural network} is a directed acyclic graph where each node represents a neuron, and each edge represents a connection between neurons. The nodes are organized into layers: the first layer is the input layer ($\inputlayer$), the last layer is the output layer ($\outputlayer$), and the layers in between are called hidden layers ($\hiddenlayers$). Each hidden layer $\hiddenlayer{i}$ consists of $\cardinalitynospaces{\hiddenlayer{i}}$ nodes and is connected to the previous layer $\hiddenlayer{i-1}$ through a weight matrix $\weightmatrix{i}$ of size $\cardinalitynospaces{\hiddenlayer{i}}\times\cardinalitynospaces{\hiddenlayer{i-1}}$ and a bias vector $\biasvector{i}$ of size $\cardinalitynospaces{\hiddenlayer{i}}$.

The entire set of nodes in the network is denoted by $\networknodes$, and the set of nodes in layer $\hiddenlayer{i}$ is denoted by $\networknodesinlayer{i}$. The $j$-th node in layer $\hiddenlayer{i}$ is denoted by $\node$. In this thesis, we focus on feed-forward neural networks used for classification tasks, where each node in the output layer represents a class, and the output of the network is the class corresponding to the highest value. The network has $\cardinalitynospaces{\outputlayer}$ target classes in total.

An input value is a $\cardinalitynospaces{\inputlayer}$-dimensional vector $\networknodesinlayer{0}$. For simplicity, we assume that the input values are normalized to the interval $[0,1]$. The value of each hidden and output node is computed by applying an activation function to the weighted sum of the values of the nodes in the previous layer and the bias. The activation function is usually a non-linear function; in this work, we consider the \relu{} activation function, defined as $\relu(x) = \max(0, x)$. Thus, the value of node $\node$ in layer $\hiddenlayer{i}$ is computed as:
\begin{align*}
\node = \relu\left(
\sum_{k=1}^{\cardinalitynospaces{\hiddenlayer{i-1}}} \weight_{j,k}^{i} \node[i-1][k] + \bias_{j}^{i}
\right)
\end{align*}
where $\weight_{j,k}^{i}$ is the weight connecting node $\node[i-1][k]$ in layer $\hiddenlayer{i-1}$ to node $\node$ in layer $\hiddenlayer{i}$, and $\bias_{j}^{i}$ is the bias of node $\node$ in layer $\hiddenlayer{i}$. Weights and biases are learned during the training phase of the network. In the following, we consider already trained networks.

For the rest of this chapter, the letter $\defmodel$ denotes a neural network model\sidenote{Analogous to the letter $\defprogram$ for generic programs.}. The trace semantics of a neural network model is denoted by $\tracesemanticsnoparam\semanticsof{\defmodel}\in\tracetype$.
Note that, under the models we consider in this thesis, we have that the output of the network is deterministic for each input configuration and that all the traces terminate after a specific number of steps, which is exactly the number of nodes of the network.

\subsection{Classification Task}

In this thesis, we focus on feed-forward neural networks used for classification tasks. Given an input vector, the network classifies it into one of the target classes, each represented by a node in the output layer. The network classifies an input vector by computing the values of the nodes in the output layer and then selecting the class associated with the node having the highest value, i.e., $\argmax_{j} \node[\lastlayerindex][j]$.

In the quantitative framework, the classification task is formalized using the output observer $\outputobs$ (\refdef*{output-observer}), which maps the network's output values to the target classes. The output observer returns $1$ if the index of the node with the highest value matches the target class, and $0$ otherwise. Thus, when the output state of a trace from the network computation is observed by $\outputobs$, the only non-zero value corresponds to the target class. Formally, the output observer is defined as:

\begin{align*}
\outputobs(\defstate) \DefeQ \lambda \defvariable.\spacer
\begin{cases}
1 & \text{if } \argmax_{j} \defstate(j) = \defvariable \\
0 & \text{otherwise}
\end{cases}
\end{align*}

This characterization shows that two different states of the network can be distinguished by the output observer if and only if they are associated with different target classes, thus yielding different outcomes.

\begin{example}
Consider a simple neural network with two input and two output variables. The weights and biases connecting the input to the output layer are irrelevant for this example. Therefore, the network states are defined as $\state = \setdef{\langle x_{0, 0}, x_{0, 1}, x_{1, 0}, x_{1, 1} \rangle}{x_{0, 0}, x_{0, 1}, x_{1, 0}, x_{1, 1} \in [0,1]}$.
Given, for instance, the neural-network state $\defstate=\langle 0, 1, 0, 1 \rangle$, the output observer $\outputobs$ returns the state $\outputobs(\defstate) = \langle 0, 0, 0, 1 \rangle$ to indicate that the target class is the second one, corresponding to $x_{1, 1}$. Given the network state $\langle 0.5, 1, 0.7, 0.2 \rangle$, the output observer returns the state $\outputobs(\defstate) = \langle 0, 0, 1, 0 \rangle$ to indicate that the target class is the first one (the first two variables are the input variables), and so on.
\end{example}

Note that the output observer always maps the values of neural network nodes that do not belong to the output layer to zero.

\subsection{Neural Network Abstract Analysis}

Abstract interpretation of a neural network model involves propagating an abstract element through each neuron of the network, over-approximating the possible states the model may reach. The analysis can be performed in two directions: forward, from the input layer to the output layer, or backward, from the output layer to the input layer.
The networks considered in this thesis are acyclic, meaning that the abstract iterator always reaches the fixpoint after a finite number of iterations, without the need of traverse the same node multiple times.
The forward analysis starts with an abstract element representing the input values and returns an abstract element representing an over-approximation of the reachable target classes. Conversely, the backward analysis begins with an abstract element representing the target classes and returns an abstract element representing an over-approximation of the input values that may lead to one of the given target classes.

When a neural network processes an input vector, it computes the value of each node via an affine transformation followed by an activation function, in our case, the \relu{} function. The \relu{} function sets negative values to zero and leaves positive values unchanged. A node is said to be \emph{active} if its value is positive and \emph{inactive} otherwise. This flag is called the node's \emph{activation status}. The set of activation statuses of all nodes in the network is called the \emph{activation pattern}, which induces a \emph{path} in the neural network from
regions in the input space where the network behaves similarly.


An \emph{abstract activation pattern} is a partial activation pattern that assigns a fixed activation status only to a subset of \relu{} nodes, thus representing a set of possible activation patterns. \relu{} nodes with an unknown activation status are those whose corresponding flag does not have a fixed value. Abstract activation patterns are valuable for the abstract analysis of networks as the knowledge of a node status can prune the search space, reducing computational cost. Specifically, the \relu{} function is simpler to handle when the activation status is known: it behaves as the identity function for active nodes and as a constant zero for inactive nodes. The abstract activation pattern can be discovered from a forward analysis of the network: if the over-approximation of input values to a node is greater than zero, the node is considered active; if less than zero, inactive; otherwise, unknown.


\marginnote[*-10]{\denis{In Latex, make two different enviroments for algorithms and programs, so that in the text we can just reference ALgorithm 1. Similar to how we handle Programs.}}
\begin{marginlisting}
  \caption{Forward analysis of neural networks.}
  \labfig{forward-neural-networks}
\vspace{0.8cm}
\begin{lstlisting}[
  language=algorithm,
  escapechar=\%,
  ]
Forward(%$\abstractdomain$%, %$\defabstractvalue$%):
  p = %$\emptyset$%
  for %$i$% = 1 up to %$\lastlayerindex$%:
    for %$j$% = 1 up to %$\cardinalitynospaces{\hiddenlayer{i}}$%:
      %$\defabstractvalue$% = %$\abstractdomainforwardaffine\semanticsof{\node}\defabstractvalue$%
      %$\defabstractvalue, p$% = %$\abstractdomainforwardrelu\semanticsof{\node}\defabstractvalue$%
  return %$\defabstractvalue, p$%
\end{lstlisting}
\end{marginlisting}


Given an abstract domain $\abstractdomain$ with sound forward and backward operators for affine and \relu{} operations, the forward analysis can be defined as network computation using the abstract sound counterparts of the concrete operators. \reffig{forward-neural-networks} illustrates a forward abstract analysis of neural network models, parameterized by the abstract domain $\abstractdomain$ and an abstract element $\defabstractvalue$ representing the input values. The abstract operator $\abstractdomainforwardaffine$ computes the affine transformation of a given node, while $\abstractdomainforwardrelu$ handles the \relu{} activation function and determines the node's activation status.

\marginnote{\denis{I should say that given input partition for forward analysis and target class for backward analysis as in oopsla, so that the first line of the algorithm create the initial abstract value.}}
Conversely, backward analysis of neural networks replaces concrete operators with their abstract sound \emph{backward} counterparts. This analysis is also parameterized by the abstract domain $\abstractdomain$ and an abstract element $\defabstractvalue$ representing the target classes.
\reffig{backward-neural-networks} depicts the backward abstract analysis of neural network models, where the abstract operator $\abstractdomainbackwardaffine$ computes the backward affine transformation of a given node, and $\abstractdomainbackwardrelu$ manages the \relu{} activation function given the node's activation status.
\marginnote{\denis{Add algorithm backward analysis here on the side.}}

Both analyses are sound, meaning they over-approximate the possible states the network may reach.
Before formally showing the soundness of the forward and backward analyses, we introduce the forward concretization $\forwardconcretization$.
Similar to the backward concretization $\backwardconcretization$, it is defined as:
\[
  \forwardconcretization(\forwardsemanticsnoparam)\defabstractvalue \DefeQ
  \nonemptysetof{\setdef{
    \inputoutputtuple{\defstate}
  }{
    \retrieveinput{\defstate}\in\abstractdomainconcretization(\forwardsemanticsnoparam(\defabstractvalue))\land\retrieveoutput{\defstate}\in\abstractdomainconcretization(\defabstractvalue)
    }
  }
  \]

\begin{lemma}[Soundness of Forward Neural Network Analysis]\lablemma{soundness-forward-network}
  Given a neural network model $\defmodel$ and an abstract domain $\abstractdomain$ with sound forward operators for affine and \relu{} operations, the forward analysis of the network is sound whenever it holds that:
  \[
    \reduce[\outputsemanticsnoparam\semanticsof{\defmodel}]{\abstractdomainconcretization(\defabstractvalue)} \SubseteQ \forwardconcretization(\abstractdomainforward(\defmodel))\defabstractvalue
  \]
\end{lemma}
\begin{proof}
  Trivially follows from the definition of the forward concretization and the soundness of the network operators.
\end{proof}
\marginnote{\denis{Missing definition of semantics restriction over the input states.}}


The next result instead shows the soundness of the backward analysis of neural networks.

\begin{lemma}[Soundness of Backward Neural Network Analysis]\lablemma{soundness-backward-network}
  Given a neural network model $\defmodel$ and an abstract domain $\abstractdomain$ with sound backward operators for affine and \relu{} operations, the backward analysis of the network is sound whenever it holds that:
  \[
    \reduce[\outputsemanticsnoparam\semanticsof{\defmodel}]{\abstractdomainconcretization(\defabstractvalue)} \SubseteQ \backwardconcretization(\abstractdomainbackward(\defmodel))\defabstractvalue
  \]
\end{lemma}
\begin{proof}
  Trivially follows from the definition of the backward concretization and the soundness of the network operators.
\end{proof}


\subsection{Abstract Domains for Neural Network Analysis}

\marginnote{\denis{Add figures on the side.}}
Different abstract domains can be used for the forward pre-analysis of neural networks.
The choice of the abstract domain depends on the trade-off between precision and scalability.
Here, we present four abstract domains: \boxes, \symbolic, \deeppoly, and \neurify. Additionally, a generic reduced product domain construction, called \reducedproduct, to combine any of these domains together.

\paragraph{Boxes}


The \boxes{} domain simply uses interval arithmetic \sidecite{Hickey2001} to compute concrete lower and upper bound estimations $l$ and $u$ for the value of each neuron \texttt{x} in the neural network.


\paragraph{Symbolic Constant Propagation}

The \symbolic{} domain combines \boxes{} with symbolic constant propagation \sidecite{Mine2006symbolic}: in addition to being bounded by concrete lower and upper bounds, the value of each neuron \texttt{x} is represented symbolically as a linear combination of the input neurons and the value of the non-fixed \relu{} nodes in previous layers. Specifically, given \texttt{x} bounded by $l < 0$ and $u > 0$, $\relu(x)$ is represented by a fresh symbolic variable bounded by $0$ and $u$ (\cf{} \reffig{naive}). By retaining variable dependencies, symbolic representations yield a tighter over-approximation of the value of each neuron in the network.


\paragraph{DeepPoly}

The \deeppoly{} domain \sidecite{Singh19} associates to each neuron $x$ of a neural network concrete lower and upper bounds $l$ and $u$ as well as symbolic bounds expressed as linear combinations of neurons in the preceding layer of the network.
%
The concrete bounds are computed by back-substitution of the symbolic bounds up to the input layer. Non-fixed \relu{} nodes are over-approximated by partially retaining dependencies with preceding neurons using the tighter convex approximation between those shown in \reffig{deeppoly} (i.e., the approximation shown on the left when $u \leq -l$, and the approximation shown on the right otherwise).

\paragraph{Neurify}

The \neurify{} domain \sidecite{Wang18} similarly maintains symbolic lower and upper bounds $low$ and $up$ for each neuron $x$ of neural network. Unlike \deeppoly, concrete lower and upper bounds are computed for \emph{each} symbolic bound: $l_{low}$ and $u_{low}$ for the symbolic lower bound, and $l_{up}$ and $u_{up}$ for the symbolic upper bound.
%
The over-approximation of non-fixed \relu{} nodes is done \emph{independently} for each symbolic bound, i.e., for the $low$ bound if $l_{low} < 0 < u_{low}$, and for the $up$ bound if $l_{up} < 0 < u_{up}$.
%
\reffig{neurify} shows the approximation for $l = l_{low} = l_{up}$ and $u = u_{low} = u_{up}$. In general, the slope of the symbolic constraints will differ through successive approximation steps.

\paragraph{Reduced Product}

Finally, the \emph{Product Builder} \sidecite{Mazzucato2021} provides a parametric interface for constructing the product of the above domains.
The reduction function consists in an exchange of concrete bounds between domains. In particular, this allows determining tighter lower and upper bound estimations for each neuron in the network and thus reducing the over-approximation error introduced by the \relu{} nodes.
New abstract domains only need to implement the interface to share bounds information to enable their combination with other domains by the Product Builder.


% \denis{Add lemmas showing that the forward and backward analyses are sound (maybe compared to the output semantics).}

\section{Quantitative Impact Notions}
\labsec{quantitative-impact-notions}

This section elaborates on quantitative notions that can be used to measure the influence of input variables on the output of a neural network.
Namely, we introduce the \changesname{} and \qlibraname{} impact notions, which are specifically designed to handle the irregular input space produced by neural-network models.

\subsection{The \changesname{} Impact Notion}[\changesname]
\labsec{changes-impact-notion}


The \changesname{} impact notion is designed to overcome the limitations of the impact notions defined in the previous chapter when applied in the context of neural networks.
Indeed, in the presence of neural networks, the input space is generally irregular, and all the possible input variations lead to all the possible outcomes.
Therefore, the impact notions defined in the previous chapter would not be useful as they measure the impact of input variables based on the number (\eg, \outcomesname{}) or the magnitude (\eg, \rangename{}) of the output values; even \qusedname{} would be meaningless in this setting as all the classification targets are often reachable by all the input values.
Therefore, these metrics would not provide any meaningful information about how the input variables influence the network's outcome.

\begin{example}
  \begin{marginfigure}
  \begin{tikzpicture}[scale=0.9]
    % Grid
    % \draw[help lines, color=gray!30, dashed] (-0.1,-0.1) grid (4.1,4.1);
    % x-axis ticks
    % \foreach \x in {-4,-3,-2,-1,0,1,2,3,4}
    %     \draw (\x+5,0.1) -- (\x+5,-0.1) node[below] {\x};
    % % y-axis ticks
    % \foreach \y in {1,2,3}
    %     \draw (0.1,\y) -- (-0.1,\y) node[left] {\y};
    % Polyhedra
    \fill[color=seabornYellow, opacity=0.5] (1,1.75) -- (2,2.75) -- (2.5,1.75) -- (3,2.75) -- (3,4) -- (2,4) -- cycle;
    % \draw[color=seabornYellow, ultra thick] (1,1.75) -- (2,2.75) -- (3,2.75) -- (3,4) -- (2,4) -- cycle;
    % Polyhedra
    \fill[color=seabornGreen, opacity=0.5] (0,2.25) -- (0.5,2.25) -- (1,1.75) -- (2,4) -- (0,4) -- cycle;
    \draw[color=seabornGreen, ultra thick] (0,2.25) -- (0.5,2.25) -- (1,1.75) -- (2,4) -- (0,4) -- cycle;
    \fill[color=seabornGreen, opacity=0.5] (3,2.75) -- (4,1.5) -- (4,4) -- (3,4) -- cycle;
    \draw[color=seabornGreen, ultra thick] (3,2.75) -- (4,1.5) -- (4,4) -- (3,4) -- cycle;
    % Polyhedra
    \fill[color=seabornRed, opacity=0.5] (0,0) -- (4,0) -- (4,1.5) -- (3,2.75) -- (2.5,1.75) -- (2,2.75) -- (1,1.75) -- (0.5,2.25) -- (0,2.25) -- cycle;
    \draw[color=seabornRed, ultra thick] (0,0) -- (4,0) -- (4,1.5) -- (3,2.75) -- (2.5,1.75) -- (2,2.75) -- (1,1.75) -- (0.5,2.25) -- (0,2.25) -- cycle;
    % Nodes
    % \fill[color=seabornRed] (0+1,0+1) circle[radius=2pt];
    % \node[above left] at (0+1,0+1) {$3$};
    % x-axis
    \draw[->,ultra thick] (0,0)--(4.3,0) node[below]{$x$};
    \draw[ultra thick] (0,4)--(4,4);
    % y-axis
    \draw[->,ultra thick] (0,0)--(0,4.3) node[left]{$y$};
    \draw[ultra thick] (4,0)--(4,4);

    \draw[dashed] (0,2.1) node[left] {$y_0$} -- (4,2.1);
    \draw[dashed] (1.75,0) node[below] {$x_0$} -- (1.75,4);
    % little dot in the intersection x_0 y_0
    \fill[color=black] (1.75,2.1) circle[radius=2pt];
  \end{tikzpicture}
    \caption{Input space with two input variables ($x$ and $y$) and three possible outcomes (green, yellow, red).}
    \label{fig:irregular}
  \end{marginfigure}
  Consider a simple neural network with two input variables $x$ and $y$, and three possible outcomes, denoted by the colors green, yellow, and red.
  A potential input space of such a model $\defmodel$ is represented in \reffig{irregular}, indeed each outcome is reachable by the portion of the input space that is colored with the corresponding color.
  In this case, the \outcomesname{} impact notion would not be useful as all the outcomes are reachable by perturbations of any input value. Consider for example the point $(x_0, y_0)$ in the input space of outcome red, by applying a perturbation of one axis, the outcome can change to green or yellow, for both axes. Thus, the impact of both $x$ and $y$ is maximal, \cf{} $\outcomesname_x(\semanticsof\defmodel) = \outcomesname_y(\semanticsof\defmodel) = 3$.
  A similar reasoning applies to the \rangename{} or \qusedname{} impact notions, hence neither of them provides any insight.

  On the other hand, we notice that one way to discriminate which input variable is more impactful is to consider the number of times the classification changes when the input variables are modified.
  To this end, we develop a quantitative notion, called \changesname{} impact notion, that measures exactly this property.
\end{example}


The \changesname{} impact notion is designed to count how many times the network outcome changes by modification in the value of the input variables $\definputvariables$.
Notably, we recall similarities with the previously defined $\outcomesname$ impact notion: in this case, we consider changes in the outcome \emph{with repetitions}, that is, if two different variations in $\definputvariables$ result in the same change in outcome, it counts as double change.
The higher the number of changes in the outcome, the greater the influence on the program outcome.
Therefore, this notion demonstrates its effectiveness when the same outcomes are reachable by multiple variations.
Intuitively, \emph{such situation often arises in the presence of neural networks, where generally all the possible input variations lead to every possible outcome}.
The design of this notion builds on this key observation.
Thus, counting the repetitions is a potential solution to define a meaningful impact definition for neural networks.


\begin{example}
  Consider the input point $(x_0, y_0)$ in the input space of the neural network in \reffig{irregular}.
  By applying a perturbation of the $x$-axis, the outcome can change to green or yellow multiple times, a higher number of changes in the outcome is observed compared to the $y$-axis.
\end{example}


In practice, the \changesname{} impact definition considers variations in the value of input configurations that do not belong to the same \emph{continuous region}, containing no gaps or interruptions.
We first define the function $\segments$, which takes as input a set of traces $\defsetoftraces$ and an output value $\defoutput$.
This function partitions the set of traces $\defsetoftraces$ into continuous subsets with respect to the input variables $\definputvariables$.
Each subset $\defsetoftraces'$ satisfies three conditions: (1) all the traces in $\defsetoftraces'$ share the same output value $\defoutput$, (2) for any two traces in $\defsetoftraces'$, there is no other trace
in $\defsetoftraces$ with an input value between the two traces leading to a different output value, and (3) the subset is maximal, that is, there is no other trace in $\defsetoftraces$ that can be added to $\defsetoftraces'$ without violating the first two conditions.
The function $\segments$ is defined as follows:


\begin{definition}[Segments of Continuous Regions]\labdef{segments}
  Let $\definputvariables\in\inputvariables$ be the set of input variables of interest.
  Given a set of traces $\defsetoftraces\in\tracetype$ and an output value $\defoutput\in\stateandbottom$, the function $\segments\in\pair\tracetype\stateandbottom\to\setof\tracetype$ is defined as:
  \begin{align*}
    \segments(\defsetoftraces, \defoutput) &\DefeQ
      \setdef{
        \defsetoftraces' \in \phi(\defsetoftraces)}{
          \foralldef{\defsetoftraces''\supset\defsetoftraces'}{
            \defsetoftraces''\notin\phi(\defsetoftraces)
          }
        }\\
    \text{where } \phi(\defsetoftraces) &\DefeQ
      \setdef*{
        \defsetoftraces'\subseteq\defsetoftraces
      }{
        \forall \deftrace\in\defsetoftraces'.\spacer \outputobs(\retrieveoutput{\deftrace}) = \defoutput \LanD \\
        \forall \deftrace, \deftrace'\in\defsetoftraces'.\spacer \exists \deftrace''\in\defsetoftraces.\spacer \\
          \quad\retrieveinput{\deftrace}(\definputvariables) \le \retrieveinput{\deftrace''}(\definputvariables) \le \retrieveinput{\deftrace'}(\definputvariables) \implies \deftrace'' \in \defsetoftraces'
      }
  \end{align*}
\end{definition}

Note that, the auxiliary function $\phi$ partitions the set of traces $\defsetoftraces$ into subsets that satisfy the first two conditions: (1) all the traces in the subset share the same output value $\defoutput$, and (2) for any two traces in the subset (\cf{} $\outputobs(\retrieveoutput{\deftrace}) = \defoutput$), there is no other trace in $\defsetoftraces$ with an input value between the two traces leading to a different output value (\cf{} $\retrieveinput{\deftrace}(\definputvariables) \le \retrieveinput{\deftrace''}(\definputvariables) \le \retrieveinput{\deftrace'}(\definputvariables) \implies \deftrace'' \in \defsetoftraces'$).
The function $\segments$ then returns the maximal subsets (3) that satisfy the first two conditions.
To better illustrate how the function $\segments$ works, consider the following example.

\begin{marginfigure}
  \begin{tikzpicture}[scale=0.9]
    % Grid
    % \draw[help lines, color=gray!30, dashed] (-0.1,-0.1) grid (4.1,4.1);
    % x-axis ticks
    % \foreach \x in {-4,-3,-2,-1,0,1,2,3,4}
    %     \draw (\x+5,0.1) -- (\x+5,-0.1) node[below] {\x};
    % % y-axis ticks
    % \foreach \y in {1,2,3}
    %     \draw (0.1,\y) -- (-0.1,\y) node[left] {\y};
    % Polyhedra
    \fill[color=seabornYellow, opacity=0.5] (1,1.75) -- (2,2.75) -- (2.5,1.75) -- (3,2.75) -- (3,4) -- (2,4) -- cycle;
    % \draw[color=seabornYellow, ultra thick] (1,1.75) -- (2,2.75) -- (3,2.75) -- (3,4) -- (2,4) -- cycle;
    % Polyhedra
    \fill[color=seabornGreen, opacity=0.5] (0,2.25) -- (0.5,2.25) -- (1,1.75) -- (2,4) -- (0,4) -- cycle;
    \draw[color=seabornGreen, ultra thick] (0,2.25) -- (0.5,2.25) -- (1,1.75) -- (2,4) -- (0,4) -- cycle;
    \fill[color=seabornGreen, opacity=0.5] (3,2.75) -- (4,1.5) -- (4,4) -- (3,4) -- cycle;
    \draw[color=seabornGreen, ultra thick] (3,2.75) -- (4,1.5) -- (4,4) -- (3,4) -- cycle;
    % Polyhedra
    \fill[color=seabornRed, opacity=0.5] (0,0) -- (4,0) -- (4,1.5) -- (3,2.75) -- (2.5,1.75) -- (2,2.75) -- (1,1.75) -- (0.5,2.25) -- (0,2.25) -- cycle;
    \draw[color=seabornRed, ultra thick] (0,0) -- (4,0) -- (4,1.5) -- (3,2.75) -- (2.5,1.75) -- (2,2.75) -- (1,1.75) -- (0.5,2.25) -- (0,2.25) -- cycle;
    % Nodes
    % \fill[color=seabornRed] (0+1,0+1) circle[radius=2pt];
    % \node[above left] at (0+1,0+1) {$3$};
    % x-axis
    \draw[->,ultra thick] (0,0)--(4.3,0) node[below]{$x$};
    \draw[ultra thick] (0,4)--(4,4);
    % y-axis
    \draw[->,ultra thick] (0,0)--(0,4.3) node[left]{$y$};
    \draw[ultra thick] (4,0)--(4,4);


    \draw[color=white] (0,-1) node[left, color=black] {$\green{\defsetoftraces'}\left\{\right.$} -- (4,-1) node[right, color=black] {$\left.\right\}$};
    \draw[color=white] (0,-1.5) node[left, color=black] {$\yellow{\defsetoftraces''}\left\{\right.$} -- (4,-1.5) node[right, color=black] {$\left.\right\}$};
    \draw[color=white] (0,-2) node[left, color=black] {$\red{\defsetoftraces'''}\left\{\right.$} -- (4,-2) node[right, color=black] {$\left.\right\}$};

    % horizontal line at y_0
    \draw[very thick] (0,2.1) node[left] {$y_0$} -- (4,2.1);

    \draw[dashed] (0, -2) -- (0, 4);
    \draw[dashed] (0.65,-2) -- (0.65,4);
    \draw[dashed] (1.15,-1.5) -- (1.15,4);
    \draw[dashed] (1.35,-2) -- (1.35,4);
    \draw[dashed] (2.3,-2) -- (2.3,4);
    \draw[dashed] (2.68,-2) -- (2.68,4);
    \draw[dashed] (3.5,-2) -- (3.5,4);
    \draw[dashed] (4,-1) -- (4,4);

    \draw[very thick] (0.65,2) -- (0.65,2.2);
    \draw[very thick] (1.15,2) -- (1.15,2.2);
    \draw[very thick] (1.35,2) -- (1.35,2.2);
    \draw[very thick] (2.3,2) -- (2.3,2.2);
    \draw[very thick] (2.68,2) -- (2.68,2.2);
    \draw[very thick] (3.5,2) -- (3.5,2.2);

    \draw[very thick, color=seabornRed] (0,-0.5) node[left, color=black] {$y_0$} -- (0.65,-0.5);
    \draw[very thick, color=seabornGreen] (0.65,-0.5) -- (1.15,-0.5);
    \draw[very thick, color=seabornYellow] (1.15,-0.5) -- (1.35,-0.5);
    \draw[very thick, color=seabornRed] (1.35,-0.5) -- (2.3,-0.5);
    \draw[very thick, color=seabornYellow] (2.3,-0.5) -- (2.68,-0.5);
    \draw[very thick, color=seabornRed] (2.68,-0.5) -- (3.5,-0.5);
    \draw[very thick, color=seabornGreen] (3.5,-0.5) -- (4,-0.5);

    \draw[very thick] (0,-0.6) -- (0,-0.4);
    \draw[very thick] (0.65,-0.6) -- (0.65,-0.4);
    \draw[very thick] (1.15,-0.6) -- (1.15,-0.4);
    \draw[very thick] (1.35,-0.6) -- (1.35,-0.4);
    \draw[very thick] (2.3,-0.6) -- (2.3,-0.4);
    \draw[very thick] (2.68,-0.6) -- (2.68,-0.4);
    \draw[very thick] (3.5,-0.6) -- (3.5,-0.4);
    \draw[very thick] (4,-0.6) -- (4,-0.4);

    \draw[very thick, color=seabornGreen] (0.65,-1) -- (1.15,-1);
    \draw[very thick, color=seabornGreen] (3.5,-1) -- (4,-1);

    \draw[very thick] (0.65,-1.1) -- (0.65,-0.9);
    \draw[very thick] (1.15,-1.1) -- (1.15,-0.9);
    \draw[very thick] (3.5,-1.1) -- (3.5,-0.9);
    \draw[very thick] (4,-1.1) -- (4,-0.9);

    \draw[very thick, color=seabornYellow] (1.15,-1.5) -- (1.35,-1.5);
    \draw[very thick, color=seabornYellow] (2.3,-1.5) -- (2.68,-1.5);

    \draw[very thick] (1.15,-1.6) -- (1.15,-1.4);
    \draw[very thick] (1.35,-1.6) -- (1.35,-1.4);
    \draw[very thick] (2.3,-1.6) -- (2.3,-1.4);
    \draw[very thick] (2.68,-1.6) -- (2.68,-1.4);

    \draw[very thick, color=seabornRed] (0,-2) -- (0.65,-2);
    \draw[very thick, color=seabornRed] (1.35,-2) -- (2.3,-2);
    \draw[very thick, color=seabornRed] (2.68,-2) -- (3.5,-2);

    \draw[very thick] (0,-2.1) -- (0,-1.9);
    \draw[very thick] (0.65,-2.1) -- (0.65,-1.9);
    \draw[very thick] (1.35,-2.1) -- (1.35,-1.9);
    \draw[very thick] (2.3,-2.1) -- (2.3,-1.9);
    \draw[very thick] (2.68,-2.1) -- (2.68,-1.9);
    \draw[very thick] (3.5,-2.1) -- (3.5,-1.9);


  \end{tikzpicture}
    \caption{Function \segments.}
    \label{fig:segments}
  \end{marginfigure}

\begin{example}
  Let us consider the set of traces from the neural network in \reffig{irregular} with $y=y_0$, we call this set $\defsetoftraces$.
  The function $\segments$ partitions the set of traces $\defsetoftraces$ into three subsets, each containing the traces leading to the same output value.
  \reffig{segments} shows the three partitions graphically, respectively for the outcome green ($\green{\defsetoftraces'}$), yellow ($\yellow{\defsetoftraces''}$), and red ($\red{\defsetoftraces'''}$).
\end{example}

Formally, \changesname{} is defined as the maximum, for each input configuration, of the number of continuous regions in which the output value changes.
In words, a set of input variables $\definputvariables$ has a high impact on the network outcome if the output value changes in many continuous regions when the input variables are modified.


\begin{definition}[\changesname]\labdef{changes}
  Given a set of input variables of interest $\definputvariables\in\setof\inputvariables$, and an output observer $\outputobs$,
  the quantity $\changes\in\tracetype\to\Nplus$ is defined as:
  \begin{align*}
    \changes(\defsetoftraces) &\DefeQ
      \sup_{\definput\in\reducedstate}
        \sup_{\defoutput\in\stateandbottom}
          \cardinality{Q_{\definputvariables, \definput, \defoutput}} \\
    \text{where } Q_{\definputvariables, \definput, \defoutput} &\DefeQ
      \bigsetjoin_{\defoutput' \in \stateandbottom\setminus\{\defoutput\}}
        \segments(
          \setdef{\deftrace\in\defsetoftraces}{\retrieveinput\deftrace \stateeq{\inputvariableswithoutw} \definput}, \defoutput'
        )
  \end{align*}
\end{definition}
The auxiliary set $Q_{\definputvariables, \definput, \defoutput}$ contains the set of continuous regions leading to an output value different from $\defoutput$.
Additionally, $\changes$ requires the set of traces $\defsetoftraces$ to be deterministic, that is, for each input configuration, there is only one output value. Such a requirement is necessary to ensure that segments are well-defined, as the function $\segments$ partitions the input space into continuous regions leading to the same output value. Otherwise, in a continuous portion of input space where the output value is not deterministic, the number of possible outcomes changes would be infinite.
Therefore, the \changesname{} impact notion would be meaningless in such a setting.
Nevertheless, since we consider neural networks for classification tasks in this chapter, the output value is deterministic for each input configuration, and thus the set of traces $\defsetoftraces$ is deterministic.

\begin{marginfigure}
  \begin{tikzpicture}[scale=0.9]

    \draw[very thick, color=seabornRed] (0,1) node[left, color=black] {$y_0$} -- (0.65,1);
    \draw[very thick, color=seabornGreen] (0.65,1) -- (1.15,1);
    \draw[very thick, color=seabornYellow] (1.15,1) -- (1.35,1);
    \draw[very thick, color=seabornRed] (1.35,1) -- (2.3,1);
    \draw[very thick, color=seabornYellow] (2.3,1) -- (2.68,1);
    \draw[very thick, color=seabornRed] (2.68,1) -- (3.5,1);
    \draw[very thick, color=seabornGreen] (3.5,1) -- (4,1);

    \draw[very thick] (0,0.9) -- (0,1.1);
    \draw[very thick] (0.65,0.9) -- (0.65,1.1);
    \draw[very thick] (1.15,0.9) -- (1.15,1.1);
    \draw[very thick] (1.35,0.9) -- (1.35,1.1);
    \draw[very thick] (2.3,0.9) -- (2.3,1.1);
    \draw[very thick] (2.68,0.9) -- (2.68,1.1);
    \draw[very thick] (3.5,0.9) -- (3.5,1.1);
    \draw[very thick] (4,0.9) -- (4,1.1);

    \draw[thick, ->] (0.9, 1.1) to[out=120, in=45] (0.32, 1.1);
    \draw[thick, ->] (0.9, 1.1) to[out=45, in=120] (1.28, 1.1);
    \draw[thick, ->] (0.9, 1.1) to[out=45, in=120] (1.9, 1.1);
    \draw[thick, ->] (0.9, 1.1) to[out=45, in=120] (2.5, 1.1);
    \draw[thick, ->] (0.9, 1.1) to[out=45, in=120] (3.1, 1.1);



    \draw[very thick, color=seabornRed] (0,0) node[left, color=black] {$y_0$} -- (0.65,0);
    \draw[very thick, color=seabornGreen] (0.65,0) -- (1.15,0);
    \draw[very thick, color=seabornYellow] (1.15,0) -- (1.35,0);
    \draw[very thick, color=seabornRed] (1.35,0) -- (2.3,0);
    \draw[very thick, color=seabornYellow] (2.3,0) -- (2.68,0);
    \draw[very thick, color=seabornRed] (2.68,0) -- (3.5,0);
    \draw[very thick, color=seabornGreen] (3.5,0) -- (4,0);

    \draw[very thick] (0,-0.1) -- (0,0.1);
    \draw[very thick] (0.65,-0.1) -- (0.65,0.1);
    \draw[very thick] (1.15,-0.1) -- (1.15,0.1);
    \draw[very thick] (1.35,-0.1) -- (1.35,0.1);
    \draw[very thick] (2.3,-0.1) -- (2.3,0.1);
    \draw[very thick] (2.68,-0.1) -- (2.68,0.1);
    \draw[very thick] (3.5,-0.1) -- (3.5,0.1);
    \draw[very thick] (4,-0.1) -- (4,0.1);

    \draw[thick, ->] (1.28, 0.1) to[out=120, in=45] (0.32, 0.1);
    \draw[thick, ->] (1.28, 0.1) to[out=120, in=45] (0.9, 0.1);
    \draw[thick, ->] (1.28, 0.1) to[out=45, in=120] (1.9, 0.1);
    \draw[thick, ->] (1.28, 0.1) to[out=45, in=120] (3, 0.1);
    \draw[thick, ->] (1.28, 0.1) to[out=45, in=120] (3.7, 0.1);


    \draw[very thick, color=seabornRed] (0,-1) node[left, color=black] {$y_0$} -- (0.65,-1);
    \draw[very thick, color=seabornGreen] (0.65,-1) -- (1.15,-1);
    \draw[very thick, color=seabornYellow] (1.15,-1) -- (1.35,-1);
    \draw[very thick, color=seabornRed] (1.35,-1) -- (2.3,-1);
    \draw[very thick, color=seabornYellow] (2.3,-1) -- (2.68,-1);
    \draw[very thick, color=seabornRed] (2.68,-1) -- (3.5,-1);
    \draw[very thick, color=seabornGreen] (3.5,-1) -- (4,-1);

    \draw[very thick] (0,-1.1) -- (0,-0.9);
    \draw[very thick] (0.65,-1.1) -- (0.65,-0.9);
    \draw[very thick] (1.15,-1.1) -- (1.15,-0.9);
    \draw[very thick] (1.35,-1.1) -- (1.35,-0.9);
    \draw[very thick] (2.3,-1.1) -- (2.3,-0.9);
    \draw[very thick] (2.68,-1.1) -- (2.68,-0.9);
    \draw[very thick] (3.5,-1.1) -- (3.5,-0.9);
    \draw[very thick] (4,-1.1) -- (4,-0.9);

    \draw[thick, ->] (0.32, -0.9) to[out=45, in=120] (0.9, -0.9);
    \draw[thick, ->] (0.32, -0.9) to[out=45, in=120] (1.28, -0.9);
    \draw[thick, ->] (0.32, -0.9) to[out=45, in=120] (2.5, -0.9);
    \draw[thick, ->] (0.32, -0.9) to[out=45, in=120] (3.7, -0.9);
  \end{tikzpicture}
    \caption{Function \changesname.}
    \label{fig:changes}
  \end{marginfigure}

\begin{example}
  \reffig{changes} shows the \changesname{} impact notion applied to the input space of the neural network in \reffig{irregular}.
  The function $\changes$ returns the maximum number of continuous regions in which the output value changes when the input variables are modified.
  As you can see, for the variable $x$, this can only happen when $y=y_0$ as all the other values of $y$ lead to the same or less changes.
  Consider the outcomes green, yellow, and red (graphically represented by the three figures in \reffig{changes}, top to bottom), modifying the input variable $x$ may lead to 5, 5, and 4 different outcomes, respectively.
  Therefore, the \changesname{} impact notion for the input variable $x$ is 5.
\end{example}


\begin{lemma}[\changesname{} is Monotonic]
  \lablemma{changes-monotonic}
For all set of traces $\defsetoftraces, \defsetoftraces'\in\tracetype$, it holds that:
  \begin{align*}
    \defsetoftraces \subseteq \defsetoftraces' \ImplieS \changes(\defsetoftraces) \le \changes(\defsetoftraces')
  \end{align*}
\end{lemma}
\begin{proof}
  The proof is based on the observation that more traces in $\defsetoftraces'$ can only increase the number of continuous regions, never a lower amount.
  Hence, the number of continuous regions leading to a different output (\cf{} $Q_{\definputvariables, \definput, \defoutput}$) can only increase with more traces.
  We conclude that $\changes(\defsetoftraces) \le \changes(\defsetoftraces')$.
\end{proof}

We show the validation of the $\defbound$-bounded impact property when instantiated with the $\changesname$ impact notion,
we call such property $\boundedchanges$.

\siderefbox{def}{output-abstraction-semantics}
\begin{lemma}[$\boundedchanges$ Validation]\lablemma{changes-validation}
  \begin{align*}
    \collectingsemantics \subseteq \BOUNDEDCHANGES \IfF \outputsemantics \subseteq \outputabstraction(\dependencyabstraction(\BOUNDEDCHANGES))
  \end{align*}
\end{lemma}
\begin{proof}
  The $\changesname$ impact notion does not consider the intermediate states, in fact, it only employs the first state in the definition of $Q_{\definputvariables, \definput, \defoutput}$ and the last one in the definition of $\segments$.
  Thus, the abstraction to dependencies does not affect the validation of the property.
  Furthermore, even handling the output abstraction at the semantic level, by abstracting output states to abstract output states, does not affect the validation of the property as the $\changesname$ impact notion already abstracts the output values before comparing to the given output $\defoutput$, \cf{} $\outputobs(\retrieveoutput{\deftrace}) = \defoutput$ in \nrefdef{segments}.
\end{proof}

\reflemma{changes-monotonic} and \reflemma{changes-validation} are of significant importance as they show that the $\changesname$ impact notion can be used to certify that a program has impact of \emph{at most} $\defbound$, meaning that the program output changes at most $\defbound$ times when the input variables vary.
In case of non-deterministic set of traces, the $\changesname$ impact notion would not be meaningful, as the number of continuous regions leading to a different output value would be always infinite.


\subsection{The \qlibraname{} Impact Notion}[\qlibraname]
\labsec{qlibra-impact-notion}

The second notion introduced in this section is the \qlibraname{} impact notion.
Notably, this notion is designed to quantify the amount of neural-network's input space that does not contain bias.
A network is fair whenever the classification determined by a model does not depend on the ``sensitive'' input variables.
In our setting, the sensitive input variables are the represented by the set of input variables $\definputvariables$ and quantifying the amount of bias is equivalent to measuring the influence of the input variables $\definputvariables$ on the network outcome.
To this end, we define the \qlibraname{} impact notion as the volume of input space that is not able to change the model classification by perturbation of the input variables $\definputvariables$.
The higher the volume, the lower the amount of space that is prone to bias, and thus the higher the fairness of the model.

In practice, we collect the input space\sidenote{The input space is assumed to be normalized in the box $[0, 1]$.} (without the input variables $\definputvariables$ to account for any possible permutations of their input values) where the variables $\definputvariables$ do not influence the network outcome.
% To do so, we employ the $\unusediowrapper$ predicate to check whether a subset of the given set of traces $\defsetoftraces$ is not able to change the network outcome.
% Among all the possible subsets, we consider the maximal one, that is, the one that cannot be extended without changing the network outcome.
To do so, we retrieve the input values of traces that do not change the network outcome when the input variables $\definputvariables$ are modified, similarly to the $\unusediowrapper$ predicate.
We determine the volume of this set of points by applying the standard volume operation inherent to metric spaces.
Formally, the \qlibraname{} impact notion is defined as follows:

\begin{definition}[\qlibraname]\labdef{qlibra}
  Given a set of input variables of interest $\definputvariables\in\setof\inputvariables$, and an output descriptor $\outputobs$,
  the quantity $\qlibra\in\tracetype\to[0, 1]$ is defined as:
  % \begin{align*}
  %   \qlibra(\defsetoftraces) &\DefeQ
  %       \volume(\bigsetjoin \setdef{
  %         \defsetoftraces' \setmeet \defsetoftraces''
  %       }{
  %         \defsetoftraces'\in Q \land \defsetoftraces''\in Q \setminus \{\defsetoftraces'\}
  %       }) \\
  %   \text{where } Q &\DefeQ
  %     \setdef{
  %       \setdef{
  %         \retrieveinput{\deftrace}(\inputvariableswithoutw)
  %       }{
  %         \deftrace \in \defsetoftraces \land \outputobs(\retrieveoutput{\deftrace}) = \defoutput
  %       }
  %     }{
  %       \defoutput\in\stateandbottom
  %     }
  % \end{align*}
  % \begin{align*}
  %   &\qlibra(\defsetoftraces) \DefeQ
  %       \volume(\setdef{\retrieveinput{\deftrace}(\inputvariableswithoutw)}{\deftrace\in\defsetoftraces'}) \\
  %   &\quad\text{where } \defsetoftraces' \subseteq \defsetoftraces \text{ such that } \forall \defsetoftraces'' \supseteq \defsetoftraces'.\spacer
  %     \unusediowrapper(\defsetoftraces') \land \neg\unusediowrapper(\defsetoftraces'')
  % \end{align*}
  \begin{align*}
    &\qlibra(\defsetoftraces) \DefeQ \\
    &\quad \volume\left(\setdef*{\retrieveinput{\deftrace}(\inputvariableswithoutw)}{
      \deftrace\in\defsetoftraces \LanD \forall \defvalue\in\values.\spacer
      \retrieveinput\deftrace(\definputvariables) \neq \defvalue \implies \\
      \exists \deftrace'\in\defsetoftraces.\spacer
      \retrieveinput{\deftrace'}(\definputvariables) = \defvalue \LanD \retrieveinput\deftrace \stateeq{\inputvariableswithoutw} \retrieveinput{\deftrace'} \LanD \\
      \quad \outputobs(\retrieveoutput{\deftrace}) = \outputobs(\retrieveoutput{\deftrace'})
    }\right)
  \end{align*}
\end{definition}

In other words, the \qlibraname{} quantifies the volume of the biggest set of traces $\defsetoftraces' \subseteq \defsetoftraces$ that does not contain bias, \ie, where the predicate $\unusediowrapper$ holds.
Indeed, the \qlibraname{} impact notion could be rewritten as:
\begin{align*}
  &\qlibra(\defsetoftraces) \spacearound= \\
     &\quad \volume(\setdef{\retrieveinput{\deftrace}(\inputvariableswithoutw)}{
        \unusediowrapper(\setdef{\deftrace'\in\defsetoftraces}{\retrieveinput{\deftrace'} \stateeq{\inputvariableswithoutw} \retrieveinput{\deftrace}})
      })
\end{align*}

Such a definition is important because it highlights the relation between the \qlibraname{} impact notion and the $\unusediowrapper$ predicate.
In fact, the \qlibraname{} impact notion quantifies the bigger volume of input space where the $\unusediowrapper$ predicate holds.

\begin{marginfigure}
  \begin{tikzpicture}[scale=0.9]
    % Grid
    % \draw[help lines, color=gray!30, dashed] (-0.1,-0.1) grid (4.1,4.1);
    % x-axis ticks
    % \foreach \x in {-4,-3,-2,-1,0,1,2,3,4}
    %     \draw (\x+5,0.1) -- (\x+5,-0.1) node[below] {\x};
    % % y-axis ticks
    % \foreach \y in {1,2,3}
    %     \draw (0.1,\y) -- (-0.1,\y) node[left] {\y};
    % Polyhedra
    \fill[color=seabornYellow, opacity=0.5] (1,1.75) -- (2,2.75) -- (2.5,1.75) -- (3,2.75) -- (3,4) -- (2,4) -- cycle;
    % \draw[color=seabornYellow, ultra thick] (1,1.75) -- (2,2.75) -- (3,2.75) -- (3,4) -- (2,4) -- cycle;
    % Polyhedra
    \fill[color=seabornGreen, opacity=0.5] (0,2.25) -- (0.5,2.25) -- (1,1.75) -- (2,4) -- (0,4) -- cycle;
    \draw[color=seabornGreen, ultra thick] (0,2.25) -- (0.5,2.25) -- (1,1.75) -- (2,4) -- (0,4) -- cycle;
    \fill[color=seabornGreen, opacity=0.5] (3,2.75) -- (4,1.5) -- (4,4) -- (3,4) -- cycle;
    \draw[color=seabornGreen, ultra thick] (3,2.75) -- (4,1.5) -- (4,4) -- (3,4) -- cycle;
    % Polyhedra
    \fill[color=seabornRed, opacity=0.5] (0,0) -- (4,0) -- (4,1.5) -- (3,2.75) -- (2.5,1.75) -- (2,2.75) -- (1,1.75) -- (0.5,2.25) -- (0,2.25) -- cycle;
    \draw[color=seabornRed, ultra thick] (0,0) -- (4,0) -- (4,1.5) -- (3,2.75) -- (2.5,1.75) -- (2,2.75) -- (1,1.75) -- (0.5,2.25) -- (0,2.25) -- cycle;
    % Nodes
    % \fill[color=seabornRed] (0+1,0+1) circle[radius=2pt];
    % \node[above left] at (0+1,0+1) {$3$};
    % x-axis
    \draw[->,ultra thick] (0,0)--(4.3,0) node[below]{$x$};
    \draw[ultra thick] (0,4)--(4,4);
    % y-axis
    \draw[->,ultra thick] (0,0)--(0,4.3);
    \draw[ultra thick] (4,0)--(4,4);


    \draw[->,ultra thick] (-0.5,0)--(-0.5,4.3) node[left]{$y$};
    \draw[very thick] (-0.4,0) -- (-0.6,0);
    \draw[very thick] (-0.4,1.5) -- (-0.6,1.5);
    % \draw[very thick] (-0.4, 4) -- (-0.6, 4);

    \draw[dashed] (-0.5,1.5) -- (4,1.5);
    % \draw[dashed] (-0.5,4) -- (0,4);
    \draw[dashed] (-0.5,0) -- (0,0);

    \fill[color=black,opacity=0.2] (0,1.5) -- (4,1.5) -- (4,4) -- (0,4) -- cycle;

    \draw[thick,decorate,decoration={brace,amplitude=3pt,raise=4pt}] (-0.5,0) -- (-0.5,1.5) node[midway, left, xshift=-5pt] {$\defbound_x$};
  \end{tikzpicture}
    \caption{Function \qlibraname.}
    \label{fig:qlibra}
  \end{marginfigure}

\begin{example}
Consider the network of example presented in \reffig{irregular}.
The \qlibraname{} impact notion quantifies the volume of the input space where the input variable $x$ does not influence the network outcome.
In \reffig{qlibra}, it is clear that the input variable $x$ does not influence the network outcome when $y$ is below a certain threshold. This volume is denoted by $\defbound_x$ in the figure.
\end{example}

As noticed for the previous impact notion $\changesname$, neural-network models produce deterministic set of traces.
Hence, as the goal of this impact notions is to measure the impact of input variables in the context of neural networks, we can require the set of traces $\defsetoftraces$ to be deterministic without loss of generality.

\begin{lemma}[\qlibraname{} is Decreasing Monotonic]
  \lablemma{qlibra-monotonic}
For all deterministic set of traces $\defsetoftraces, \defsetoftraces'\in\tracetype$, it holds that:
  \begin{align*}
    \defsetoftraces \subseteq \defsetoftraces' \ImplieS \qlibra(\defsetoftraces) \ge \qlibra(\defsetoftraces')
  \end{align*}
\end{lemma}
\begin{proof}
  The proof is based on the observation that more traces in $\defsetoftraces'$ can only increase the volume of bias space as, in the worst case scenario, the added traces are the ones able to change the network outcome.
  In fact, \refprop{ani-predicate-equivalence} proves that the $\unusediowrapper$ predicate is equivalent to the $\aniwrapper$ predicate whenever the given set of traces is deterministic, which is decreasing monotonic in the amount of traces.
  Hence, the volume of the set of points leading to different output values can only increase with more traces, hence the volume of fair space decreases.
  We conclude that $\qlibra(\defsetoftraces) \ge \qlibra(\defsetoftraces')$.
\end{proof}

We show the validation of the $\defbound$-bounded impact property when instantiated with the $\qlibraname$ impact notion,
we call such property $\boundedqlibra$.

\siderefbox{def}{output-abstraction-semantics}
\begin{lemma}[$\boundedqlibra$ Validation]\lablemma{qlibra-validation}
  \begin{align*}
    \collectingsemantics \subseteq \BOUNDEDQLIBRA \IfF \outputsemantics \subseteq \outputabstraction(\dependencyabstraction(\BOUNDEDQLIBRA))
  \end{align*}
\end{lemma}
\begin{proof}
  The $\qlibraname$ impact notion employs the $\unusediowrapper$ predicate to determine the volume of the input space that does not contain bias.
  Thus, this proof directly follows from \refthm{output-validation}.
\end{proof}

\reflemma{qlibra-monotonic} and \reflemma{qlibra-validation} are of significant importance as they show that the $\qlibraname$ impact notion can be used to certify that a program has impact of \emph{at least} $\defbound$, meaning that the input variables $\definputvariables$ of a network are fair for at least $\defbound$ fraction of the input space.

For clarity, we unfold the $\unusediowrapper$ predicate in the definition of the \qlibraname{} impact notion (assuming the set of traces $\defsetoftraces$ is deterministic).
From \refprop{ani-predicate-equivalence}, we notice that the $\unusediowrapper$ predicate is equivalent to the $\aniwrapper$ predicate whenever the set of traces is deterministic, thus obtaining the following definition:

\begin{align*}
  &\qlibra(\defsetoftraces) \\
    &\quad\spacearound=
    \volume\left(\setdef*{\retrieveinput{\deftrace}(\inputvariableswithoutw)}{
      \deftrace\in\defsetoftraces \LanD \forall \deftrace' \in \defsetoftraces.\spacer \\
      \retrieveinput{\deftrace} \stateeq{\inputvariableswithoutw} \retrieveinput{\deftrace'} \implies \outputobs(\retrieveoutput{\deftrace}) = \outputobs(\retrieveoutput{\deftrace'})
    }\right)
\end{align*}

Furthermore, we could also devise a notion that reverse the amount of bias, that is, the volume of the input space that contains bias.

\begin{align*}
  &\qlibra(\defsetoftraces) \\
    &\quad\spacearound=
    1 - \volume\left(
      \setdef*{\retrieveinput{\deftrace}(\inputvariableswithoutw)}{
        \deftrace\in\defsetoftraces \LanD \exists \deftrace' \in \defsetoftraces.\spacer \\
        \retrieveinput{\deftrace} \stateeq{\inputvariableswithoutw} \retrieveinput{\deftrace'} \LanD \outputobs(\retrieveoutput{\deftrace}) \neq \outputobs(\retrieveoutput{\deftrace'})
      }
      \right)
\end{align*}

In the latter, we quantify the volume of the input space that contains bias, and then we subtract this volume from 1 to obtain the volume of the input space that does not contain bias. All these definitions are equivalent.



\section{Quantitative Analysis of Neural Networks}
\labsec{quantitative-analysis-neural-networks}


This section presents the abstract implementations of the \changesname{} and \qlibraname{} impact notions, respectively called $\abstractchangesname$ and $\abstractqlibraname$.
We show how to validate the $\defbound$-bounded impact property for both notions.


\subsection{Abstract Implementation \texorpdfstring{$\abstractchanges$}{Abstract Changes}}[Abstract \texorpdfstring{$\abstractchanges$}{Changes}]

In this section, we introduce $\abstractchanges$ as a sound implementation of $\changes$.
First, we describe the implementation, followed by the validation of the $\defbound$-bounded impact property $\boundedchanges$.

% As previously noted, the $\changesname$ impact notion suffers from non-deterministic set of traces, which would render it meaningless.
% The backward analysis, starting from the output buckets, computes an over-approximation of the set of traces leading to the same output bucket.
% In this case, whenever the over-approximation overlaps the result of the abstract backward analysis coming from different output buckets, in the concrete, this would be like having non-deterministic set of traces.
% To address this issue, we need an abstract domain that is exact for our property and underlying network model.

As previously noted, the $\changesname$ impact notion suffers from the case in which the input is a non-deterministic set of traces, rendering it ineffective.
The backward analysis, starting from the output buckets, computes an over-approximation of the set of traces leading to the same output bucket.
In cases where this over-approximation overlaps with the results of the abstract backward analysis from different output buckets, it would correspond to a non-deterministic set of traces when concretized.
To address this issue, we require an abstract domain that is exact for our property and underlying network model.

The core idea of the $\changesname$ impact notion is to group together abstract elements representing distinct continuous regions. Convex abstract domains, such as polyhedra, octagons, or intervals (introduced in \refsec{abstract-domains}), are inadequate for representing multiple distinct continuous regions because they also include the intermediate points. Therefore, the abstract domain $\abstractdomain$ used in the backward analysis $\backwardsemanticsnoparam$ needs to employ disjunctive sets, which allow for the representation of distinct continuous regions.
%
We leverage the \textit{disjunctive polyhedra abstract domain} $\disjunctivepolyabstractdomain$, defined as
$
  \langle \disjunctivepolyabstractdomaintype, \disjunctivepolyabstractdomainsubseteq \rangle
$, where $\polyabstractdomain$ represents the \textit{convex polyhedra abstract domain}~\sidecite{Cousot1978} and $\disjunctivepolyabstractdomaintype$ is the set of all finite subset of $\polyabstractdomain$.
From the polyhedra domain $\polyabstractdomain$, the function $\abstractdomainproject$ computes the Fourier-Motzkin elimination algorithm~\sidecite{Dantzig1973}.
Specifically, $\abstractdomainproject$ takes as input a set of variables $\definputvariables$ and a polyhedron in $d$-dimensions where $d \ge \cardinalitynospaces{\definputvariables}$, returning a polyhedron in $(d-\cardinalitynospaces{\definputvariables})$-dimensions, removing the variable $\definputvariables$.
Importantly, this domain is exact for the property of interest, as it allows us to represent multiple distinct continuous regions, when applied to feed-forward \relu-activated neural networks.

The function $\abstractchanges$ takes as input the variables of interest $\definputvariables$, $\numberofbuckets$ output buckets $\buckets\in\vectorbuckets$, and $\numberofbuckets$ disjunctions of polyhedra $\prefrombucket\in\disjunctivepolyabstractdomaintype^\numberofbuckets$ obtained from the backward analysis.
For clarity, we access each polyhedron and disjunctions of polyhedra via indexing as in a matrix-based structure, that is, $\prefrombucket=\{\{P_{1,1}\vee \dots\vee P_{1,p}\},\dots,\{P_{n,1}\vee \dots\vee P_{n,q}\}\}$ where $p = \cardinalitynospaces{\prefrombucket[1]}$ and $q = \cardinalitynospaces{\prefrombucket[n]}$. For instance, $\prefrombucket[j]$ refers to the disjunction of polyhedra $\{P_{j,1}\vee \dots\vee P_{j,k}\vee \dots\}$, for $j\le n$, and $\prefrombucket[{j, k}]$ refers to the polyhedron $P_{j,k}\in\polyabstractdomain$.
The projection of disjunctions of polyhedra applies, in turn, the polyhedron projection to each polyhedron in the disjunction, then collects the projected polyhedra in a disjunction of polyhedra, \ie, $\abstractdomainproject(\prefrombucket \in \disjunctivepolyabstractdomaintype) \defeq \bigvee_{k \le \cardinalitynospaces{\prefrombucket}} \abstractdomainproject(\prefrombucket[j, k])$.

Computationally speaking, the function $\abstractchanges$ projects away the input variables $\definputvariables$ from each polyhedron $\prefrombucket[{j, k}]$.
The projected polyhedra represent regions where $\definputvariables$ ranges on all possible values, considering all potential variations of this variable.
The function $\intersectallfunction$ gathers the set of indexes $J$, also called the connected components, where the projected polyhedra intersect.
The underlying idea is that each connected component corresponds to the set of continuous regions reachable through variations of $\definputvariables$.
% Note that, each connected component is implemented as a \textit{multiset} \denis{no explaination of why using a multiset}, \ie, a set that allows multiple instances for each of its elements.
% This allows us to later exclude regions leading to the same bucket.
%
Finally, $\abstractchanges$ determines the maximum count of changes across all connected components $J$ and buckets $\buckets$.
It counts the number of indices $l$ in each connected component $J$ where $k$ is not equal to $j$, thus excluding the polyhedra leading to the same output bucket $j$.

\begin{definition}[$\abstractchanges$]\labdef{abstractchanges}
  We define $\abstractchanges\in\pair\vectorbuckets\vectorbuckets\to\valuesposplus$ as:
  \begin{align*}
    &\abstractchanges(\presfrombuckets, \buckets) \DefeQ \\
      &\quad \max
      \setdef{
        \max_{j \le \numberofbuckets} \cardinalitynospaces{\setdef{l \in J}{l \neq j}}
      % \\ &\qquad\quad
      }{
        J \in \intersectallfunction((\abstractdomainproject(\prefrombucket))_{j\le\numberofbuckets})
      }
  \end{align*}
\end{definition}

Before proceeding with the soundness proof of $\abstractchanges$, we recall the requirements on the concrete impact notion $\changes$, namely, the requirement on determinism.
Such a requirement is necessary to ensure a meaningful quantity of changes in the outcome, otherwise the number of changes would be infinite in the presence of continuous regions with non-deterministic output values.
Equivalently, in the abstract, any over-approximation from the backward analysis that overlaps different preconditions $\prefrombucket$ would lead to an infinite number of changes.
To this end, we require the backward analysis to be exact: sound and \emph{complete}.
The soundness ensures that the backward analysis does not miss any possible behavior, while the completeness ensures that the backward analysis does not introduce any spurious behavior.
In addition to \refdef*{sound-over-approximation}, we require the following completeness condition on the backward analysis:

\begin{definition}[Complete Under-Approximation]\labdef{complete-under-approximation}
  For all programs $\defprogram$, and output bucket $\bucket\in\abstractdomain$, the family of semantics $\backwardsemanticsnoparam$ is a \textup{complete under-approximation} of the output-abstraction semantics $\outputsemantics$
  when it holds that:
  \[\reducedoutputsemantics \SupseteQ \backwardconcretization(\backwardsemantics)\bucket\]
\end{definition}

Whenever the backward semantics $\backwardsemantics$ is both sound and complete, it yields an exact semantics.

\begin{lemma}[Exact Backward Semantics]\lablemma{exact-backward-semantics}
  Whenever the backward semantics $\backwardsemantics$ is sound and complete, it holds that:
  \[\reducedoutputsemantics \spacearound= \backwardconcretization(\backwardsemantics)\bucket\]
\end{lemma}
\begin{proof}
  Trivially follows from the definitions of soundness (\refdef{sound-over-approximation}) and completeness (\refdef{complete-under-approximation}).
\end{proof}

Note that, whenever the given program is deterministic, the traces concretized from the backward analysis are deterministic as well.
With the exactness of the backward analysis, we show that $\abstractchanges$ is a sound implementation of $\changes$.

\begin{lemma}[$\abstractchanges$ is a Sound Implementation of $\changes$]\lablemma{abstractchanges-is-sound}
  Let $\definputvariables\in\variables$ be the input variable of interest, $\abstractdomain$ the abstract domain, $\backwardsemanticsnoparam$ the family of semantics, and $\buckets\in\vectorbuckets$ the starting output buckets.
  Whenever the following conditions hold:
  \begin{enumerate}[label=(\roman*)]
    \item \label{tio1} $\buckets$ covers the subset of potential outcomes, \cf{} \refdef*{covering}[*-2],
    \item \label{tio2} $\buckets$ is compatible with $\outputobs$, \cf{} \refdef*{compatibility}[*4],
    \item \label{tio3} $\backwardsemanticsnoparam$ is an exact backward semantics, \cf{} \reflemma{exact-backward-semantics}, and
    \item \label{tio4} $\abstractdomainproject$ is sound, \cf{} \refdef*{soundness-project}[*8];
  \end{enumerate}
  then, $\abstractchanges$ is a sound implementation of $\changes$.
\end{lemma}
\begin{proof}
  \denis{todo}
\end{proof}

While this result shows that an exact backward analysis is required to ensure a sound implementation of $\changes$, it also presents scalability challenges as the backward analysis is computationally expensive and requires to explore all the possible paths of the network.
In the next section, we introduce the \qlibraname{} impact notion, which--in a sense--still requires an exact backward analysis, but is more scalable and efficient thanks to the use or parallelization.

\subsection{Abstract Implementation \texorpdfstring{$\abstractqlibra$}{Abstract QLibra}}[Abstract \texorpdfstring{$\abstractqlibra$}{QLibra}]
\labsec{abstract-qlibra}


% We assume $\abstractdomain$ is equipped with an additional abstract operator $\abstractdomainvolume\in\abstractdomain\to[0, 1]$, which returns the volume of the given abstract element normalized in the interval $[0, 1]$.
% The abstract implementation $\abstractqlibra$ is defined as the volume of the intersecting abstract regions leading to different output buckets.
% Specifically, the volume is computed by first projecting away the input variables $\definputvariables$ from all the given abstract values resulting from the backward analysis, \cf{} $\presfrombuckets$.
% Then, it collects together all the possible pairwise intersections among the projected abstract values to find the portion of input space leading to different output buckets only via variations of the input variables $\definputvariables$.
% Assuming that all the input variables are bounded in $[0, 1]$, which is a common practice in neural networks, the volume of the abstract element is normalized in the interval $[0, 1]$.

% \begin{example}
%   In the context of the interval domain, where each input variable is related to a possibly unbounded lower and upper bound, $\abstractdomainvolume(\langle \defvariable \mapsto [2, 4]\rangle) = 2$.
%   On the other hand, whenever the input abstract element is unbounded, the size is $+\infty$, \eg, $\abstractdomainvolume(\langle \defvariable \mapsto [0, +\infty]\rangle) = +\infty$.
%   The function $\abstractdomainvolume$ expects only a single variable to be constrained in the abstract domain, or in other words, only one variable is allowed to be not $\top$.
% \end{example}

% The abstract range $\abstractqlibra$ first projects away the input variables $\definputvariables$ from all the given abstract values.
% Then, it collects all the possible intersections among the projected abstract values.
% These intersections represent concrete input configurations where variations on the values of $\definputvariables$ \emph{may} lead to changes of program outcome, from a bucket to another.
% All the possible combination of intersections are joined together to find the maximum range of the extreme values of the buckets.
% Formally, the abstract implementation $\abstractqlibra$ is defined as follows:

% \begin{definition}[$\abstractqlibra$]\labdef{abstractqlibra}
%   We define $\abstractqlibra\in\pair\vectorbuckets\vectorbuckets\to[0, 1]$ as:
%   \begin{align*}
%     &\abstractqlibra(\presfrombuckets, \buckets) \DefeQ \\
%       &\quad \abstractdomainvolume\left(
%         \bigjoin_{
%           J \in \intersectallfunction((\abstractdomainproject(\prefrombucket))_{j\le\numberofbuckets})
%         }
%           \setdef{
%             \prefrombucket[j] \abstractdomainmeet \prefrombucket[k]
%           }{
%             j, k \in J \land j \neq k
%           }
%       \right)
%   \end{align*}
% \end{definition}

% \begin{example}
%   \denis{Example of the abstractqlibra function on the previous network space.}
% \end{example}

% To prove that $\abstractqlibra$ is a sound implementation of $\qlibra$, we require the following soundness condition on the abstract operator $\abstractdomainvolume$ to ensure that the abstract volume is always greater than the concrete one.

% \begin{definition}[Soundness of \texorpdfstring{$\abstractdomainvolume$}{Volume}]\labdef{soundness-volume}
%   Given an abstract value $\defstate^\natural\in\abstractdomain$ and the set of input variables of interest $\definputvariables\in\inputvariables$, it holds that:
%   \[\abstractdomainvolume(\defstate^\natural) \GE \cardinality{\setdef{\retrieveinput{\defstate}(\definputvariables)}{\defstate\in\abstractdomainconcretization(\defstate^\natural)}}\]
% \end{definition}

% The next result shows that the abstract impact $\abstractqlibra$ is a sound over-approximation of the concrete impact $\qlibra$, \cf{} \refdef{qlibra}.

% \begin{lemma}[$\abstractqlibra$ is a Sound Implementation of $\qlibra$]\lablemma{abstractqlibra-is-sound}
%   Let $\definputvariables\in\variables$ be the input variable of interest, $\abstractdomain$ the abstract domain, $\backwardsemanticsnoparam$ the family of semantics, and $\buckets\in\vectorbuckets$ the starting output buckets.
%   Whenever the following conditions hold:
%   \begin{enumerate}[label=(\roman*)]
%     \item \label{rts1} $\buckets$ covers the subset of potential outcomes, \cf{} \refdef*{covering}[*-2],
%     \item \label{rts2} $\buckets$ is compatible with $\outputobs$, \cf{} \refdef*{compatibility}[*4],
%     \item \label{rts3} $\abstractdomainproject$ is sound, \cf{} \refdef*{soundness-project}[*8], and
%     \item \label{rts4} $\abstractdomainvolume$ is sound, \cf{} \refdef{soundness-volume}[*12];
%   \end{enumerate}
%   then, $\abstractqlibra$ is a sound implementation of $\qlibra$.
% \end{lemma}
% \begin{proof}
%   \denis{todo}
% \end{proof}


% \begin{example}
%   The quantities computed by the abstract implementation $\abstractqlibra$ in \refexample{abstract-qlibra} are sound over-approximations of the concrete implementation $\qlibra$:
%   \begin{align*}
%     & \qlibraname_{\{\texttt{angle}\}}(\dependencysemanticsnoparam\semanticsof{\landingprogram}) = 3 \\
%     &\qquad\GE \abstractqlibraname_{\{\texttt{angle}\}}(\presfrombuckets, \buckets) = 1
%   \end{align*}
%   \begin{align*}
%     & \qlibraname_{\{\texttt{speed}\}}(\dependencysemanticsnoparam\semanticsof{\landingprogram}) = 2 \\
%     &\qquad\GE \abstractqlibraname_{\{\texttt{speed}\}}(\presfrombuckets, \buckets) = 1
%   \end{align*}
%   as expected by \refthm*{soundness}, where $\landingprogram$ is the program of the landing alarm system, \cf{} \refprog{landing-alarm-system}.
% \end{example}


In this section, we present $\abstractqlibra$ as a sound implementation of $\qlibra$, computing an over-approximation of the volume of input space that may contain bias. Conversely, $\abstractqlibra$ computes an under-approximation of the fair input space.
As introduced at the beginning of this chapter, this analysis is too na\"ive to be practical, it is still useful for building upon later in the next section.

\marginnote{\denis{No. Give the definition of $\abstractqlibra$ as done in the previous sections.}}
Instead of introducing an abstract implementation as a definition, we describe $\abstractqlibra$ in the form of a pseudo-code algorithm: \reffig{abstract-qlibra}.
The algorithm takes as input the neural-network model $\defmodel$, a set of input variables of interest $\definputvariables$ (the sensitive features), the $\numberofbuckets$ output buckets $\buckets$, and an abstract domain $\abstractdomain$.
The output buckets $\buckets$ represent all the possible classification targets of the neural network.
In this way, both covering (\refdef{covering}) and compatibility (\refdef{compatibility}) conditions are satisfied as all the target classes are covered and for any two different target classes, there exist two different output buckets representing them.
The analysis proceeds backwardly from each output bucket $\bucket$ via the backward semantics $\backwardsemanticsnoparam$ in order to determine an over-approximation of the initial states $\prefrombucket$.

Finally, the algorithm projects the input variables $\definputvariables$ from the abstract states $\prefrombucket$ to account for any possible permutations of their input values.
Then, it computes the pairwise intersections among the projected abstract states to find the portion of input space leading to different output buckets only via variations of the input variables $\definputvariables$.
The complement of such a volume is an over-approximation of the biased input space, hence an under-approximation of the fair input space.
The quantity measured by $\abstractqlibra$, algorithm in \reffig{abstract-qlibra}, is always lower than the concrete $\qlibra$.

\begin{lemma}[$\abstractqlibra$ is a Sound Implementation of $\qlibra$]\lablemma{abstractqlibra-is-sound}
  Let $\definputvariables\in\variables$ be the input variable of interest, $\abstractdomain$ the abstract domain, $\backwardsemanticsnoparam$ the family of semantics, and $\buckets\in\vectorbuckets$ the starting output buckets.
  Whenever the following conditions hold:
  \begin{enumerate}[label=(\roman*)]
    \item \label{nioa1} $\buckets$ covers the subset of potential outcomes, \cf{} \refdef*{covering}[*-2],
    \item \label{nioa2} $\buckets$ is compatible with $\outputobs$, \cf{} \refdef*{compatibility}[*4],
    \item \label{nioa3} $\backwardsemanticsnoparam$ is a sound backward semantics, \cf{} \reflemma{sound-over-approximation}, and
    \item \label{nioa4} $\abstractdomainproject$ is sound, \cf{} \refdef*{soundness-project}[*8];
  \end{enumerate}
  then, $\abstractqlibra$ is a sound implementation of $\qlibra$.
\end{lemma}
\begin{proof}
  \denis{todo}
\end{proof}



Although sound, the abstract implementation $\abstractqlibra$ does not work well in presence of imprecision from the abstract domain employed during the backward analysis.
In fact, the abstract implementation $\abstractqlibra$ applied directly to the backward analysis represents the so called \emph{na\"ive casual-fairness analysis}~\sidecite{Urban2020}.
Such analysis suffers from the choice of existing abstract domains, which are rather fast but too imprecise to handle non-linear constraints, such as those arising from the activation functions in neural networks.
Indeed, even using the polyhedra domain for the backward analysis, handling the \relu{} activation function would over-approximate what effectively is a conditional branch, leading to a loss of precision that is reflected for each node of the neural network.
On the other hand, one could use a disjunctive completion of the polyhedra domain~\sidecite{Cousot1979}, which would retain a separate polyhedron each condition.
However, this analysis would be extremely slow.

\section{Parallel Analysis for Efficient Validation of the \texorpdfstring{$\boundedqlibra$ Property}{k-Bounded Impact Property applied to qlibra}}[Parallel Analysis]

To overcome the limitation of $\abstractqlibra$ described above, we first reason at a concrete-semantics level, introducing an additional semantics: the \emph{parallel semantics}.
Intuitively, we show how partitioning the input space into \emph{fair} partitions still allows for property validation.
Thus, we could measure the amount of bias in the input space in parallel for each partition, and then aggregate the quantity.
Finally, we show how to validate the $\defbound$-bounded impact property for the parallel semantics.
This section is based on the work presented in \sidetextcite{Urban2020} and \sidetextcite{Mazzucato2021}.


\subsection{Parallel Semantics}\labsec{parallel-semantics}

We observe that the semantics of a program satisfying the $\boundedqlibra$ property induces a partition of the input space restricted to the input variables in $\inputvariableswithoutw$.
We call this input partition \emph{fair}.

\begin{definition}[Fair Input Partition]\labdef{fair-input-partition}
  An input partition $\definputpartitions$ of $\reducedstate$ is \emph{fair} if all value choices $\values$ for the variables of interest $\definputvariables$ are possible in all the partitions:
  \begin{math}
    \forall \definputpartition\in\definputpartitions,\defvalue\in\values.\spacer
    \exists \retrieveinput\defstate\in\definputpartition.\spacer
    \retrieveinput\defstate(\definputvariables) = \defvalue
  \end{math}
\end{definition}

Given a fair input partition $\definputpartitions$ of $\reducedstate$, we can verify whether a program $\defprogram$ has an impact of $\defbound$ for each element $\definputpartition\in\definputpartitions$, \emph{independently}, and aggregate the results.

\begin{lemma}
  \begin{math}
    \defprogram \satisfies \boundedqlibra \IfF
    \sum_{\definputpartition\in\definputpartitions} \qlibra(\tracesemantics) \ge \defbound
  \end{math}
\end{lemma}
\begin{proof}
  \denis{todo}
\end{proof}

We exploit the above insight to further abstract the output-abstraction semantics $\outputabstraction$ to the \emph{parallel semantics} $\parallelsemantics$.
Formally, the right adjoint\sidenote{The left adjoint is uniquely defined by the right one.} $\parallelabstraction$ for the parallel semantics is defined as:
%
\begin{definition}[Right Adjoint for the Parallel Semantics]\labdef{right-adjoint-for-the-parallel-abstraction-semantics}
\begin{align*}
  \parallelabstraction \IN& \outputtype \to \paralleltype \\
  \parallelabstraction(\defsetofsetofdependencies) \DefeQ& \setdef{
    \partition{\defsetofdependencies}{\definputpartition}
  }{
    \defsetofdependencies \in \defsetofsetofdependencies \LanD \definputpartition\in\definputpartitions
  }
\end{align*}
\end{definition}

The order of the parallel semantics $\parallelsubseteq$ is the pointwise ordering between sets of pairs of states restricted to the same input partition of $\definputpartitions$. Formally,
\begin{align*}
  \defsetofsetofdependencies \parallelsubseteq \defsetofsetofdependencies' \IfF
  \forall \defsetofdependencies\in\defsetofsetofdependencies, \defsetofdependencies'\in\defsetofsetofdependencies'.\spacer
  \defsetofdependencies \neq \emptyset \land \defsetofdependencies' \neq \emptyset \ImplieS
  \bigwedge_{\definputpartition\in\definputpartitions}\partition{\defsetofdependencies}{\definputpartition} \subseteq \partition{\defsetofdependencies'}{\definputpartition}
\end{align*}

Ensuring that all the non-empty set of dependencies in $\defsetofsetofdependencies$ are included in $\defsetofsetofdependencies'$ for each input partition $\definputpartition$.
We have the following result:


\begin{theorem}\labthm{output-parallel-galois-connection}
  The two adjoints $\tuple{\parallelabstraction}{\parallelconcretization}$ form a \emph{Galois connection}:
\begin{align*}
  \galoisbetweensemantics{output}{parallel}
\end{align*}
\end{theorem}
\begin{proof}
  \denis{todo}
\end{proof}


We can now derive the parallel semantics as an abstraction of the output-abstraction semantics.

\begin{definition}[Parallel Abstraction Semantics]\labdef{parallel-abstraction-semantics}
  The \emph{parallel-abstraction semantics} $\parallelsemanticsnoparam\in\paralleltype$ is defined as:
  \begin{align*}
    \parallelsemanticsnoparam\DefeQ&\parallelabstraction(\outputsemanticsnoparam) \\
    % \spacearound{=}&\parallelabstraction(\{\spacearound{\setdef{\inputoutputtuple{\deftrace}}{\deftrace \in \tracesemanticsnoparam}}\}) \\
    \spacearound{=}&
    \setdef{\spacearound{
      \setdef{
        \tuple{\retrieveinput{\deftrace}}{\outputobs(\retrieveoutput{\deftrace})}
      }{
        \deftrace \in \partition\tracesemanticsnoparam\definputpartition
      }
    }}{\definputpartition\in\definputpartitions}
  \end{align*}
\end{definition}

It remains to show soundness and completeness for the parallel semantics when applied to the $\boundedqlibra$ property.
\begin{theorem}
  \begin{math}
    \defprogram \satisfies \boundedqlibra \IfF
    \sum_{j} \defbound_j \ge \defbound
  \end{math}
  such that:
  \begin{align*}
    \parallelsemantics \parallelsubseteq \parallelabstraction(\outputabstraction(\dependencyabstraction(\resize{\mathscr{B}}{\qlibra}[\ge\defbound_j])))
  \end{align*}
\end{theorem}
\begin{proof}
  \denis{todo}
\end{proof}



\subsection{Parallel Implementation \texorpdfstring{$\parallelqlibra$}{QLibra}}[Parallel \texorpdfstring{$\parallelqlibra$}{QLibra}]

In this section, we build upon the parallel semantics to design an abstract implementation $\abstractqlibra$, called $\parallelqlibra$, that computes an over-approximation of the volume of input space that may be fair.
This static analysis automatically find a fair partition of the input space, then computes the volume of the fair input space.
The abstract implementation $\parallelqlibra$ is defined as the sum of the volumes of the fair partitions.

\marginnote{\denis{Add the algorithm as definition of $\parallelqlibra$ as margin figure.}}
The algorithm is presented in \reffig{abstract-qlibra-parallel}, it combines a forward with a backward analysis.
The forward analysis uses an abstract domain $\abstractdomain$ and builds partition $\definputpartitions$ of the input space, while the backward analysis employs the disjunction of polyhedra abstract domain $\disjunctivepolyabstractdomain$.
Then, we perform the quantification of the biased space.

More specifically, the forward analysis bounds the number of paths that the backward analysis has to explore.
We represent each path by an activation pattern\sidenote{The activation pattern determines the activation status of every \relu{} operation in the network model.}.
The analysis receives a \emph{budget} providing an upper bound $\upperbound$ on the number of tolerated \relu{} nodes with an unknown activation status for each element $\definputpartition\in\definputpartitions$, \ie, the number of paths that are to be explored by the backward analysis in each partition $\definputpartition$.
The forward analysis starts with the trivial partition of the whole input space, \ie, $\definputpartitions = \{\reducedstate\}$.
Then, it proceeds forward for each element $\definputpartition\in\definputpartitions$ by computing the abstract activation patterns that are compatible with the input partition $\definputpartition$, starting from the empty set of activation patterns. If $\definputpartition$ leads to a unique outcome, then the partition is already fair without further analysis.
Therefore, we add the partition $\definputpartition$ to the set of \emph{completed} partitions $\completedpartitions$.
On the other hand, if abstract activation pattern $\defabstractactivationpattern$ fixes the activation status of enough \relu{} nodes, then we declare the partition $\definputpartition$ \emph{feasible} and ready for the backward analysis.
In this case, the pair of $\defabstractactivationpattern$ and $\definputpartition$ is added into a map $\feasiblepartitions$ from abstract activation patterns to input partitions.
The insertion takes care of merging the activation patterns that are subsumed by other (weaker) activation patterns.
Otherwise, the partition $\definputpartition$ needs to be further refined, with respect to the variables in $\inputvariableswithoutw$.
Partitioning may continue until the volume of the partition is below a certain threshold $\lowerbound$.
Finally, whenever the partition is smaller than $\lowerbound$, we exclude it from the set of partitions to be analyzed until more resources become available.
Note that, the forward analysis does not need expressive and slow abstract domain since it does not need to precisely handle polyhedral constraints.

The budget configuration of the pre-analysis (i.e., choices of an abstract domain, lower bound $\lowerbound$, and upper bound $\upperbound$) allows trading-off between precision and scalability of the approach. Ultimately however, the optimal configuration largely depends on the analyzed neural network. For this reason, a \emph{configuration auto-tuning mechanism} dynamically updates the lower bound and upper bound configuration according to a chosen \emph{search heuristic}. By default, whenever an input partition exceeds the current configuration, the pre-analysis alternates between increasing the upper bound by one, up to a maximum upper bound $\maxupperbound$, and halving the lower bound, down to a minimum lower bound $\minlowerbound$. Other bound update patterns are configurable (e.g., by updating both bounds at the same time, or performing multiple increments to the upper bound before halving the lower bound, etc.).

Then, the analysis proceeds backwardly, independently for each abstract activation pattern $\defabstractactivationpattern$ and input partition $\definputpartition$ in $\feasiblepartitions$.
By exploiting the knowledge of the activation pattern, the backward analysis does not need to explore all the possible paths in the network model, but only the paths that have an unknown activation status.
This is also why it is advantageous to merge subsumed abstract activation paths in the forward analysis.

Finally, the abstract implementation $\parallelqlibra$ computes the sum of the volumes of the fair portion of each partition together.
The next result shows that the abstract implementation $\parallelqlibra$ is a sound over-approximation of the concrete implementation $\qlibra$.
Therefore, it can be used to validate the $\boundedqlibra$ property.

\begin{theorem}
  Given the model $\defmodel$ and the input variable $\definputvariables\in\setof\inputvariables$, an abstract domain $\abstractdomain$, and the budget $\lowerbound, \upperbound$, the abstract implementation $\parallelqlibra$ is a sound over-approximation of the concrete implementation $\qlibra$:
  \begin{align*}
    \parallelqlibra(\defmodel, \buckets) = \defbound' \land \defbound' \ge \defbound \ImplieS \defprogram \satisfies \BOUNDEDQLIBRA
  \end{align*}
\end{theorem}
\begin{proof}
  \denis{todo}
\end{proof}

\section{Summary}

In this chapter, we presented the \changesname{} and \qlibraname{} impact notions, and their abstract implementations, $\abstractchangesname$ and $\abstractqlibraname$, respectively.
We validated the $\defbound$-bounded impact property for both notions.
To address the scalability issues of the backward analysis, we introduced the parallel semantics and the abstract implementation $\parallelqlibraname$.
In the settings of timing analyses, the next chapter presents a quantitative analysis of the impact of input variables on the number of iterations of a program.
