\setchapterpreamble[u]{\margintoc}


\chapter{Quantitative Verification for Neural Networks}
\labch{quantitative-fairness}

\marginemptybox{9.6cm}

In this chapter, we introduce feed-forward deep neural networks classifiers.
We define two new quantitative impact quantifiers: the \changesname{} impact quantifier, which targets the repetitions of changes in the outcome, and the \qlibraname{} impact quantifier, which measures the amount of unused input space. We then present the abstract implementation of the two impact quantifiers, respectively called $\abstractchangesname$ and $\abstractqlibraname$, and we show how to validate the $\defbound$-bounded impact property for both quantifiers. Finally, we present the parallel implementation $\parallelqlibraname$ for efficient neural network verification of the $\qlibraname$ impact quantifier.
This chapter is based on the work presented at the 28th Static Analysis Symposium (SAS) 2021~\sidecite{Mazzucato2021}.
The next chapter will present the experimental evaluation for the quantitative verification of neural networks.


\frenchdiv

\emph{Dans ce chapitre, nous introduisons les classificateurs de réseaux neuronaux profonds feed-forward. Nous définissons deux nouveaux quantificateurs d'impact quantitatif : le quantificateur d'impact \changesname{}, qui cible les répétitions de changements dans le résultat, et le quantificateur d'impact \qlibraname{}, qui mesure la quantité d'espace d'entrée non utilisé. Nous présentons ensuite l'implémentation abstraite de ces deux quantificateurs d'impact, respectivement appelés $\abstractchangesname$ et $\abstractqlibraname$, et nous montrons comment valider la propriété d'impact borné par $\defbound$ pour ces deux quantificateurs. Enfin, nous présentons l'implémentation parallèle $\parallelqlibraname$ pour une vérification efficace des réseaux neuronaux en utilisant le quantificateur d'impact \qlibraname{}. Ce chapitre est basé sur les travaux présentés au 28e Symposium sur l'analyse statique (SAS) 2021~\cite{Mazzucato2021}. Le prochain chapitre présentera l'évaluation expérimentale pour la vérification quantitative des réseaux neuronaux.}







\section{Summary}

In this chapter, we presented the \changesname{} and \qlibraname{} impact quantifiers, and their abstract implementations, $\abstractchangesname$ and $\abstractqlibraname$, respectively.
We validated the $\defbound$-bounded impact property for both quantifiers.
To address the scalability issues of the backward analysis, we introduced the parallel semantics and the abstract implementation $\parallelqlibraname$.
The next chapter will present the experimental results of the \changesname{} and \qlibraname{} impact quantifiers applied to neural networks.
Afterwards, the thesis will focus on intensional properties, specifically how to quantify the influence of input variables on the number of iterations of a program.


\frenchdiv

\emph{Dans ce chapitre, nous avons présenté les quantificateurs d'impact \changesname{} et \qlibraname{}, ainsi que leurs implémentations abstraites, $\abstractchangesname$ et $\abstractqlibraname$, respectivement. Nous avons validé la propriété d'impact borné par $\defbound$ pour les deux quantificateurs. Pour résoudre les problèmes de scalabilité de l'analyse rétrograde, nous avons introduit les sémantiques parallèles et l'implémentation abstraite $\parallelqlibraname$. Le prochain chapitre présentera les résultats expérimentaux des quantificateurs d'impact \changesname{} et \qlibraname{} appliqués aux réseaux neuronaux. Par la suite, la thèse se concentrera sur les propriétés intentionnelles, en particulier sur la manière de quantifier l'influence des variables d'entrée sur le nombre d'itérations d'un programme.}
