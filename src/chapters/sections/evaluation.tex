\chapter{Experimental Evaluation}
\labch{experimental-evaluation}

This section outlines the experimental evaluation of \nrefch{quantitative-input-data-usage}, \nrefch{quantitative-fairness}, and \nrefch{quantitative-static-timing-analysis}.
Respectively, the evaluation is divided into three parts: a showcase of impact notions against some use case programs (\refsec{showcase-of-outcomes-and-ranges}), an evaluation on neural networks (\refsec{evaluation-on-neural-networks}), and a study on timing side-channels (\refsec{timing-side-channels}).

\emph{Cette section présente l'évaluation expérimentale de \nrefch{quantitative-input-data-usage}, \nrefch{quantitative-fairness}, et \nrefch{quantitative-static-timing-analysis}.
Respectivement, l'évaluation est divisée en trois parties : une démonstration des notions d'impact contre certains programmes d'utilisation, une évaluation sur les réseaux neuronaux, et une étude sur les canaux cachés temporels.}

\section{Showcase of \outcomesname{} and \rangename{}}
\labsec{showcase-of-outcomes-and-ranges}


The goal of this section is to highlight the potential of our static analysis for quantitative input data usage.
We implemented a proof-of-concept tool, called \impatto\sidenote{\url{https://github.com/denismazzucato/impatto}}, in Python 3 that employs the \interproc\sidenote{\url{https://github.com/jogiet/interproc}} abstract interpreter to perform the backward analysis.
Then, we exploited this tool to automatically derive a sound input data usage of six different scenarios.
As each impact result must be interpreted with respect to what the program computes, we analyze each scenario separately.

\subsection{Growth in a Time of Debt}
\labsec{rr}


Reinhart and Rogoff article ``Growth in a Time of Debt''~\sidecite{Reinhart2010} proposed a correlation between high levels of public debt and low economic growth,
and %. As a consequence, the article
was heavily cited to justify austerity measures around the world. %Notably,
One of the several errors discovered in the article is the incorrect usage of the input value relative to Norway's economic growth in 1964.
The data used in the article is publicly available but not the spreadsheet file. We reconstructed this simplified example based on
the technical critique by \sidetextcite{Herndon2014}, and an online discussion\sidenote{\url{https://economics.stackexchange.com/q/18553}}.
The~\refprog{rr}
computes the cross-country mean growth for the public debt-to-GDP $60-90\%$ category, key point to the article's conclusions.
The input data is the average growth rate for each country within this public dept-to-GDP category. The problem with this computation is that Norway has only one observation in such category, which alone could disrupt the mean computation among all the countries. Indeed, the year that Norway appears in the $60-90\%$ category achieved a growth rate of $10.2\%$, while the average growth rate for the other countries is $2.7\%$.
With such high rate, the mean growth rate raised to $3.4\%$, altering the article's conclusions.
We assume growth rate values between $-20\%$ and $20\%$ for all countries, consequentially, the output ranges are between these bounds as well. We instrumented the output buckets to cover the full output space in buckets of size $1$, \ie, $\setdef{t \le \texttt{avg} < t + 1}{-20 \le t \le 20}$.
%
\newcommand{\dg}{60}
\newcommand{\x}{\texttt{angle}}
\newcommand{\y}{\texttt{speed}}
\begin{table*}[t]
  \caption{Quantitative input usage for \refprog{rr} from the Reinhart and Rogoff's article.}
  \labtab{rr}
  \centering
  \begin{tabular}{c | ccccccccccc}
    \textsc{Impact} & \rotatebox{\dg}{\texttt{portugal1}} & \rotatebox{\dg}{\texttt{portugal2}} & \rotatebox{\dg}{\texttt{portugal3}} & \rotatebox{\dg}{\texttt{norway1}} & \rotatebox{\dg}{\texttt{uk1}} & \rotatebox{\dg}{\texttt{uk2}} & \rotatebox{\dg}{\texttt{uk3}} & \rotatebox{\dg}{\texttt{uk4}} & \rotatebox{\dg}{\texttt{us1}} & \rotatebox{\dg}{\texttt{us2}} & \rotatebox{\dg}{\texttt{us3}} \\
    \toprule
    \outcomesname{} & 5 & 5 & 5 & 10 & 2 & 2 & 2 & 2 & 3 & 3 & 3 \\
    \rangename{} & 5 & 5 & 5 & 10 & 2 & 2 & 2 & 2 & 3 & 3 & 3 \\
    \bottomrule
  \end{tabular}
\end{table*}
%
Results for both \outcomesname{} and \rangename{} are shown in \reftab{rr}.

\begin{lstlisting}[
  language=customPython,
  escapechar=\%,
  caption={Program computing the mean growth rate in the $60-90\%$ category.},
  label={lst:rr},
  % float,
  % floatplacement=H
]
 def mean_growth_rate_60_90(
     portugal1, portugal2, portugal3,
     norway1,
     uk1, uk2, uk3, uk4,
     us1, us2, us3):
   portugal_avg = (portugal1 + portugal2 + portugal3) / 3%\label{l:portugal-avg}%
   norway_avg = norway1%\label{l:norway-avg}%
   uk_avg = (uk1 + uk2 + uk3 + uk4) / 4%\label{l:uk-avg}%
   us_avg = (us1 + us2 + us3) / 3%\label{l:us-avg}%
   avg = (portugal_avg + norway_avg + uk_avg + us_avg) / 4%\label{l:final-avg}%
\end{lstlisting}
%\vspace{%-15pt}
%
%
The analysis discovers that the Norway's only observation for this category $\texttt{norway1}$ has the biggest impact on the output, as perturbations on its value are capable of reaching 10 different outcomes (\cf~column $\texttt{norway1}$), while the other countries only have 5, 2, and 3, respectively for Portugal, UK, and US.
The same applies to \rangename{} as the output buckets have size $1$ and all the input perturbations are only capable of reaching contiguous buckets. Hence, we obtain the same exact results.

Our analysis is able to discover the disproportionate impact of Norway's only observation in the mean computation, which would have prevented one of the several programming errors found in the article.
%Nevertheless,
From a review of~\refprog{rr}, it is clear that Norway's only observation has a greater contribution to the computation,
%of the average growth rate,
as it does not need to be averaged with other observations first.
However, such methodological error is less evident when dealing with a higher number of input observations ($1175$ observations in the original work) and the computation is hidden behind a spreadsheet.

% As noted in many reports, a possible solution would be to improve the weighting procedure or filter outliers.


\subsection{GPT-4 Turbo}
\labsec{gpt-4-turbo}

The second use case we present is drawn from Sam Altman's OpenAI keynote in September 2023\sidenote{\url{https://www.youtube.com/live/U9mJuUkhUzk?si=HOzuH3-gr_kTdhCt&t=2330}}, where he presented the GPT-4 Turbo.
This new version of the GPT-4 language model brings the ability to write and interpret code directly without the need of human interaction.
Hence, as showcased in the keynote, the user could prompt multiple information to the model, such as related to the organization of a holiday trip with friends in Paris, and the model automatically generates the code to compute the share of the total cost of the trip and run it in background.
In this environment, users are unable to directly view the code unless they access the backend console.
This limitation makes it challenging for them to evaluate whether the function has been implemented correctly or not, assuming users have the capability to do so.
%
From the keynote, we extracted the~\refprog{share-division} which computes the user's share of the total cost of a holiday trip to Paris, given the total cost of the Airbnb, the flight cost, and the number of friends going on the trip.
%
\begin{table}
  \caption{Quantitative input usage for \refprog{share-division} computing the share division among friends.}
  \labtab{gpt-4-turbo}
  \begin{tabular}{c | ccc}
    \textsc{Impact} & \rotatebox{45}{\texttt{airbnb\_total\_cost\_eur}} & \rotatebox{45}{\texttt{flight\_cost\_usd}} & \rotatebox{45}{\texttt{number\_of\_friends}} \\
    \toprule
    \outcomesname{} & 10 & 17 & 9 \\
    \rangename{} & 1099 & 1709 & 999 \\
    \bottomrule
  \end{tabular}
\end{table}
%

\begin{lstlisting}[
  language=customPython,
  escapechar=\%,
  label={lst:share-division},
  caption={Program computing share division for holiday planning among friends.},
  % float,
  % floatplacement=H
]
 def share_division(
     airbnb_total_cost_eur,
     flight_cost_usd,
     number_of_friends):
   share_airbnb = airbnb_total_cost_eur / number_of_friends
   usd_to_eur = 0.92
   flight_cost_eur = flight_cost_usd * usd_to_eur
   total_cost_eur = share_airbnb + flight_cost_eur
\end{lstlisting}
%
Regarding the input bounds, users are willing to spend between 500 and 2000 for the Airbnb, between 50 and 1000 for the flight, and travel with between 2 and 10 friends. As a result, they expect their share, variable $\texttt{total\_cost\_eur}$, to be between 90 and 1900.
To compute the impact of the input variables we choose the output buckets to cover the expected output space in buckets of size $100$, \ie, $\setdef{100t + 90 \le \texttt{total\_cost\_eur} < \min \{100(t + 1) + 90, 1900\}}{0 \le t \le 19}$.
The %analysis discovers similar
findings are similar for both the \outcomesname{} and \rangename{} analysis, see~\reftab{gpt-4-turbo}.
The input variable $\texttt{flight\_cost\_usd}$ has the biggest impact on the output, as perturbations on its value are capable of reaching 17 different output buckets (resp. a range of 1709 output values), while the other two, $\texttt{airbnb\_total\_cost\_eur}$ and $\texttt{number\_of\_friends}$, only reach 10 and 9 output buckets (resp. have ranges of size 1099 and 999), respectively.
%Since the output buckets reached by perturbations of input values are contiguous, the \rangename{} analysis shows similar findings: 1709, 1099, and 999, respectively for \texttt{flight\_cost\_usd}, \texttt{airbnb\_total\_cost\_eur}, and \texttt{number\_of\_friends},

These results confirm the user expectations about the proposed program from ChatGPT: the flight cost yields the biggest impact as it cannot be shared among friends.


\subsection{Termination Analysis (A)}
\labsec{termination-analysis-A}

%
\begin{marginlisting}
  \caption{Example program from termination analysis.}
  \labprog{timing-analysis}
  \vspace{0.5cm}
\begin{lstlisting}[
    language=customPython,
    escapechar=\%,
    ]
 def example(x, y):
   counter = 0
   while x >= 0:
     if y <= 50:
       x += 1
     else
       x -= 1
     y += 1
     counter += 1
\end{lstlisting}
\end{marginlisting}

\refprog{timing-analysis} is adapted from the termination category of the software verification competition \textsc{sv-comp}\sidenote{\url{https://sv-comp.sosy-lab.org/}}.
Assuming both input positives, $\texttt{x},\texttt{y} \ge 0$, this program terminates in $\texttt{x}+1$ iterations if $\texttt{y} >50$, otherwise it terminates in $\texttt{x} - 2\texttt{y} + 103$ iterations.
We define $\texttt{counter}$ as the output variable, with output buckets defined as $\setdef{10k \le \texttt{counter} < 10(k+1)}{0 \le k < 50}$ and $\{\texttt{counter}\ge 500\}$. These output buckets represent cumulative ranges of iterations required for termination.
The analysis results are illustrated in~\reftab{termination-analysis}, they show that the input variable $\texttt{x}$ has the biggest impact.
Modifying the value of $\texttt{x}$ can result in the program terminating within any of the other 50 iteration ranges.
On the other hand, perturbations on $\texttt{y}$ can only result in the program terminating within 10 different iteration ranges.
Such difference is motivated by the fact that $\texttt{y}$ is only used to determine the number of iterations in the case where $\texttt{y}$ is greater than 50, otherwise it is not used at all. Therefore, two values of $\texttt{y}$, \eg, $y_0$ and $y_1$, only result in two different ranges of iterations required to make the program terminate if either both of them are below $50$ or $y_0 < 50\land y_1 \ge 50$ or $y_0\ge50\land y_1 <50$, not in all the cases.


\begin{margintable}[-4cm]
  \caption{Quantitative input usage for \refprog{timing-analysis}.}
  \labtab{termination-analysis}
  \begin{tabular}{c | c@{\hskip 5pt}c}
    \textsc{Impact} & \rotatebox{0}{\texttt{x}} & \rotatebox{0}{\texttt{y}} \\
    \toprule
    \outcomesname{} & 50 & 10 \\
    \rangename{} & 499 & 99 \\
    \bottomrule
  \end{tabular}
\end{margintable}

The given results can be interpreted as follows: the speed of termination of this loop is highly dependent on the value of $\texttt{x}$, while $\texttt{y}$ has a much smaller impact.
% This information could be used to attack such program just by looking at its timing behavior.
% For instance, given the number of iterations \texttt{counter}, we infer that the value of \texttt{x} is either $\texttt{counter} - 1$ or $\texttt{counter} + 2\texttt{y} - 103$. On the other hand, since \texttt{y} has less impact as discovered by our tool, we cannot infer much about its value.



\subsection{Termination Analysis (B)}
\labsec{app:termination-analysis-B}

This use case comes from the software verification competition SV-Comp\sidenote{\url{https://sv-comp.sosy-lab.org/}}, where the goal is to verify the termination of a program. \refprog{termination-a} and~\refprog{termination-b} have originally been proposed by \sidetextcite{Chen2012}, respectively these are Example (2.16) and Example (2.21) of such work.

\begin{marginlisting}
  \caption{Program Ex2.16 from software verification competition SV-Comp.}
  \labprog{termination-a}
  \vspace{0.5cm}
\begin{lstlisting}[
  language=customPython,
  escapechar=\%,
]
def termination_a(x, y):
  while x > 0:
    x = y
    y = y - 1
  result = x + y
  return result
\end{lstlisting}
\end{marginlisting}


\refprog{termination-a} returns the value of \texttt{y} whenever $\texttt{x} = 0$, otherwise it returns $-1$.
We assume both input variables are positive up to $1000$, $0 \le \texttt{x} \le 1000$ and $0 \le \texttt{y} \le 1000$.
Regarding such a function, it is interesting to study its behaviors around $0$, thus the output bucket are $\{ \texttt{result < 0} \}, \{ \texttt{result} = 0 \}$, and $\{ \texttt{result > 0} \}$.
With the above parameters, the analysis \outcomesname{} returns 1 for both input variables.
Such result is not too interesting, but by looking at the internal stages of the analysis we notice that perturbations on the value of the variable \texttt{x} may be able to produce from an output negative value to zero or a positive one (and viceversa).
While perturbations on the value of the variable \texttt{y} are only able to produce from zero to positive (and viceversa).

As a second experiments, we consider the buckets from -1 to 19, $\setdef{ \texttt{result} = n}{-1 \le n \le 19}$, and we notice that the analysis \outcomesname{} returns 1 for the input variable \texttt{x} and 19 for \texttt{y}, meaning that the variable \texttt{y} is able to affect far more output values than \texttt{x}. However, combing the results of the previous experiment, only the variable \texttt{x} is able to affect the negative output values.

\begin{marginlisting}[-1.4cm]
  \caption{Program Ex2.21 from software verification competition SV-Comp.}
  \labprog{termination-b}
  \vspace{0.5cm}
\begin{lstlisting}[
  language=customPython,
  escapechar=\%,
]
def termination_b(x, y):
  while x > 0:
    x = x + y
    y = -y - 1
  result = x + y
  return result
\end{lstlisting}
\end{marginlisting}

From the same work, we also consider \refprog{termination-b} which returns the value of \texttt{y} whenever $\texttt{x} = 0$, otherwise it returns $-1$.
Unfortunately, the backward analysis does not capture a precise loop invariant, thus both the analyses \outcomesname{} and \rangename are inconclusive in such case.
The key takeaway is that our analysis is highly dependent on the precision of the underlying backward analysis.

As a conclusion, even though SV-Comp proposes challenging benchmarks for termination, reachability, and safety analyses, they are not amenable for information flow analysis.
Most of the time, their examples involve loops with complex invariant, but as input-output relations, the variables involved are just zeroed out after the loop.
Drawing examples from their dataset is less appealing to our work.

\subsection{Linear Loops}
\labsec{linear-loops}


\begin{marginlisting}
  \caption{Program computing the linear expression $(5x + 2y)$ via repeated additions.}
  \labprog{linear-expression}
  \vspace{0.9cm}
\begin{lstlisting}[
  language=customPython,
  escapechar=\%,
]
def linear_expression(x, y):
  result = 0
  i = 0
  while i < 5:
    result = result + x
    i += 1
  i = 0
  while i < 2:
    result = result + y
    i += 1
\end{lstlisting}
\end{marginlisting}

\refprog{linear-expression} computes the linear expression $(5x + 2y)$ via repeated additions.
Note that the invariant of the loop is indeed non-linear ($\texttt{result} = \texttt{i} * \texttt{x}$ and $\texttt{result} = \texttt{result}' + \texttt{i} * \texttt{y}$ respectively for the first and second loop, where $\texttt{result}'$ is the value of \texttt{result} before entering the second loop), but the loop is executed a fixed number of times, thus the analysis is able to compute the exact output buckets through loop unrolling.

For the analysis the input bounds are $0 \le \texttt{x} \le 1000$ and $0 \le \texttt{y} \le 1000$, while the output buckets are $\setdef{n * 100 \le \texttt{result} < (n + 1) * 100}{n \le 70}$.
Both analyses, \outcomesname{} and \rangename, show that \texttt{x} has an impact $\frac{5}{2}$ times bigger than \texttt{y} on the output.
Thus, the impact quantity provides insight about the termination speed.
Indeed, the loop for \texttt{x} is executed 5 times, while the one for \texttt{y} only 2.


\subsection{Landing Risk System}
\labsec{landing-risk}




\begin{figure}[t]
  \centering
\begin{tikzpicture}
  % Grid
  \draw[help lines, color=gray!30, dashed] (-0.1,-0.1) grid (9.9,3.9);
  % x-axis
  \draw[->,ultra thick] (0,0)--(10,0) node[rotate=90,below]{\x};
  % y-axis
  \draw[->,ultra thick] (0,0)--(0,4) node[above]{\y};
  % x-axis ticks
  \foreach \x in {-4,-3,-2,-1,0,1,2,3,4}
      \draw (\x+5,0.1) -- (\x+5,-0.1) node[below] {\x};
  % y-axis ticks
  \foreach \y in {1,2,3}
      \draw (0.1,\y) -- (-0.1,\y) node[left] {\y};
  % Polyhedra
  \fill[color=seabornGreen, opacity=0.5] (5,3) -- (7,1) -- (3,1) -- cycle;
  \draw[color=seabornGreen, ultra thick] (5,3) -- (7,1) -- (3,1) -- cycle;
  % Polyhedra
  \fill[color=seabornYellow, opacity=0.5] (4,3) -- (5,3) -- (3,1) -- (2,1) -- cycle;
  \draw[color=seabornYellow, ultra thick] (4,3) -- (5,3) -- (3,1) -- (2,1) -- cycle;
  % % Polyhedra
  \fill[color=seabornYellow, opacity=0.5] (5,3) -- (6,3) -- (8,1) -- (7,1) -- cycle;
  \draw[color=seabornYellow, ultra thick] (5,3) -- (6,3) -- (8,1) -- (7,1) -- cycle;
  % Polyhedra
  \fill[color=seabornOrange, opacity=0.5] (3,3) -- (4,3) -- (2,1) -- (1,1) -- cycle;
  \draw[color=seabornOrange, ultra thick] (3,3) -- (4,3) -- (2,1) -- (1,1) -- cycle;
  % % Polyhedra
  \fill[color=seabornOrange, opacity=0.5] (6,3) -- (7,3) -- (9,1) -- (8,1) -- cycle;
  \draw[color=seabornOrange, ultra thick] (6,3) -- (7,3) -- (9,1) -- (8,1) -- cycle;
  % Polyhedra
  \fill[color=seabornRed, opacity=0.5] (1,3) -- (3,3) -- (1,1) -- cycle;
  \draw[color=seabornRed, ultra thick] (1,3) -- (3,3) -- (1,1) -- cycle;
  % Polyhedra
  \fill[color=seabornRed, opacity=0.5] (7,3) -- (9,3) -- (9,1) -- cycle;
  \draw[color=seabornRed, ultra thick] (7,3) -- (9,3) -- (9,1) -- cycle;
  % Nodes
  \fill[color=seabornRed] (0+1,0+1) circle[radius=2pt];
  \node[above left] at (0+1,0+1) {$3$};
  \fill[color=seabornRed] (0+1,1+1) circle[radius=2pt];
  \node[above left] at (0+1,1+1) {$3$};
  \fill[color=seabornRed]    (0+1,2+1) circle[radius=2pt];
  \node[above left] at (0+1,2+1) {$3$};
  \fill[color=seabornOrange] (1+1,0+1) circle[radius=2pt];
  \node[above left] at (1+1,0+1) {$2$};
  \fill[color=seabornRed]    (1+1,1+1) circle[radius=2pt];
  \node[above left] at (1+1,1+1) {$3$};
  \fill[color=seabornRed] (1+1,2+1) circle[radius=2pt];
  \node[above left] at (1+1,2+1) {$3$};
  \fill[color=seabornYellow]    (2+1,0+1) circle[radius=2pt];
  \node[above left] at (2+1,0+1) {$1$};
  \fill[color=seabornOrange] (2+1,1+1) circle[radius=2pt];
  \node[above left] at (2+1,1+1) {$2$};
  \fill[color=seabornRed] (2+1,2+1) circle[radius=2pt];
  \node[above left] at (2+1,2+1) {$3$};
  \fill[color=seabornGreen] (3+1,0+1) circle[radius=2pt];
  \node[above left] at (3+1,0+1) {$0$};
  \fill[color=seabornYellow] (3+1,1+1) circle[radius=2pt];
  \node[above left] at (3+1,1+1) {$1$};
  \fill[color=seabornOrange]    (3+1,2+1) circle[radius=2pt];
  \node[above left] at (3+1,2+1) {$2$};
  \fill[color=seabornGreen] (4+1,0+1) circle[radius=2pt];
  \node[above left] at (4+1,0+1) {$0$};
  \fill[color=seabornGreen]    (4+1,1+1) circle[radius=2pt];
  \node[above left] at (4+1,1+1) {$0$};
  \fill[color=seabornYellow]   (4+1,2+1) circle[radius=2pt];
  \node[above left] at (4+1,2+1) {$1$};
  \fill[color=seabornGreen]    (5+1,0+1) circle[radius=2pt];
  \node[above right] at (5+1,0+1) {$0$};
  \fill[color=seabornYellow]   (5+1,1+1) circle[radius=2pt];
  \node[above right] at (5+1,1+1) {$1$};
  \fill[color=seabornOrange]   (5+1,2+1) circle[radius=2pt];
  \node[above right] at (5+1,2+1) {$2$};
  \fill[color=seabornYellow]   (6+1,0+1) circle[radius=2pt];
  \node[above right] at (6+1,0+1) {$1$};
  \fill[color=seabornOrange]   (6+1,1+1) circle[radius=2pt];
  \node[above right] at (6+1,1+1) {$2$};
  \fill[color=seabornRed]   (6+1,2+1) circle[radius=2pt];
  \node[above right] at (6+1,2+1) {$3$};
  \fill[color=seabornOrange]   (7+1,0+1) circle[radius=2pt];
  \node[above right] at (7+1,0+1) {$2$};
  \fill[color=seabornRed]   (7+1,1+1) circle[radius=2pt];
  \node[above right] at (7+1,1+1) {$3$};
  \fill[color=seabornRed]   (7+1,2+1) circle[radius=2pt];
  \node[above right] at (7+1,2+1) {$3$};
  \fill[color=seabornRed]   (8+1,0+1) circle[radius=2pt];
  \node[above right] at (8+1,0+1) {$3$};
  \fill[color=seabornRed]   (8+1,1+1) circle[radius=2pt];
  \node[above right] at (8+1,1+1) {$3$};
  \fill[color=seabornRed]   (8+1,2+1) circle[radius=2pt];
  \node[above right] at (8+1,2+1) {$3$};
\end{tikzpicture}
\caption{Input space composition with continuous input values.}
\labfig{extended}
\end{figure}


\begin{table}[t]
  \caption{Quantitative input usage for~\refprog{landing-alarm-system}.}
  \labtab{landing-risk}
  \centering
  \begin{tabular}{cc|cc|cc}
    \multicolumn{2}{c|}{\multirow{2}{*}{~\textbf{Input Bounds}}} & \multicolumn{2}{c|}{\outcomesname} & \multicolumn{2}{c}{\rangename} \\ \cline{3-6}
    & & \texttt{angle} & \hspace{-5.5pt}\texttt{speed} & \texttt{angle} & \hspace{-5.5pt}\texttt{speed} \\ \hline\hline
    % $\texttt{angle} = -1 \lor \texttt{angle} = 4$ & \multirow{4}{*}{$1 \le \texttt{speed} \le 3$} & &
    % 1  &  2  & 3  & 2  \\ \cline{1-1} \cline{4-7}
    $-4 \le \texttt{angle} \le 4$ & \multirow{3}{*}{$~~\land 1 \le \texttt{speed} \le 3$} &
    3  &  3  & 3  & 3  \\ \cline{1-1} \cline{3-6}
    $-4 \le \texttt{angle} \le 0$ & &
    3  &  2  & 3  & 2  \\ \cline{1-1} \cline{3-6}
    $0 \le \texttt{angle} \le 4$ & &
    3 &  2 & 3 & 2 \\
  \end{tabular}
\end{table}

Finally, we apply our quantitative analysis to~\refprog{landing-alarm-system}\marginprop{landing-alarm-system} (reported on the side) for the landing alarm system extended with the  continuous input space for the aircraft angle of approach, where $(-4 \le \texttt{angle} \le 4) \land (1 \le \texttt{speed} \le 3)$, see \reffig{extended}.
In this instance, the precision of the abstraction drastically drops as convex abstract domains are not able to capture the symmetric features of the input space around 0.
Indeed, the analysis result, first row of~\reffig{analysis}, is unable to reveal any difference in the input usage of input variables as all the abstract preconditions result of the backward analysis intersect together.
As a consequence, \outcomesname{} and \rangename{} are unable to provide any meaningful information, first row of \reftab{landing-risk}.

A possible approach to overcome the non-convexity of the input space is to split the input space into two subspaces (as a bounded set of disjunctive polyhedra), $-4 \le \texttt{angle} \le 0$ and $0 \le \texttt{angle} \le 4$, second and third row of \reftab{landing-risk}.
In the first subset $-4 \le \texttt{angle} \le 0$, we are able to perfectly captures the input regions that lead to each output bucket with our abstract analysis, second row of~\reffig{analysis}.
Therefore, we are able to recover the information that the input configurations from the bucket $\{\texttt{risk} =3\}$ do not intersect with the ones from the bucket $\{\texttt{risk} = 0\}$ after projecting away the axis \texttt{speed}.
As the end, our analysis notices that variations in the value of the input \texttt{angle} results in three possible output values, while variations in the \texttt{speed} input lead to two.
Similarly, regarding the range of values, variations in the \texttt{angle} input cover the entire spectrum of output values, whereas to the \texttt{speed} input only span a range of 2 since it exists no input value such that modifications in the \texttt{speed} value could obtain a range of output values bigger than 2.
The same reasoning applies to the other subspace with $0 \le \texttt{angle} \le 4$.


\begin{figure*}[t]
  \centering
  \begin{subfigure}{\textwidth}
  \begin{subfigure}[b]{0.24\textwidth}
    \begin{tikzpicture}[scale=0.8]
      % Grid
      \foreach \y in {0.5, 1.5, 2.5} {
        \draw[help lines, color=gray!30, dashed] (0,\y) -- (2.9,\y);
      }
      \foreach \x in {0.5, 1, 1.5, 2, 2.5} {
        \draw[help lines, color=gray!30, dashed] (\x, 0) -- (\x, 2.9);
      }
      % x-axis
      \draw[->,ultra thick] (0,0)--(3,0);
      % \draw[->,ultra thick] (0,0)--(3,0) node[rotate=90,below]{\x};
      % % y-axis
      \draw[->,ultra thick] (0,0)--(0,3) node[above]{\y};
      % % x-axis ticks
      \draw (0.5,0.1) -- (0.5,-0.1) node[below] {$-4$};
      % \draw (1,0.1) -- (1,-0.1) node[below] {$-2$};
      \draw (1,0.1) -- (1,-0.1);
      \draw (1.5,0.1) -- (1.5,-0.1) node[below] {$0$};
      % \draw (2,0.1) -- (2,-0.1) node[below] {$2$};
      \draw (2,0.1) -- (2,-0.1);
      \draw (2.5,0.1) -- (2.5,-0.1) node[below] {$4$};
      % % y-axis ticks
      \foreach \y in {1,2,3}
          \draw (0.1,\y-0.5) -- (-0.1,\y-0.5) node[left] {\y};
      % % Polyhedra
      \fill[color=seabornRed, opacity=0.5] (0.5,0.5) -- (2.5,0.5) -- (2.5,2.5) -- (0.5,2.5) -- cycle;
      \draw[color=seabornRed, ultra thick] (0.5,0.5) -- (2.5,0.5) -- (2.5,2.5) -- (0.5,2.5) -- cycle;
    \end{tikzpicture}
    % \caption{$\{\texttt{risk} = 3\}$}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.23\textwidth}
    \begin{tikzpicture}[scale=0.8]
      % Grid
      \foreach \y in {0.5, 1.5, 2.5} {
        \draw[help lines, color=gray!30, dashed] (0,\y) -- (2.9,\y);
      }
      \foreach \x in {0.5, 1, 1.5, 2, 2.5} {
        \draw[help lines, color=gray!30, dashed] (\x, 0) -- (\x, 2.9);
      }
      % x-axis
      \draw[->,ultra thick] (0,0)--(3,0);
      % \draw[->,ultra thick] (0,0)--(3,0) node[rotate=90,below]{\x};
      % % y-axis
      % \draw[->,ultra thick] (0,0)--(0,3) node[above]{\y};
      % % x-axis ticks
      \draw (0.5,0.1) -- (0.5,-0.1) node[below] {$-4$};
      % \draw (1,0.1) -- (1,-0.1) node[below] {$-2$};
      \draw (1,0.1) -- (1,-0.1);
      \draw (1.5,0.1) -- (1.5,-0.1) node[below] {$0$};
      % \draw (2,0.1) -- (2,-0.1) node[below] {$2$};
      \draw (2,0.1) -- (2,-0.1);
      \draw (2.5,0.1) -- (2.5,-0.1) node[below] {$4$};
      % % y-axis ticks
      % \foreach \y in {1,2,3}
      %     \draw (0.1,\y-0.5) -- (-0.1,\y-0.5) node[left] {\y};
      % % Polyhedra
      \fill[color=seabornOrange, opacity=0.5] (0.5,0.5) -- (2.5,0.5) -- (2.25,2.5) -- (0.75,2.5) -- cycle;
      \draw[color=seabornOrange, ultra thick] (0.5,0.5) -- (2.5,0.5);
      \draw[color=seabornOrange, ultra thick] (0.75,2.5) -- (2.25,2.5);
      \draw[color=seabornOrange, ultra thick, dotted] (2.5,0.5) -- (2.25,2.5);
      \draw[color=seabornOrange, ultra thick, dotted] (0.75,2.5) -- (0.5,0.5);
    \end{tikzpicture}
    % \caption{$\{\texttt{risk} = 2\}$}
  \end{subfigure}
  % \hfill
  \begin{subfigure}[b]{0.23\textwidth}
    \begin{tikzpicture}[scale=0.8]
      % Grid
      \foreach \y in {0.5, 1.5, 2.5} {
        \draw[help lines, color=gray!30, dashed] (0,\y) -- (2.9,\y);
      }
      \foreach \x in {0.5, 1, 1.5, 2, 2.5} {
        \draw[help lines, color=gray!30, dashed] (\x, 0) -- (\x, 2.9);
      }
      % x-axis
      \draw[->,ultra thick] (0,0)--(3,0);
      % \draw[->,ultra thick] (0,0)--(3,0) node[rotate=90,below]{\x};
      % % y-axis
      % \draw[->,ultra thick] (0,0)--(0,3) node[above]{\y};
      % % x-axis ticks
      \draw (0.5,0.1) -- (0.5,-0.1) node[below] {$-4$};
      % \draw (1,0.1) -- (1,-0.1) node[below] {$-2$};
      \draw (1,0.1) -- (1,-0.1);
      \draw (1.5,0.1) -- (1.5,-0.1) node[below] {$0$};
      % \draw (2,0.1) -- (2,-0.1) node[below] {$2$};
      \draw (2,0.1) -- (2,-0.1);
      \draw (2.5,0.1) -- (2.5,-0.1) node[below] {$4$};
      % % y-axis ticks
      % \foreach \y in {1,2,3}
      %     \draw (0.1,\y-0.5) -- (-0.1,\y-0.5) node[left] {\y};
      % % Polyhedra
      \fill[color=seabornYellow, opacity=0.5] (1,0.5) -- (2,0.5) -- (1.75,2.5) -- (1.25,2.5) -- cycle;
      \draw[color=seabornYellow, ultra thick] (1,0.5) -- (2,0.5);
      \draw[color=seabornYellow, ultra thick] (1.75,2.5) -- (1.25,2.5);
      \draw[color=seabornYellow, ultra thick, dotted] (2,0.5) -- (1.75,2.5);
      \draw[color=seabornYellow, ultra thick, dotted] (1.25,2.5) -- (1,0.5);
    \end{tikzpicture}
    % \caption{$\{\texttt{risk} = 1\}$}
  \end{subfigure}
  % \hfill
  \begin{subfigure}[b]{0.24\textwidth}
    \begin{tikzpicture}[scale=0.8]
      % Grid
      \foreach \y in {0.5, 1.5, 2.5} {
        \draw[help lines, color=gray!30, dashed] (0,\y) -- (2.9,\y);
      }
      \foreach \x in {0.5, 1, 1.5, 2, 2.5} {
        \draw[help lines, color=gray!30, dashed] (\x, 0) -- (\x, 2.9);
      }
      % x-axis
      % \draw[->,ultra thick] (0,0)--(3,0);
      \draw[->,ultra thick] (0,0)--(3,0) node[rotate=90,below]{\x};
      % % y-axis
      % \draw[->,ultra thick] (0,0)--(0,3) node[above]{\y};
      % % x-axis ticks
      \draw (0.5,0.1) -- (0.5,-0.1) node[below] {$-4$};
      % \draw (1,0.1) -- (1,-0.1) node[below] {$-2$};
      \draw (1,0.1) -- (1,-0.1);
      \draw (1.5,0.1) -- (1.5,-0.1) node[below] {$0$};
      % \draw (2,0.1) -- (2,-0.1) node[below] {$2$};
      \draw (2,0.1) -- (2,-0.1);
      \draw (2.5,0.1) -- (2.5,-0.1) node[below] {$4$};
      % % y-axis ticks
      % \foreach \y in {1,2,3}
      %     \draw (0.1,\y-0.5) -- (-0.1,\y-0.5) node[left] {\y};
      % % Polyhedra
      \fill[color=seabornGreen, opacity=0.5] (1.25,0.5) -- (1.75,0.5) -- (1.5,2.5) -- cycle;
      \draw[color=seabornGreen, ultra thick] (1.25,0.5) -- (1.75,0.5);
      \draw[color=seabornGreen, ultra thick, dotted] (1.75,0.5) -- (1.5,2.5);
      \draw[color=seabornGreen, ultra thick, dotted] (1.5,2.5) -- (1.25,0.5);
    \end{tikzpicture}
    % \caption{$\{\texttt{risk} = 0\}$}
  \end{subfigure}
% \caption{Analysis result using the polyhedra domain.}
\label{fig:analysis-extended}
\end{subfigure}
\begin{subfigure}{\textwidth}
  \begin{subfigure}[b]{0.24\textwidth}
    \begin{tikzpicture}[scale=0.8]
      % Grid
      \foreach \y in {0.5, 1.5, 2.5} {
        \draw[help lines, color=gray!30, dashed] (0,\y) -- (2.9,\y);
      }
      \foreach \x in {0.5, 1, 1.5, 2, 2.5} {
        \draw[help lines, color=gray!30, dashed] (\x, 0) -- (\x, 2.9);
      }
      % x-axis
      \draw[->,ultra thick] (0,0)--(3,0);
      % \draw[->,ultra thick] (0,0)--(3,0) node[rotate=90,below]{\x};
      % % y-axis
      \draw[->,ultra thick] (0,0)--(0,3) node[above]{\y};
      \draw[dashed] (1.5,0)--(1.5,3);
      % % x-axis ticks
      \draw (0.5,0.1) -- (0.5,-0.1) node[below] {$-4$};
      % \draw (1,0.1) -- (1,-0.1) node[below] {$-2$};
      \draw (1,0.1) -- (1,-0.1);
      \draw (1.5,0.1) -- (1.5,-0.1) node[below] {$0$};
      % \draw (2,0.1) -- (2,-0.1) node[below] {$2$};
      \draw (2,0.1) -- (2,-0.1);
      \draw (2.5,0.1) -- (2.5,-0.1) node[below] {$4$};
      % % y-axis ticks
      \foreach \y in {1,2,3}
          \draw (0.1,\y-0.5) -- (-0.1,\y-0.5) node[left] {\y};
      % % Polyhedra
      \fill[color=seabornRed, opacity=0.5] (0.5,0.5) -- (1,2.5) -- (0.5,2.5) -- cycle;
      \fill[color=seabornRed, opacity=0.5] (2.5,0.5) -- (2.5,2.5) -- (2,2.5) -- cycle;
      \draw[color=seabornRed, ultra thick] (0.5,0.5) -- (1,2.5) -- (0.5,2.5) -- cycle;
      \draw[color=seabornRed, ultra thick] (2.5,0.5) -- (2.5,2.5) -- (2,2.5) -- cycle;
    \end{tikzpicture}
    % \caption{$\{\texttt{risk} = 3\}$}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.23\textwidth}
    \begin{tikzpicture}[scale=0.8]
      % Grid
      \foreach \y in {0.5, 1.5, 2.5} {
        \draw[help lines, color=gray!30, dashed] (0,\y) -- (2.9,\y);
      }
      \foreach \x in {0.5, 1, 1.5, 2, 2.5} {
        \draw[help lines, color=gray!30, dashed] (\x, 0) -- (\x, 2.9);
      }
      % x-axis
      \draw[->,ultra thick] (0,0)--(3,0);
      \draw[dashed] (1.5,0)--(1.5,3);
      % \draw[->,ultra thick] (0,0)--(3,0) node[rotate=90,below]{\x};
      % % y-axis
      % \draw[->,ultra thick] (0,0)--(0,3) node[above]{\y};
      % % x-axis ticks
      \draw (0.5,0.1) -- (0.5,-0.1) node[below] {$-4$};
      % \draw (1,0.1) -- (1,-0.1) node[below] {$-2$};
      \draw (1,0.1) -- (1,-0.1);
      \draw (1.5,0.1) -- (1.5,-0.1) node[below] {$0$};
      % \draw (2,0.1) -- (2,-0.1) node[below] {$2$};
      \draw (2,0.1) -- (2,-0.1);
      \draw (2.5,0.1) -- (2.5,-0.1) node[below] {$4$};
      % % y-axis ticks
      % \foreach \y in {1,2,3}
      %     \draw (0.1,\y-0.5) -- (-0.1,\y-0.5) node[left] {\y};
      % % Polyhedra
      \fill[color=seabornOrange, opacity=0.5] (0.5,0.5) -- (0.75,0.5) -- (1,2.5) -- (0.75,2.5) -- cycle;
      \fill[color=seabornOrange, opacity=0.5] (2.25,0.5) -- (2.5,0.5) -- (2.25,2.5) -- (2,2.5) -- cycle;
      \draw[color=seabornOrange, ultra thick] (0.5,0.5) -- (0.75,0.5);
      \draw[color=seabornOrange, ultra thick] (2.25,0.5) -- (2.5,0.5);
      \draw[color=seabornOrange, ultra thick] (0.75,2.5) -- (1,2.5);
      \draw[color=seabornOrange, ultra thick] (2,2.5) -- (2.25,2.5);
      \draw[color=seabornOrange, ultra thick, dotted] (2.5,0.5) -- (2.25,2.5);
      \draw[color=seabornOrange, ultra thick] (2.25,0.5) -- (2,2.5);
      \draw[color=seabornOrange, ultra thick, dotted] (0.75,2.5) -- (0.5,0.5);
      \draw[color=seabornOrange, ultra thick] (1,2.5) -- (0.75,0.5);
    \end{tikzpicture}
    % \caption{$\{\texttt{risk} = 2\}$}
  \end{subfigure}
  % \hfill
  \begin{subfigure}[b]{0.23\textwidth}
    \begin{tikzpicture}[scale=0.8]
      % Grid
      \foreach \y in {0.5, 1.5, 2.5} {
        \draw[help lines, color=gray!30, dashed] (0,\y) -- (2.9,\y);
      }
      \foreach \x in {0.5, 1, 1.5, 2, 2.5} {
        \draw[help lines, color=gray!30, dashed] (\x, 0) -- (\x, 2.9);
      }
      % x-axis
      \draw[->,ultra thick] (0,0)--(3,0);
      \draw[dashed] (1.5, 0)--(1.5, 3);
      % \draw[->,ultra thick] (0,0)--(3,0) node[rotate=90,below]{\x};
      % % y-axis
      % \draw[->,ultra thick] (0,0)--(0,3) node[above]{\y};
      % % x-axis ticks
      \draw (0.5,0.1) -- (0.5,-0.1) node[below] {$-4$};
      % \draw (1,0.1) -- (1,-0.1) node[below] {$-2$};
      \draw (1,0.1) -- (1,-0.1);
      \draw (1.5,0.1) -- (1.5,-0.1) node[below] {$0$};
      % \draw (2,0.1) -- (2,-0.1) node[below] {$2$};
      \draw (2,0.1) -- (2,-0.1);
      \draw (2.5,0.1) -- (2.5,-0.1) node[below] {$4$};
      % % y-axis ticks
      % \foreach \y in {1,2,3}
      %     \draw (0.1,\y-0.5) -- (-0.1,\y-0.5) node[left] {\y};
      % % Polyhedra
      \fill[color=seabornYellow, opacity=0.5] (1,0.5) -- (1.25,0.5) -- (1.5,2.5) -- (1.75,0.5) -- (2,0.5) -- (1.75,2.5) -- (1.25,2.5) -- cycle;
      \draw[color=seabornYellow, ultra thick] (1,0.5) -- (1.25,0.5);
      \draw[color=seabornYellow, ultra thick] (1.75,0.5) -- (2,0.5);
      \draw[color=seabornYellow, ultra thick] (1.75,2.5) -- (1.25,2.5);
      \draw[color=seabornYellow, ultra thick, dotted] (2,0.5) -- (1.75,2.5);
      \draw[color=seabornYellow, ultra thick] (1.75,0.5) -- (1.5,2.5);
      \draw[color=seabornYellow, ultra thick, dotted] (1.25,2.5) -- (1,0.5);
      \draw[color=seabornYellow, ultra thick] (1.5,2.5) -- (1.25,0.5);
    \end{tikzpicture}
    % \caption{$\{\texttt{risk} = 1\}$}
  \end{subfigure}
  % \hfill
  \begin{subfigure}[b]{0.24\textwidth}
    \begin{tikzpicture}[scale=0.8]
      % Grid
      \foreach \y in {0.5, 1.5, 2.5} {
        \draw[help lines, color=gray!30, dashed] (0,\y) -- (2.9,\y);
      }
      \foreach \x in {0.5, 1, 1.5, 2, 2.5} {
        \draw[help lines, color=gray!30, dashed] (\x, 0) -- (\x, 2.9);
      }
      % x-axis
      % \draw[->,ultra thick] (0,0)--(3,0);
      \draw[->,ultra thick] (0,0)--(3,0) node[rotate=90,below]{\x};
      \draw[dashed] (1.5, 0)--(1.5, 3);
      % % y-axis
      % \draw[->,ultra thick] (0,0)--(0,3) node[above]{\y};
      % % x-axis ticks
      \draw (0.5,0.1) -- (0.5,-0.1) node[below] {$-4$};
      % \draw (1,0.1) -- (1,-0.1) node[below] {$-2$};
      \draw (1,0.1) -- (1,-0.1);
      \draw (1.5,0.1) -- (1.5,-0.1) node[below] {$0$};
      % \draw (2,0.1) -- (2,-0.1) node[below] {$2$};
      \draw (2,0.1) -- (2,-0.1);
      \draw (2.5,0.1) -- (2.5,-0.1) node[below] {$4$};
      % % y-axis ticks
      % \foreach \y in {1,2,3}
      %     \draw (0.1,\y-0.5) -- (-0.1,\y-0.5) node[left] {\y};
      % % Polyhedra
      \fill[color=seabornGreen, opacity=0.5] (1.25,0.5) -- (1.75,0.5) -- (1.5,2.5) -- cycle;
      \draw[color=seabornGreen, ultra thick] (1.25,0.5) -- (1.75,0.5);
      \draw[color=seabornGreen, ultra thick, dotted] (1.75,0.5) -- (1.5,2.5);
      \draw[color=seabornGreen, ultra thick, dotted] (1.5,2.5) -- (1.25,0.5);
      \draw[color=seabornGreen, ultra thick, dotted] (1.5,2.5) -- (1.5,0.5);
    \end{tikzpicture}
    % \caption{$\{\texttt{risk} = 0\}$}
  \end{subfigure}
% \caption{Analysis result after splitting the input space into two subspaces around $\texttt{angle}=0$.}
\end{subfigure}
\caption{Above, result of the analysis with convex polyhedra. Below, result after splitting the input space into two subspaces around $\texttt{angle}=0$.}
%\vspace{%-15pt}
\labfig{analysis}
\end{figure*}


\section{Evaluation on Neural Networks}
\labsec{evaluation-on-neural-networks}

The impact definitions \rangename{} and \outcomesname{} evaluated above are not well suited for the analysis of neural networks.
To address this limitation, this section shows the evaluation of the impact definitions \chaptername{} (\refsec{eval-changes}) and \qlibraname{} (\refsec{eval-qlibra}) on the context of neural network models.

\subsection{Changes}
\labsec{eval-changes}



\paragraph{Experimental setup}

For our evaluation, we used public datasets from the online community platform Kaggle\sidenote{\url{https://www.kaggle.com}} to train several neural network models.
We focused on four datasets: \wine~\sidecite{Cortez2009}, \diabetes~\sidecite{Smith1988}, \rain~\sidecite{Young2019}, and \princess~\sidecite{Unmoved2023}.
We pre-processed the \rain{} database and removed non-continuous input features, as our prototype does not support discrete input features yet.
To preserve data consistency, since majority of daily weather observations were collected in Canberra, we eliminated the observations from other stations.
As a result of this pre-processing step, we retained approximately 2000 entries, aligning with the sizes of other datasets.
We trained about 700 networks per database by permuting the network structure and number of input features to obtain a uniform benchmark.
The number of input features ranges from at least 3, to the number of attribute of the databases (after pre-processing),
respectively, 10 attributes for \wine, 8 for \diabetes, 17 for \rain, and 13 for \princess.
The model size ranges from 2 to 6 hidden layers, each with 3 to 6 nodes, for a total of 6 to 36 nodes for each network model.
All models were trained with Keras\sidenote{\url{https://github.com/keras-team/keras}}, using the Adam optimizer~\sidecite{Kingma2014} with the default learning rate, and binary crossentropy as the loss function. Each model was trained for 150 iterations.
The obtained neural network accuracy usually depends on the chosen subset of input features which is usually lower than the accuracy achieved in the literature.
However, we remark that our study focuses on the impact analysis, therefore high accuracy is not needed in our benchmarks.
All models used in our experiments are open source as part of the tool implementing our static analysis, called \impatto\sidenote{Anonymized for submission}. %TODO: change to url after submission
In particular, the analysis performed by our tool is $\impatto{}(\defmodel, \definputvariable)=F(\disjunctivepolyabstractdomain, \defmodel, \definputvariable, \buckets, \abstractchanges)$ where the output buckets represent all the target classes in the model $\defmodel$, one for each bucket: $\buckets = \vecdef{\{\bigwedge_{j'\le m} x_{\outputlayer, j'}\le x_{\outputlayer, j}\}}{j\le m}$.
% For each model, we assume a uniform distribution of the input space.


To empirically check that our static analysis, specialized with $\abstractchanges$ and $\disjunctivepolyabstractdomain$, behaves similarly to $\changesname$, we uniformly sample 1000 points in the input space of the network and then apply $\changesname$ to this set; we refer to this result as the \textit{baseline}.
Indeed, this approach is not sound as we could miss changes in the outcome not exploited by unsampled points, however it is overall a close-enough approximation of $\changesname$.
%
We compare the result of \impatto{} with baseline employing four heuristics, called maximum common prefix length (\mcpl), relaxed maximum common prefix length (\rmcpl), Euclidean distance (\euclidean), and Manhattan distance (\manhattan), defined below.
% Let $\defaultanalysisletter\in\simpleset{\pfi,\rfe, \impatto}$ be an analysis, defined as a mapping from a neural network model $\defmodel$ and an input feature $\definputvariable$, to a quantity value.
% In particular,

Given two analyses $F, F'$ (\eg, \impatto{} and baseline), the heuristic (\mcpl) first sorts the result of the analyses $F, F'$ applied to all the input features, by decreasing order.
The heuristic then returns the corresponding indices of the sorted results.
Formally, it computes $I=\argsort_\definputvariable F(\defmodel, \definputvariable)$ and $J=\argsort_\definputvariable F'(\defmodel, \definputvariable)$ for the two analyses respectively, where $\argsort$ returns the corresponding indices of the sorted list (by decreasing order), \eg, $\argsort \langle 30, 65, 2, 60 \rangle = \langle 2,4,1,3\rangle$.
Afterwards, (\mcpl) retrieves the length of the maximal common prefix between $I$ and $J$:
\[
  \mcpl(I, J) \DefeQ
  \begin{cases}
    1+\mcpl(\langle i_2, \dots, i_m \rangle, \langle j_2, \dots, j_m \rangle) &\text{if } I = \langle i_1, \dots, i_m \rangle \land  J = \langle i_1, \dots, j_m \rangle\\
    0 & \text{otherwise}
  \end{cases}
\]
For instance, assuming $I=\langle 4,1,2,3,5 \rangle$ and $J=\langle 4,1,2,5,3 \rangle$, $\mcpl(I, J)= 3$ since the maximum common prefix is $\langle 4,1,2\rangle$ of length 3. The relaxed variation (\rmcpl) allows a 10\% of overlap among quantity values, \eg, given $\vecdef{F(\defmodel, \definputvariable)}{\definputvariable} = \langle 0.4, 0.95, 0.6, 1 \rangle$ and $\vecdef{F'(\defmodel, \definputvariable)}{\definputvariable} = \langle 30, 65, 2, 60 \rangle$, we obtain $I=\argsort \langle 0.4, 0.95, 0.6, 1 \rangle = \langle 4, 2, 3, 1 \rangle $ and $J=\argsort \langle 30, 65, 2, 60 \rangle = \langle 2, 4, 1, 3 \rangle$, hence $\mcpl(I, J)$ would return 0. However, a 10\% of margin of error permits to swap the indices of values 0.95 and 1 in the first list obtaining $\langle 2, 4, 3, 1 \rangle$, we say that $I$ is equivalent to $\langle 2, 4, 3, 1 \rangle$ (written $I\simeq\langle 2, 4, 3, 1 \rangle$), hence $\mcpl(\langle 2, 4, 3, 1 \rangle, J)=2$. Formally,
we define \rmcpl{} as the maximum of \mcpl{} of all the possible equivalences:
\[\rmcpl(I, J) \DefeQ \max \setdef{\mcpl(I', J')}{{I' \simeq I \land J' \simeq J}}\]
The Euclidean distance is defined as $\euclidean(I, J) \defeq \sqrt{\sum_k (I_k - J_k)^2}$, and the Manhattan distance as $\manhattan(I, J) \defeq \sum_k \cardinality{I_k - J_k}$.

\paragraph{Quantifying Usage of Input Features}


{\newlength\figureheight
\newlength\figurewidth
\setlength\figureheight{5cm}
\setlength\figurewidth{6cm}

\begin{figure}[t]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    % \input{figures/diabetes-abs_common-baseline-qlibra.tikz}
    \begin{tikzpicture}
      \begin{axis}[
      height=\figureheight,
      tick align=outside,
      tick pos=left,
      width=\figurewidth,
      xmin=-0.74, xmax=6.74,
      ymin=0, ymax=37.8,
      ylabel={Frequency},
      xlabel={Length},
      ]
      \draw[draw=none,fill=seabornBlue] (axis cs:3.6,0) rectangle (axis cs:4.4,24);
      \draw[draw=none,fill=seabornBlue] (axis cs:1.6,0) rectangle (axis cs:2.4,10);
      \draw[draw=none,fill=seabornBlue] (axis cs:0.6,0) rectangle (axis cs:1.4,23);
      \draw[draw=none,fill=seabornBlue] (axis cs:4.6,0) rectangle (axis cs:5.4,19);
      \draw[draw=none,fill=seabornBlue] (axis cs:2.6,0) rectangle (axis cs:3.4,34);
      \draw[draw=none,fill=seabornBlue] (axis cs:-0.4,0) rectangle (axis cs:0.4,36);
      \draw[draw=none,fill=seabornBlue] (axis cs:5.6,0) rectangle (axis cs:6.4,7);
      \end{axis}
    \end{tikzpicture}
    \caption{Maximum common prefix length.}
    \label{fig:diabetes-abscommon-baseline-qlibra}
  \end{subfigure}%
  \vspace{10pt}
  \hspace{0.05\textwidth}%
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    % \input{figures/diabetes-relaxed_common-baseline-qlibra.tikz}
    \begin{tikzpicture}
      \begin{axis}[
      height=\figureheight,
      tick align=outside,
      tick pos=left,
      width=\figurewidth,
      xmin=-0.74, xmax=6.74,
      ymin=0, ymax=31.5,
      xlabel={Length},
      ]
      \draw[draw=none,fill=seabornBlue] (axis cs:3.6,0) rectangle (axis cs:4.4,26);
      \draw[draw=none,fill=seabornBlue] (axis cs:1.6,0) rectangle (axis cs:2.4,30);
      \draw[draw=none,fill=seabornBlue] (axis cs:4.6,0) rectangle (axis cs:5.4,16);
      \draw[draw=none,fill=seabornBlue] (axis cs:2.6,0) rectangle (axis cs:3.4,30);
      \draw[draw=none,fill=seabornBlue] (axis cs:0.6,0) rectangle (axis cs:1.4,24);
      \draw[draw=none,fill=seabornBlue] (axis cs:-0.4,0) rectangle (axis cs:0.4,19);
      \draw[draw=none,fill=seabornBlue] (axis cs:5.6,0) rectangle (axis cs:6.4,8);
      \end{axis}
      \end{tikzpicture}
    \caption{Relaxed maximum common prefix length.}
    \label{fig:diabetes-relaxedcommon-baseline-qlibra}
  \end{subfigure}
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    % \input{figures/diabetes-euclidean-baseline-qlibra.tikz}
    \begin{tikzpicture}
      \begin{axis}[
      height=\figureheight,
      tick align=outside,
      tick pos=left,
      width=\figurewidth,
      xmin=-0.887213595499958, xmax=9.83148550549912,
      ymin=0, ymax=86.1,
      xlabel={Distance},
      ylabel={Frequency},
      ]
      \draw[draw=none,fill=seabornBlue] (axis cs:-0.4,0) rectangle (axis cs:0.4,82);
      \draw[draw=none,fill=seabornBlue] (axis cs:1.0142135623731,0) rectangle (axis cs:1.8142135623731,18);
      \draw[draw=none,fill=seabornBlue] (axis cs:3.34165738677394,0) rectangle (axis cs:4.14165738677394,7);
      \draw[draw=none,fill=seabornBlue] (axis cs:2.42842712474619,0) rectangle (axis cs:3.22842712474619,10);
      \draw[draw=none,fill=seabornBlue] (axis cs:4.69901951359278,0) rectangle (axis cs:5.49901951359278,2);
      \draw[draw=none,fill=seabornBlue] (axis cs:3.06410161513775,0) rectangle (axis cs:3.86410161513775,2);
      \draw[draw=none,fill=seabornBlue] (axis cs:2.04948974278318,0) rectangle (axis cs:2.84948974278318,8);
      \draw[draw=none,fill=seabornBlue] (axis cs:4.29041575982343,0) rectangle (axis cs:5.09041575982343,4);
      \draw[draw=none,fill=seabornBlue] (axis cs:4.07213595499958,0) rectangle (axis cs:4.87213595499958,3);
      \draw[draw=none,fill=seabornBlue] (axis cs:8.54427190999916,0) rectangle (axis cs:9.34427190999916,1);
      \draw[draw=none,fill=seabornBlue] (axis cs:5.25685424949238,0) rectangle (axis cs:6.05685424949238,1);
      \draw[draw=none,fill=seabornBlue] (axis cs:5.6,0) rectangle (axis cs:6.4,1);
      \draw[draw=none,fill=seabornBlue] (axis cs:3.84264068711928,0) rectangle (axis cs:4.64264068711929,8);
      \draw[draw=none,fill=seabornBlue] (axis cs:2.76227766016838,0) rectangle (axis cs:3.56227766016838,2);
      \draw[draw=none,fill=seabornBlue] (axis cs:6.2332495807108,0) rectangle (axis cs:7.0332495807108,1);
      \draw[draw=none,fill=seabornBlue] (axis cs:4.89150262212918,0) rectangle (axis cs:5.69150262212918,1);
      \draw[draw=none,fill=seabornBlue] (axis cs:6.81110255092798,0) rectangle (axis cs:7.61110255092798,1);
      \draw[draw=none,fill=seabornBlue] (axis cs:8.20232526704263,0) rectangle (axis cs:9.00232526704263,1);
      \end{axis}
      \end{tikzpicture}
    \caption{Euclidean distance}
    \label{fig:diabetes-euclidean-baseline-qlibra}
  \end{subfigure}%
  \hspace{0.05\textwidth}%
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    % \input{figures/diabetes-manhattan-baseline-qlibra.tikz}
    \begin{tikzpicture}
      \begin{axis}[
      height=\figureheight,
      tick align=outside,
      tick pos=left,
      width=\figurewidth,
      xmin=-1.44, xmax=21.44,
      ymin=0, ymax=86.1,
      xlabel={Distance},
      ]
      \draw[draw=none,fill=seabornBlue] (axis cs:-0.4,0) rectangle (axis cs:0.4,82);
      \draw[draw=none,fill=seabornBlue] (axis cs:1.6,0) rectangle (axis cs:2.4,18);
      \draw[draw=none,fill=seabornBlue] (axis cs:5.6,0) rectangle (axis cs:6.4,19);
      \draw[draw=none,fill=seabornBlue] (axis cs:3.6,0) rectangle (axis cs:4.4,18);
      \draw[draw=none,fill=seabornBlue] (axis cs:7.6,0) rectangle (axis cs:8.4,8);
      \draw[draw=none,fill=seabornBlue] (axis cs:19.6,0) rectangle (axis cs:20.4,1);
      \draw[draw=none,fill=seabornBlue] (axis cs:11.6,0) rectangle (axis cs:12.4,2);
      \draw[draw=none,fill=seabornBlue] (axis cs:13.6,0) rectangle (axis cs:14.4,2);
      \draw[draw=none,fill=seabornBlue] (axis cs:9.6,0) rectangle (axis cs:10.4,2);
      \draw[draw=none,fill=seabornBlue] (axis cs:17.6,0) rectangle (axis cs:18.4,1);
      \end{axis}
      \end{tikzpicture}
    \caption{Manhattan distance}
    \label{fig:diabetes-manhattan-baseline-qlibra}
  \end{subfigure}
  \caption{\diabetes{} database, comparison between baseline and \impatto.}
  \label{fig:diabetes-baseline-qlibra}
\end{figure}
}

We first verify whether \impatto{} produces a similar impact quantification with respect to $\changesname$.
To this end, we demonstrate \impatto{} in comparison to baseline.
This experiment uses the \diabetes{} dataset (full evaluation with all the databases in \refsec{full-experimental-overview}).
The evaluation, depicted in \reffig{diabetes-baseline-qlibra}, considers all the four heuristics.

The results confirm the similarity between \impatto{} and baseline.
\reffig{diabetes-abscommon-baseline-qlibra} and \reffig{diabetes-relaxedcommon-baseline-qlibra} show the exact and relaxed maximum common prefix length results.
In this context, the result of our analysis \impatto{} is fairly similar to the baseline, this is even enhanced by the relaxed heuristic where close impact quantities (up to 10\% difference) are softened together.
In particular, \reffig{diabetes-relaxedcommon-baseline-qlibra} shows that more than 100 test cases produce similar impact quantity to baseline up to, at least, the 3 most influent features.
% Note that, no instance found an exact match with the output of baseline, this is due to the fact that baseline is not sound and that our analysis is sound--but not complete.

\reffig{diabetes-euclidean-baseline-qlibra} and \reffig{diabetes-manhattan-baseline-qlibra} show the result for the Euclidean and Manhattan distance respectively.
For these two heuristics, a less dense graph shows higher similarity. Low distance means that the two input vectors (or list of sorted indices as in our case) are close together.
Confirming our expectancy, both \reffig{diabetes-euclidean-baseline-qlibra} and \reffig{diabetes-manhattan-baseline-qlibra} show high similitude in most of the test cases.
Note that, the Euclidean distance (\euclidean) returns a real number as distance value, thus bars do not necessarily correspond to a discrete values in the graph.

\setlength\figureheight{5cm}
\setlength\figurewidth{5cm}

  \begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.36\textwidth}
      \centering
      % \input{figures/diabetes-relaxed_common-baseline-permutation.tikz}
      \begin{tikzpicture}
        \begin{axis}[
          xlabel={Length},
        height=\figureheight,
        tick align=outside,
        tick pos=left,
        width=\figurewidth,
        xmin=-0.74, xmax=6.74,
        ymin=0, ymax=64.05,
        ylabel={Frequency},
        ]
        \draw[draw=none,fill=seabornBlue] (axis cs:0.6,0) rectangle (axis cs:1.4,61);
        \draw[draw=none,fill=seabornBlue] (axis cs:3.6,0) rectangle (axis cs:4.4,11);
        \draw[draw=none,fill=seabornBlue] (axis cs:-0.4,0) rectangle (axis cs:0.4,25);
        \draw[draw=none,fill=seabornBlue] (axis cs:1.6,0) rectangle (axis cs:2.4,24);
        \draw[draw=none,fill=seabornBlue] (axis cs:2.6,0) rectangle (axis cs:3.4,26);
        \draw[draw=none,fill=seabornBlue] (axis cs:4.6,0) rectangle (axis cs:5.4,4);
        \draw[draw=none,fill=seabornBlue] (axis cs:5.6,0) rectangle (axis cs:6.4,2);
        \end{axis}
        \end{tikzpicture}
      \caption{Baseline vs (\pfi).}
      \label{fig:diabetes-relaxedcommon-baseline-permutation}
    \end{subfigure}%
    \hfill
    % \vspace{10pt}
    % \hspace{0.05\textwidth}%
    \begin{subfigure}[b]{0.31\textwidth}
      \centering
      % \input{figures/diabetes-relaxed_common-baseline-retraining.tikz}
      \begin{tikzpicture}
        \begin{axis}[
          xlabel={Length},
        height=\figureheight,
        tick align=outside,
        tick pos=left,
        width=\figurewidth,
        xmin=-0.69, xmax=5.69,
        ymin=0, ymax=61.95,
        ]
        \draw[draw=none,fill=seabornBlue] (axis cs:1.6,0) rectangle (axis cs:2.4,28);
        \draw[draw=none,fill=seabornBlue] (axis cs:0.6,0) rectangle (axis cs:1.4,59);
        \draw[draw=none,fill=seabornBlue] (axis cs:2.6,0) rectangle (axis cs:3.4,18);
        \draw[draw=none,fill=seabornBlue] (axis cs:-0.4,0) rectangle (axis cs:0.4,43);
        \draw[draw=none,fill=seabornBlue] (axis cs:4.6,0) rectangle (axis cs:5.4,1);
        \draw[draw=none,fill=seabornBlue] (axis cs:3.6,0) rectangle (axis cs:4.4,4);
        \end{axis}
        \end{tikzpicture}
      \caption{Baseline vs (\rfe).}
      \label{fig:diabetes-relaxedcommon-baseline-retraining}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
      \centering
      % \input{figures/diabetes-relaxed_common-baseline-qlibra.tikz}
      \begin{tikzpicture}
        \begin{axis}[
          xlabel={Length},
        height=\figureheight,
        tick align=outside,
        tick pos=left,
        width=\figurewidth,
        xmin=-0.74, xmax=6.74,
        ymin=0, ymax=31.5,
        ]
        \draw[draw=none,fill=seabornBlue] (axis cs:3.6,0) rectangle (axis cs:4.4,26);
        \draw[draw=none,fill=seabornBlue] (axis cs:1.6,0) rectangle (axis cs:2.4,30);
        \draw[draw=none,fill=seabornBlue] (axis cs:4.6,0) rectangle (axis cs:5.4,16);
        \draw[draw=none,fill=seabornBlue] (axis cs:2.6,0) rectangle (axis cs:3.4,30);
        \draw[draw=none,fill=seabornBlue] (axis cs:0.6,0) rectangle (axis cs:1.4,24);
        \draw[draw=none,fill=seabornBlue] (axis cs:-0.4,0) rectangle (axis cs:0.4,19);
        \draw[draw=none,fill=seabornBlue] (axis cs:5.6,0) rectangle (axis cs:6.4,8);
        \end{axis}
        \end{tikzpicture}
      \caption{Baseline vs \impatto.}
      \label{fig:diabetes-relaxedcommon-baseline-qlibra-2}
    \end{subfigure}%
    \caption{\diabetes{} database, comparison between baseline and (left to right) permutation feature importance, retraining feature importance, and \impatto.}
    \label{fig:diabetes-baseline-all}
  \end{figure}

\paragraph{Comparison with Stochastic Methods}
% In the field of interpretability and explainability, stochastic quantitative measures, known as feature importance metrics, are used to determine the influence of input features in machine learning models.
% These metrics aim to provide insights into which input feature has the most influence on the model prediction.
%
% Among the plenty of metrics defined by the machine learning community
% For our evaluation, we chose \textit{Permutation Feature Importance} (PFI)~\sidecite{Breiman2001} and we define \textit{Retraining Feature Elimination} (RFE).
In this second experiment, we compare to the results of our analysis with stochastic quantitative measures, known as feature importance metrics, which are used to determine the influence of input variables in machine learning models.
Specifically, we compare to:
\begin{itemize}[font=\normalfont]
  \item[(\rfe)] A na\"ive feature importance metric that evaluates the changes in performance of a model when retrained without the feature of interest; in the following we call this metric \textit{Retraining Feature Elimination}.
  \item[(\pfi)] \textit{Permutation Feature Importance}~\sidecite{Breiman2001}, one of the most popular feature importance metrics, which monitors changes in performance when the values of the feature of interest are randomly shuffled.
\end{itemize}

% Permutation feature importance is one of the most popular feature importance metric among the plenty of metrics defined by the machine learning community due to its ability of breaking the relationship between the input features and output values by randomly shuffling this single input feature and monitor the drop in the model score.
% Therefore, obtaining an indication of how much the model depends on that feature.
% Next, retraining feature elimination involves various passes of fitting a model and eliminating a different input feature at each iteration.

% We now compare the results to permutation feature importance and retraining feature elimination metrics.
% The result of this evaluation shows that our approach is the only one adapt to tackle a formally defined impact property.
% To this end, \impatto{} approximates a precisely formalized impact definition.
% Unlike stochastic techniques that often rely on hardcoded impact definitions within their methods, limiting the flexibility and adaptability of their approach.

This experiment uses the \diabetes{} dataset (full evaluation with all the datasets in \refsec{full-experimental-overview}).
\reffig{diabetes-baseline-all} demonstrates the comparison of baseline with, permutation feature importance (\pfi), retraining feature importance (\rfe), and \impatto, respectively \reffig{diabetes-relaxedcommon-baseline-permutation}, \reffig{diabetes-relaxedcommon-baseline-retraining}, and \reffig{diabetes-relaxedcommon-baseline-qlibra-2}.
In this evaluation, we focus on the relaxed maximum common prefix length (\rmcpl) heuristic.
Like the (\mcpl) approach, it highlights the most impactful features.
However, the key advantage of \rmcpl{} is the employment of a margin opf error when computing the common prefix, which allows for a clearer distinction when two analysis results are similar.
In summary, we notice that our static analyzer \impatto{} always achieves a higher similarity compared to the other two stochastic metrics considered.


In conclusion, our evaluation demonstrates the effectiveness of the prototype of our static analysis in quantifying the impact of input features with respect to a formally defined impact property.
Through comparison with stochastic metrics, we consistently observe a higher degree of similarity between the results produced by \impatto{} and the baseline.
The experiments conducted on the \diabetes{} dataset illustrate this similarity across various heuristics and distance measures. \refsec{full-experimental-overview} contains the full overview of our experiments, we obtained similar results to the ones presented in this section.
Notably, our approach stands out as the only one capable of addressing a formally defined impact property, providing a flexible framework that surpasses the hardcoded intuitions of the other two methods.
Overall, these findings highlight the reliability and strength of \impatto{} in assessing the significance of input features.

\subsection{QLibra}
\labsec{eval-qlibra}

\denis{talk a bit about the tool}.

To demonstrate the effectiveness of \libra, we evaluated it on neural networks trained on
%popular datasets in the fairness literature. Specifically, we used the Japanese Credit Screening\sidenote{\url{https://archive.ics.uci.edu/ml/datasets/Japanese+Credit+Screening}} (\textsc{japanese} in the following) and
the
Adult dataset\sidenote{\url{https://archive.ics.uci.edu/ml/datasets/adult}} from the UCI Machine Learning Repository.
%
The dataset assigns to individuals a yearly income greater or smaller than \$50k based on personal attributes such as education and occupation but also gender, marital status, or race.
We set \libra to use gender as sensitive input feature.

We show below the experimental results on the smaller neural networks used by Urban et al. \cite{Urban20}, which better demonstrate the benefits of our implementation compared to its preliminary version.
%
In practice, Urban et al. \cite{Urban20} have already shown that the approach can scale to much larger networks with sizes on par with the literature on fairness certification, e.g., \cite{ManishaG20,YurochkinBS20}.
%
The neural networks were trained with Keras for 50 iterations, using the RMSprop optimizer with the default learning rate, and categorical cross-entropy as the loss function.
%
All networks are open source as part of \libra.

%We performed all the experiments using the CLEPS infrastructure\sidenote{\url{https://paris-cluster-2019.gitlabpages.inria.fr/cleps/cleps-userguide/}}, on the \texttt{cpu\_homogen} partition. Based on a 64-core Intel\textsuperscript{\tiny{\textregistered}} Xeon\textsuperscript{\tiny{\textregistered}} 5218 CPU @ 2.4GHz machine with 192GB of RAM, running CentOS 7.7. with linux kernel 3.10.0.
%

The experiments were conducted on the Inria Paris \textsc{cleps} infrastructure, on a machine with two 16-core Intel\textsuperscript{\tiny{\textregistered}} Xeon\textsuperscript{\tiny{\textregistered}} 5218 CPU @ 2.4GHz, 192GB of RAM, and running CentOS 7.7. with linux kernel 3.10.0.
%
For each experiment, we report the average results of five executions to account for the effect of randomness in the input space partitioning done by the forward pre-analysis (cf. refsec~\ref{sec:tool-architecture}).


\subsubsection{Effect of Neural Network Structure on Precision and Scalability.}
The precision and scalability of \libra's analysis depend on the analyzed neural network. reftab~\ref{tbl:models} shows the result of running \libra on different neural networks with different choices for the abstract domain used by the pre-analysis.
Column $|\textsc{m}|$ refers to the analyzed neural network by the number of its ReLU activations. From top to bottom, the neural networks have the following number of hidden layers and nodes per layer: $2$ and $5$, $4$ and $3$, $4$ and $5$, $4$ and $10$, and $9$ and $5$.
% \todo{larger networks are not actually needed for tabular datasets.
% These network sizes are on par with the fairness verification literature, e.g., see \cite{ManishaG20,YurochkinBS20}.
% However, \cite{Urban20} demonstrates the approach for networks with over than $1000$ nodes.}
% Indeed, these networks sizes are on par with the fairness verification literature, e.g., see \cite{ManishaG20,YurochkinBS20}.
% \todo{let's be careful here; the nets in the literature are slightly larger, having hundreds of nodes; we should say it as in the rebuttal}
We configured the pre-analysis with lower bound $\lowerbound = 0.5$ and upper bound $\upperbound = 5$. Each column shows the chosen abstract domain. We show here the results for \boxes, \symbolic, \deeppoly, \neurify, and the reduced product \textsc{deeppoly+neurify+symbolic} (i.e., \textsc{product} in the reftab~\ref{tbl:models}), which is the most precise of all possible reduced products.
%
% reftabs~\ref{tbl:models1-ext} and~\ref{tbl:models2-ext} in Appendix~\ref{apx:evaluation} show the results for all domain choices, including all possible reduced products.
%
The \textsc{input} rows show the average input-space coverage, that is, the average
%over $5$ executions)
percentage of the input space that \libra was able to analyze with the chosen pre-analysis configuration.
% Column $|\textsc{c}|$ shows the total number of analyzed input space partitions, while column $|\textsc{f}|$ shows the number of activation patterns (left) and feasible input partitions (right) passed to the backward analysis.
The \textsc{time} rows show the average running time.

For all neural networks, \textsc{product} achieves the highest input-space coverage, an improvement of up to $12.49\%$ over the best coverage obtained with only the abstract domains available in the preliminary version of \libra \cite{Urban20} (i.e., with respect to the \deeppoly domain for $|\textsc{m}| = 40$). Interestingly, such an improvement comes at the cost of a very modest increase in running time (i.e., just over $1$ minute).
% More often, the running time even decreases.
Indeed, using a more precise abstract domain for the pre-analysis generally results in fewer input space partitions being passed to the backward analysis and, in turn, this reduces the overall running time.

For the smallest neural networks (i.e., $|\textsc{m}| \in \{10, 12, 20\}$), the \symbolic abstract domain is the second best choice in terms of input-space coverage. This is likely due to the convex ReLU approximations of \deeppoly and \neurify which in some case produce a negative lower bound (cf. Figure~\ref{fig:deeppoly} and~\ref{fig:neurify}),
%in refsec~\ref{subsec:domains}),
while \symbolic always sets the lower bound to zero (cf. Figure~\ref{fig:naive}).
%in refsec~\ref{subsec:domains}).

Finally, for the largest neural networks (i.e., $|\textsc{m}| \in \{40, 45\}$), it is the structure of the network (rather than its number of ReLU activations) that impacts the precision and scalability of the analysis: for the deep but narrow network (i.e., $|\textsc{m}| = 45$), \libra achieves a higher input-space coverage in a shorter running time than for the shallow but wide network (i.e., $|\textsc{m}| = 40$). \\
%

\subsubsection{Precision-vs-Scalability Tradeoff.}
The configuration of \libra's pre-analysis allows trading-off between precision and scalability.
reftab~\ref{tbl:configurations} shows the average results of running \libra on the neural network with $20$ ReLUs
with different lower and upper bound configurations, and different choices for the abstract domain used by the pre-analysis.
%
Columns $\lowerbound$ and $\upperbound$ show the configured lower and upper bounds. We tried $\lowerbound \in \{0.5, 0.25\}$ and $\upperbound \in \{3, 5\}$. We again show the results for the \boxes, \symbolic, \deeppoly, \neurify abstract domains, and the most precise reduced product domain \textsc{deeppoly+neurify+symbolic} (i.e., \textsc{product} in reftab~\ref{tbl:configurations}).
% , while reftab~\ref{tbl:configurations-ext} in Appendix~\ref{apx:evaluation} shows the results for all domain choices.
%
%The differences between the results shown for the same neural network in reftab~\ref{tbl:models} and reftab~\ref{tbl:configurations} with $\lowerbound = 0.5$ and $\upperbound = 5$ are due to random choices that happen during the analysis. Indeed, to take account of this randomness, we show average results over $5$ executions. However, these differences concerning the input space coverage are always within a range of $\pm 0.05\%$ (maximum difference obtained with \deeppoly).
%
%As before, column \textsc{input} shows the input-space coverage, column $|\textsc{c}|$ shows the total number of analyzed input space partitions, column $|\textsc{f}|$ shows the number of activation patterns (left) and feasible input partitions (right) passed to the backward analysis. Finally, column \textsc{time} shows the running time.

As expected, decreasing the lower bound $\lowerbound$ or increasing the upper bound $\upperbound$ improves the input-space coverage (\textsc{input} rows) and increases the running time (\textsc{time} rows). We obtain an improvement of up to $12.44\%$ by increasing $\upperbound$ from $3$ to $5$ (with $\lowerbound = 0.25$ and $\boxes$), and up to $42.05\%$ by decreasing $\lowerbound$ from $0.5$ to $0.25$ (with $\upperbound = 5$ and $\boxes$). The smaller is $\lowerbound$ and the larger is $\upperbound$, the higher is the impact on the running time. Once again, for all lower and upper bound configurations, \textsc{deeppoly+neurify+symbolic} achieves the highest input-space coverage, improving up to $12.08\%$ over the best coverage obtained with only the abstract domains available in the preliminary version of \libra (i.e., with respect to \deeppoly with $\lowerbound = 0.5$ and $\upperbound = 5$). The improvement is more important for configurations with larger lower bounds.

Notably, reftab \ref{tbl:configurations} shows that \emph{none among the \symbolic, \deeppoly, and \neurify abstract domains is always more precise than the others}. There are cases where even \symbolic (implemented by \cite{WangPei2018-SecurityAnalysis}) outperforms \neurify (implemented by \cite{Wang18} which is the successor of \cite{WangPei2018-SecurityAnalysis} and is believed to be strictly superior to its predecessor), e.g., configuration $\lowerbound=0.5$ and $\upperbound=5$. We thus argue
for using reduced products of abstract domains also in other contexts beyond fairness certification, e.g., verifying local robustness~\cite[etc.]{Li19,Singh19} or verifying functional properties of neural networks~\cite{Katz17}. \\



\subsubsection{Leveraging Multiple CPUs.}
The optimal pre-analysis configuration in terms of precision or scalability depends on the analyzed neural network.
%
In order to push \libra to its limits and obtain $100\%$ input-space coverage on the neural network with 20 ReLUs, we used the new configuration auto-tuning mechanism starting with $\lowerbound = 1$ and $\upperbound = 0$ (i.e., the most restrictive lower and upper bound configuration) and setting $\minlowerbound = 0$ and $\maxupperbound = 20$ (i.e., the most permissive configuration).
% (in practice, analyzing the entire input space is not necessarily needed).
For all choices of abstract domains, the pre-analysis eventually stabilizes with lower bound $\lowerbound = 0.015625$ and upper bound $\upperbound = 6$.

Figure~\ref{plt:cpus} compares the average running times for \boxes, \symbolic, \deeppoly, \neurify, and the reduced product \textsc{deeppoly+neurify+symbolic} (i.e., \textsc{product}) as a function of the number of available CPUs.
%
%
With \textsc{product} we obtained a running time improvement of $14.39\%$ over \symbolic, i.e., the fastest domain available in the preliminary version of \libra (a minimum improvement of $11.54\%$ with $16$ CPUs, and a maximum improvement of $18.24\%$ with $64$ vCPUs).
%
As expected, adding more CPUs always improves \libra running time. The most limited improvement in running time that occurs between $32$ CPUs and $64$ vCPUs is likely due to the use of hyperthreading as context switches between processes running intense numeric computations produce more overhead.
%

reftab~\ref{tbl:cpus} additionally shows the estimated percentage of bias detected with each abstract domain, i.e., \libra is able to certify fairness for about $95\%$ of the neural network input space. Note that, the bias estimate depends on the partitioning of the input space computed by the pre-analysis, cf. refsec~\ref{sec:tool-architecture}. This explains the different percentages found even by runs with the same abstract domain. Within the same column, the difference is at most $0.14\%$ on average.

Finally, we remark that \emph{the new auto-tuning mechanisms is essential for scalability}. We tried repeating this experiment by directly
running \libra with the configuration at which auto-tuning stabilizes, i.e., $\lowerbound = 0.015625$ and $\upperbound = 6$. After six days it still had not completed and we had to interrupt it.



\section{Timing Side-Channels}
\labsec{timing-side-channels}


In this section, we showcase the potential of \impatto{} on the \bignum{} library\sidenote{\url{https://github.com/awslabs/s2n-bignum}}.
% In \Appendix{sv-comp}, we show an evaluation on the \svcomp{} benchmarks, focusing on the effect of changes in the input space, the analysis time, and the categorization of input variables.

% \subsection{\texorpdfstring{\bignum{} Library}{S2N Bignum Library}}
% \label{sec:s2n-bignum-library}

The \bignum{} library~\sidecite{bignum} is a collection of arithmetic routines designed for cryptographic applications.
All the routines are written in pure machine code, designed to be callable from C and other high-level languages.
Each function is written in a constant-time style, to avoid leaking information through timing side-channels.
Constant-time means that the execution time of an \bignum{} operation is independent of the actual numbers involved,
depending only on their nominal sizes.
% Indeed, each \bignum{} operation manipulates numbers on the basis of the nominal sizes only, independently of the actual values, even if those are zero.
If a result does not fit in the provided size, it is systematically truncated modulo that size.
Allocation of memory is always the caller's responsibility, the \bignum{} interface only uses pointers to pre-existing arrays.
The developers avoid the use of certain machine instructions known to be problematic for constant-time execution, such as the division instruction.
Furthermore, on ARM platforms, the library sets the DIT (Data Independent Timing) bit to have hardware guaranteed constant-time execution.

The library is fully verified for functional correctness in \hollight~\sidecite{Harrison2009}, but the verification of the constant-time property is still ongoing.
At present, the constant-time property is enforced by the strict compliance to the constant-time design discipline and the use of empirical testing.
Their empirical
result\sidenote{(Last accessed: 14th May 2024) \url{https://github.com/awslabs/s2n-bignum?tab=readme-ov-file\#benchmarking-and-constant-time}}
shows that the variation in runtime with respect to the data being manipulated is within a few percent in all the cases.
Unfortunately, the empirical study is not sufficient to guarantee the constant-time property, as it is not exhaustive and does not cover all the possible inputs.
On the other hand, the quantitative analysis of \impatto{} provides a formal verification of the constant-time property.
In particular, whenever an input variable has no impact on the global number of loop iterations, it is formally guaranteed that the number of iterations is independent of the values of that input variable.
Formally, a program $\defprogram$ is free of timing side-channels with respect to an input variable $\definputvariable\in\inputvariables$, if and only if $\defprogram \satisfies \mathscr{B}^{\le 0}_{\definputvariable}$.
% \denis{Add a theorem such like: if is side channel freedom on numerical variables that the library is safe.}
By \refthm{soundness}, we know that this is implied from $\abstractrange(\backwardsemantics) \le 0$.
Therefore, the verification of \textit{timing side-channel freedom} is sound with respect to our quantitative analysis of input variables.
We partition the input variables of the \bignum{} library into two subsets.
The nominal size variables and additional parameters that may safely influence the runtime into $\nominalvariables$.
The variables that represent the actual numerical values and additional parameters that, instead, should not influence the execution time into $\numericalvariables$.
The \bignum{} library is free of timing side-channels, whenever for any program $\defprogram$ in \bignum{} and any numerical input variable $\definputvariable\in\numericalvariables$, it holds that $\abstractrange(\backwardsemantics) = 0$.

For our setup, we consider the disassembled operations\sidenote{We used \ghidra{} (\url{https://ghidra-sre.org/}) to disassemble the library and extract the arithmetic routines.} of the \bignum{} library as input programs with a few rewriting steps to fit the set of supported operations of our tool.
Mostly, the rewriting steps soundly resolve the few jumps that arise from the disassembling process.
Our benchmark contains a total of 72 disassembled arithmetic routines, excluding only a single operation (program \texttt{bignum\_modexp}) that has function calls, which our tool does not yet support.
On average, each program has about 83 lines of code, for a total of 5984 lines of code.
% For each input variable we consider an unbounded non-negative input space, \ie, for any input variable $\definputvariable\in\inputvariables$, we have $\definputvariable \ge 0$.

% \input{tables/bignum-inputs}
%
The library contains a total of 1172 variables, 272 of which are input variables.
\reftab{bignum-inputs} reports the analysis findings for the input variables of the \bignum{} library: column \textsc{Maybe Dangerous} reports variables which could be prone to timing side-channel attacks (namely $\abstractrange(\backwardsemantics) > 0$),
column \textsc{Zero Impact} reports the variables with an impact quantity of zero (namely $\abstractrange(\backwardsemantics) = 0$).
The property $\mathscr{B}\scalerel*{\vphantom{}^{{\scaleto{\le}{5pt}0}}_{\definputvariable}}{\padding}$ holds for input variables $\definputvariable$ that have an impact quantity of zero (column \textsc{Zero Impact}).
% Interesting, the syntactic dependency analysis is already able to prove that 182 (66\%) of the input variables do not influence the global number of iterations.
% The quantification of the impact further reduce the number of input variables that influence the global number of iterations to 85 (31\%).
Overall, we soundly verified that 187 (69\%) of the input variables do not influence the global number of iterations, while 85 (31\%) are maybe dangerous and maybe susceptible to timing side-channel attacks.
%
Column \textsc{Safe} $\nominalvariables$ reports the nominal size variables (called \green{$s_i$}), column \textsc{Numerical} $\numericalvariables$ reports the numerical variables (called \red{$n_i$}, where $i$ is the index of the variable as they appear in the function signature).
\reftab{bignum-inputs} shows that no numerical variable is identified as potentially dangerous, indeed $\textsc{Maybe Dangerous} \setmeet \numericalvariables = \emptyset$ in all rows.
We conclude that the \bignum{} library is \emph{free of timing side-channels}.


%


% \input{tables/bignum-ablation}

Additionally, we perform an ablation study to evaluate the impact of the dependency analysis and the other optimizations on our tool.
The first row \nodepnoopt{} of \reftab{bignum-abl} reports the analysis findings without the various analysis stages of \refsec{implementation}, while the second row \depopt{} shows the finding of the full \impatto{} analysis.
Without the dependency analysis we do not apply program slicing anymore, we handle bitwise operations and array accesses with a conservative over-approximation that may lead to false positives.
Generally, we notice that the invariant inferred from the global loop bound analysis alone is not tight enough to produce a precise quantification of the impact.
Therefore, we are not able to infer useful insights from our analysis as 266 input variables are maybe dangerous.
In particular, the 6 input variables with zero impact belong to acyclic programs.
% Without the forward pre-analysis and other optimizations, only 95 variables are maybe dangerous, but unfortunately the quantification of the impact is not tight enough to exclude more unused variables.
Regarding the analysis time, column \depslabel{} refers to the time of the dependency analysis, column \invlabel{} for the global loop bound analysis, and column \lplabel{} for the quantification of impact.
The time is reported in seconds for the evaluation of the 72 programs.
The last column \textsc{Tot} reports the total analysis time, with the standard deviation after the symbol $\pm$.
\reftab{bignum-abl} does not show the time for parsing, logging and other overheads of the tool.
We notice that without the optimizations, the analysis time is about 4 times faster than the full analysis, with most time spent on the linear programming problem as more variables need to be quantified.
In this case, the standard deviation of the total analysis time (after $\pm$ in the column \textsc{Tot}) is the lowest, meaning that the analysis time is more consistent among programs.
With only the dependency analysis on, the analysis usually takes around 50 seconds and, without optimizations, the global loop bound analysis is quite fast.
The full analysis is about 100 seconds in total, with an average of 1.22 seconds per program.
Most of the analysis time is spent on the syntactic dependency and the global loop bound analysis. Notably, the linear programming problem to quantify the impact of input variables takes less than half a second in total for the whole library.
However, the analysis time is not consistent for all the programs, in fact, the analysis time for each program ranges from 0.03 to 33.88 seconds (standard deviation of about 4 seconds).
Nevertheless, the full analysis is also the most precise, as it is able to exclude the most number of maybe dangerous variables.
%

In conclusion, the \bignum{} library is a good candidate for our analysis, as it is a real-world cryptographic library potentially vulnerable against timing side-channel attacks for numerical input variables.
Up to the decompilation phase and the chosen abstraction of the runtime, \cf{} the global number of iterations, our analysis soundly verifies that no input variable containing numerical data is susceptible to timing side-channel attacks.

\section{Summary}

This chapter concludes the main body of the thesis, presenting the evaluation of our static analysis for quantitative program properties.
Next, we provide a discussion of the related work, followed by the conclusion and future work in the final chapter.
